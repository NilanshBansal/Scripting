title,body,tags
Machine Learning for random number prediction,"<p>I am working on machine learning task. I want to make a user input number prediction. For instance, program will ask user for input a number between a range lets say 1-50 and model will predict this number which user is going to do. After prediction, model will adjust itself or learning from this if prediction is correct or not. I have been working on machine language for a long but never encountered this situation in which I have to just model at run time. Can anyone suggest me better strategy to do this task?  I have tried the following code but it does not predict correctly</p>

<pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_blobs
# create the inputs and outputs
X, y = make_blobs(n_samples=1000, centers=50, n_features=1, random_state=0)
# define model
model = LogisticRegression(solver='lbfgs')

# fit model
model.fit(X, y)
# define input
new_input = [[4],[3],[2],[1],[3]]
# get prediction for new input
new_output = model.predict(new_input)
# summarize input and output
print(""New Input:"",new_input)
print(""New Output"", new_output)
</code></pre>
","['x', 'scikit-learn', 'random', 'a', 'reinforcement-learning', 'range', 'predict', 'machine learning', 'import', 'machine language', 'python', 'model', 'prediction', 'machine-learning']"
Machine Learning for random number prediction,"<p>I am working on machine learning task. I want to make a user input number prediction. For instance, program will ask user for input a number between a range lets say 1-50 and model will predict this number which user is going to do. After prediction, model will adjust itself or learning from this if prediction is correct or not. I have been working on machine language for a long but never encountered this situation in which I have to just model at run time. Can anyone suggest me better strategy to do this task?  I have tried the following code but it does not predict correctly</p>

<pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_blobs
# create the inputs and outputs
X, y = make_blobs(n_samples=1000, centers=50, n_features=1, random_state=0)
# define model
model = LogisticRegression(solver='lbfgs')

# fit model
model.fit(X, y)
# define input
new_input = [[4],[3],[2],[1],[3]]
# get prediction for new input
new_output = model.predict(new_input)
# summarize input and output
print(""New Input:"",new_input)
print(""New Output"", new_output)
</code></pre>
","['x', 'scikit-learn', 'random', 'a', 'reinforcement-learning', 'range', 'predict', 'machine learning', 'import', 'machine language', 'python', 'model', 'prediction', 'machine-learning']"
updating a view with changing variables,"<p>I have this program with SwiftUI. The program is for calculating the bedtime using machine learning based on 3 user inputs. I have a Text("""") showing users their updated bedtime. </p>

<p>I want the program to update the bedtime automatically and display it on my Text(""""). I tried many methods and none seems to work. What I tried so far </p>

<ol>
<li>onAppear - only updates once bedtime when the program first runs</li>
<li>onTapGesture - only updates the bedtime when tapping on the picker (scrolling the picker doesn't work), and it somehow hinders updating the stepper (clicking +/- doesn't change the hours)</li>
<li>using didSet with class conforming to observableObject, @Pulished vars in the class and @ObservedObject in the view struct. Didn't work as well but I tried it only when the class has default values </li>
<li>using didSet in the struct - it didn't update bedtime</li>
</ol>

<p>Does anyone know if there's an easier way to have the bedtime updated however the user scrolls the picker and whenever a variable changes? </p>

<p><a href=""https://i.stack.imgur.com/P5H4R.png"" rel=""nofollow noreferrer"">UI looks for detail</a></p>

<pre><code>struct ContentView: View {

    static var defaultWakeUpTime : Date {
        var defaultTime = DateComponents()
        defaultTime.hour = 7
        defaultTime.minute = 0
        return Calendar.current.date(from: defaultTime) ?? Date()
    }

    @State private var wakeUp = defaultWakeUpTime
    @State private var sleepAmount = 8.0
    @State private var coffeeAmount = 0 {
        didSet {
            calculateSleepTime()
        }
    }

    @State private var showTime : String = "" ""

    func calculateSleepTime() {**CONTENT**}

    var body: some View {
        NavigationView {
                VStack {
                    Spacer(minLength: 20)

                    Text(""Your optimum sleep time is \(showTime)"")

                    Spacer(minLength: 10)

                    Section {
                    Text(""When do you want to wake up?"")
                        .font(.headline)
                        DatePicker(""Please choose a time"", selection: $wakeUp, displayedComponents: .hourAndMinute)
                        .labelsHidden()
                        .datePickerStyle(WheelDatePickerStyle())
                    }

                Spacer()

                    Form {
                        Text(""How many hours would you like to sleep?"")
                            .font(.headline)
                        Stepper(value: $sleepAmount, in: 4...12, step: 0.25) {
                            Text(""\(sleepAmount, specifier: ""%g"" ) hours"")
                        }

                    }

                Spacer()

                    Section {
                        Text(""How many cups of coffee do you drink?"")
                            .font(.headline)

                        Picker(""Coffee Selector"", selection: $coffeeAmount) {
                            ForEach (1..&lt;21) {
                                Text(""\($0) "" + ""Cup"")
                            }
                        }
                        .labelsHidden()
                    }
                }
                .navigationBarTitle(Text(""BetterSleep""))
                .onAppear(perform: calculateSleepTime)
        }
    }
}
</code></pre>
","['font', 'foreach', 'static', 'clicking', 'navigationview', 'swift', 'form', 'picker', 'cups', 'a', 'calendar', 'swiftui', 'private', 'cup', 'machine learning', 'datepicker', 'func', 'var', 'didset']"
updating a view with changing variables,"<p>I have this program with SwiftUI. The program is for calculating the bedtime using machine learning based on 3 user inputs. I have a Text("""") showing users their updated bedtime. </p>

<p>I want the program to update the bedtime automatically and display it on my Text(""""). I tried many methods and none seems to work. What I tried so far </p>

<ol>
<li>onAppear - only updates once bedtime when the program first runs</li>
<li>onTapGesture - only updates the bedtime when tapping on the picker (scrolling the picker doesn't work), and it somehow hinders updating the stepper (clicking +/- doesn't change the hours)</li>
<li>using didSet with class conforming to observableObject, @Pulished vars in the class and @ObservedObject in the view struct. Didn't work as well but I tried it only when the class has default values </li>
<li>using didSet in the struct - it didn't update bedtime</li>
</ol>

<p>Does anyone know if there's an easier way to have the bedtime updated however the user scrolls the picker and whenever a variable changes? </p>

<p><a href=""https://i.stack.imgur.com/P5H4R.png"" rel=""nofollow noreferrer"">UI looks for detail</a></p>

<pre><code>struct ContentView: View {

    static var defaultWakeUpTime : Date {
        var defaultTime = DateComponents()
        defaultTime.hour = 7
        defaultTime.minute = 0
        return Calendar.current.date(from: defaultTime) ?? Date()
    }

    @State private var wakeUp = defaultWakeUpTime
    @State private var sleepAmount = 8.0
    @State private var coffeeAmount = 0 {
        didSet {
            calculateSleepTime()
        }
    }

    @State private var showTime : String = "" ""

    func calculateSleepTime() {**CONTENT**}

    var body: some View {
        NavigationView {
                VStack {
                    Spacer(minLength: 20)

                    Text(""Your optimum sleep time is \(showTime)"")

                    Spacer(minLength: 10)

                    Section {
                    Text(""When do you want to wake up?"")
                        .font(.headline)
                        DatePicker(""Please choose a time"", selection: $wakeUp, displayedComponents: .hourAndMinute)
                        .labelsHidden()
                        .datePickerStyle(WheelDatePickerStyle())
                    }

                Spacer()

                    Form {
                        Text(""How many hours would you like to sleep?"")
                            .font(.headline)
                        Stepper(value: $sleepAmount, in: 4...12, step: 0.25) {
                            Text(""\(sleepAmount, specifier: ""%g"" ) hours"")
                        }

                    }

                Spacer()

                    Section {
                        Text(""How many cups of coffee do you drink?"")
                            .font(.headline)

                        Picker(""Coffee Selector"", selection: $coffeeAmount) {
                            ForEach (1..&lt;21) {
                                Text(""\($0) "" + ""Cup"")
                            }
                        }
                        .labelsHidden()
                    }
                }
                .navigationBarTitle(Text(""BetterSleep""))
                .onAppear(perform: calculateSleepTime)
        }
    }
}
</code></pre>
","['font', 'foreach', 'static', 'clicking', 'navigationview', 'swift', 'form', 'picker', 'cups', 'a', 'calendar', 'swiftui', 'private', 'cup', 'machine learning', 'datepicker', 'func', 'var', 'didset']"
Is there a service that detects an inappropriate picture,"<p>I have an android project in which user uploads different car pictures but i need to make sure those pictures are not inappropriate and strictly Cars relaged so how would i make sure? 
One thing is to do that admin should approve it first but there would be loads of pictures it would be hassle for an admin to approve or reject all of them.
Other thing would be to apply a machine learning model for vehicle detection and make sure that there would only be cars in that picture, so are there any pretrain models or Api that would resolve this issue.
or anyother solutions for this ?
Note : my backend is on node.js and client side is on java.</p>
","['java', 'models', 'admin', 'machine-learning', 'a', 'node.js', 'machine learning', 'android', 'model', 'api', 'detection']"
Is there a service that detects an inappropriate picture,"<p>I have an android project in which user uploads different car pictures but i need to make sure those pictures are not inappropriate and strictly Cars relaged so how would i make sure? 
One thing is to do that admin should approve it first but there would be loads of pictures it would be hassle for an admin to approve or reject all of them.
Other thing would be to apply a machine learning model for vehicle detection and make sure that there would only be cars in that picture, so are there any pretrain models or Api that would resolve this issue.
or anyother solutions for this ?
Note : my backend is on node.js and client side is on java.</p>
","['java', 'models', 'admin', 'machine-learning', 'a', 'node.js', 'machine learning', 'android', 'model', 'api', 'detection']"
General Questions: Azure Virtual Machine Scale Sets,"<p>I am learning VMSS and have two general questions for my understanding.</p>

<p>Q1 : <strong>How new virtual machines gets provisioned in the VMSS?</strong></p>

<ul>
<li>Does it clone the existing VM from VMSS or use Image every time to provision a new VM?</li>
</ul>

<p>. If it clone existing VM from VMSS then which VM out of the existing VM’s it clone?</p>

<p>. If it provision new VM from the image then in the case of platform image, do I need to install web server and other changes every time because I do not have a requirement to use a custom image?</p>

<ol start=""2"">
<li>How to make changes to the VM’s in VMSS?</li>
</ol>

<p>. If I need to modify web server settings then how do I make the same settings on the other VM’s in VMSS?</p>
","['image', 'azure', 'install', 'a', 'clone', 'machine learning', 'azure-vm-scale-set', 'virtual-machine']"
General Questions: Azure Virtual Machine Scale Sets,"<p>I am learning VMSS and have two general questions for my understanding.</p>

<p>Q1 : <strong>How new virtual machines gets provisioned in the VMSS?</strong></p>

<ul>
<li>Does it clone the existing VM from VMSS or use Image every time to provision a new VM?</li>
</ul>

<p>. If it clone existing VM from VMSS then which VM out of the existing VM’s it clone?</p>

<p>. If it provision new VM from the image then in the case of platform image, do I need to install web server and other changes every time because I do not have a requirement to use a custom image?</p>

<ol start=""2"">
<li>How to make changes to the VM’s in VMSS?</li>
</ol>

<p>. If I need to modify web server settings then how do I make the same settings on the other VM’s in VMSS?</p>
","['image', 'azure', 'install', 'a', 'clone', 'machine learning', 'azure-vm-scale-set', 'virtual-machine']"
Capturing an encrypted network traffic,"<p>I am doing a project in Machine Learning, for that, I need data of encrypted network flow. I want the data to be in pcap file. How can I simulate and capture the encrypted network flow data?</p>

<p>I came across a tool <a href=""https://github.com/hanzhang0116/BotTalker"" rel=""nofollow noreferrer"">BotTalker</a> in Github, developed in C, openssl. I exactly expect those functionalities, but the tool is not working properly. </p>

<p>Please help me out to get the flow data? Either by fixing the tool or recommending another way to capture them. Can I try capturing flow data from VPN? will it be encrypted?</p>
","['openssl', 'flow', 'network-programming', 'a', 'network', 'encryption', 'github', 'machine learning', 'pcap', 'c', 'networking', 'capture']"
Capturing an encrypted network traffic,"<p>I am doing a project in Machine Learning, for that, I need data of encrypted network flow. I want the data to be in pcap file. How can I simulate and capture the encrypted network flow data?</p>

<p>I came across a tool <a href=""https://github.com/hanzhang0116/BotTalker"" rel=""nofollow noreferrer"">BotTalker</a> in Github, developed in C, openssl. I exactly expect those functionalities, but the tool is not working properly. </p>

<p>Please help me out to get the flow data? Either by fixing the tool or recommending another way to capture them. Can I try capturing flow data from VPN? will it be encrypted?</p>
","['openssl', 'flow', 'network-programming', 'a', 'network', 'encryption', 'github', 'machine learning', 'pcap', 'c', 'networking', 'capture']"
C# which data structure is suitable for efficient querying in both ways(forward and backward),"<p>I'm considering how to decide data structures for this case.</p>

<p>In my application (kind of machine learning application), three classes exist <code>ImageData</code>, <code>LabelData</code>, and <code>ClassData</code>.
And list of <code>ImageData</code> and list of <code>ClassData</code> are needed, probably list of <code>LabelData</code> also. </p>

<p>Their relationships are shown in the figure at link(lack of reputation...).
<a href=""https://i.stack.imgur.com/cy1LE.png"" rel=""nofollow noreferrer"">Images, Labels and Classes</a></p>

<ul>
<li>The program has a list of images</li>
<li>The program has a list of classes</li>
<li>Each image has multiple labels</li>
<li>Every label indicates a class in the list of classes (by ref. type)</li>
</ul>

<p>So I constructed data structure like this:</p>

<pre><code>class ImageData {
    public List&lt;LabelData&gt; Labels {get; set;}
}
class LabelData {
    ClassData ClassData {get; set;}
}
</code></pre>

<p>If I count 'how many labels in a single image per class', it seems OK.
But If I count 'how many labels per class in all images' it seems little bit inefficient because a loop for images is needed. I know LINQ helps this issue, but I guess a more efficient way exists in the point of data structure.</p>

<p>Any comments are welcome, or pleases let me know any kewords to solve this issue.
Thanks!</p>

<p>==================== revised! ======================</p>

<p>It was not clear as other said, so I added more specific examples.</p>

<p>Conditions about this application:</p>

<ul>
<li>The program handles multiple images.</li>
<li>User can draw a label to images.</li>
<li>Multiple labels can be drawn to a single image.</li>
<li>The program handles multiple classes, and a label must be assigned to one of these classes before it is drawn at the image.</li>
</ul>

<p>Currently, I implemented classes like below</p>

<pre><code>class ImageData {
    public List&lt;LabelData&gt; Labels {get; set;}
    public string ImagePath {get; set;}
    // constructor
    public ImageData(string imagePath) {
        Labels = new List&lt;LabelData&gt;();
        ImagePath = imagePath;
    }
}
class LabelData {
    public ClassData ClassData {get; set;}
    // constructor
    public LabelData(ClassData classData) {
        ClassData = classData
    }
}
class ClassData { }
</code></pre>

<p>and my application has data as:</p>

<pre><code>private List&lt;ImageData&gt; _images;
private List&lt;ClassData&gt; _classes;
</code></pre>

<p>Here comes the issues. In my program I want to show some statistics, for example, number of labels.</p>

<p>First, if user want to know ""number of labels in a single image(<code>_images[0]</code>) assigned as a single class(<code>_classes[0]</code>)"":</p>

<pre><code>_images[0].Labels.Count(v =&gt; v.ClassData == _classes[0])
</code></pre>

<p>It's not an issue for this first case, but secondly, if user want to know ""number of labels in all images assigned as a single class(<code>_classes[0]</code>):</p>

<pre><code>_images.Sum(v =&gt; v.Labels.Count(x =&gt; x.ClassData == _classes[0]))
</code></pre>

<p>But, I think this implementation of second case is not efficient because <code>_images.Sum(~)</code> implicates foreach loop of <code>_images</code>. If this program handles thousands of images, it gets slower.</p>

<p>So my question is that is there a better implementation of <code>ImageData</code>, <code>LabelData</code>, and <code>ClassData</code> to improve efficiency. For example, i) the program handles list of images, list of labels, list of classes; ii) and additionally handles other join index containers like <code>Tuple&lt;int, int&gt; LabelImageIndices</code>, <code>Tuple&lt;int, int&gt; LabelClassIndices</code>.</p>

<p>Which data structures fit this issue?
Or which keywords that I can find to solve this issue?
Thanks!!!</p>
","['image', 'list', 'querying', 'forward', 'linq', 'handles', 'a', 'imagedata', 'constructor', 'foreach', 'index', 'private', 'exists', 'labels', 'c#', 'containers', 'x', 'label', 'draw', 'machine learning', 'data-structures']"
C# which data structure is suitable for efficient querying in both ways(forward and backward),"<p>I'm considering how to decide data structures for this case.</p>

<p>In my application (kind of machine learning application), three classes exist <code>ImageData</code>, <code>LabelData</code>, and <code>ClassData</code>.
And list of <code>ImageData</code> and list of <code>ClassData</code> are needed, probably list of <code>LabelData</code> also. </p>

<p>Their relationships are shown in the figure at link(lack of reputation...).
<a href=""https://i.stack.imgur.com/cy1LE.png"" rel=""nofollow noreferrer"">Images, Labels and Classes</a></p>

<ul>
<li>The program has a list of images</li>
<li>The program has a list of classes</li>
<li>Each image has multiple labels</li>
<li>Every label indicates a class in the list of classes (by ref. type)</li>
</ul>

<p>So I constructed data structure like this:</p>

<pre><code>class ImageData {
    public List&lt;LabelData&gt; Labels {get; set;}
}
class LabelData {
    ClassData ClassData {get; set;}
}
</code></pre>

<p>If I count 'how many labels in a single image per class', it seems OK.
But If I count 'how many labels per class in all images' it seems little bit inefficient because a loop for images is needed. I know LINQ helps this issue, but I guess a more efficient way exists in the point of data structure.</p>

<p>Any comments are welcome, or pleases let me know any kewords to solve this issue.
Thanks!</p>

<p>==================== revised! ======================</p>

<p>It was not clear as other said, so I added more specific examples.</p>

<p>Conditions about this application:</p>

<ul>
<li>The program handles multiple images.</li>
<li>User can draw a label to images.</li>
<li>Multiple labels can be drawn to a single image.</li>
<li>The program handles multiple classes, and a label must be assigned to one of these classes before it is drawn at the image.</li>
</ul>

<p>Currently, I implemented classes like below</p>

<pre><code>class ImageData {
    public List&lt;LabelData&gt; Labels {get; set;}
    public string ImagePath {get; set;}
    // constructor
    public ImageData(string imagePath) {
        Labels = new List&lt;LabelData&gt;();
        ImagePath = imagePath;
    }
}
class LabelData {
    public ClassData ClassData {get; set;}
    // constructor
    public LabelData(ClassData classData) {
        ClassData = classData
    }
}
class ClassData { }
</code></pre>

<p>and my application has data as:</p>

<pre><code>private List&lt;ImageData&gt; _images;
private List&lt;ClassData&gt; _classes;
</code></pre>

<p>Here comes the issues. In my program I want to show some statistics, for example, number of labels.</p>

<p>First, if user want to know ""number of labels in a single image(<code>_images[0]</code>) assigned as a single class(<code>_classes[0]</code>)"":</p>

<pre><code>_images[0].Labels.Count(v =&gt; v.ClassData == _classes[0])
</code></pre>

<p>It's not an issue for this first case, but secondly, if user want to know ""number of labels in all images assigned as a single class(<code>_classes[0]</code>):</p>

<pre><code>_images.Sum(v =&gt; v.Labels.Count(x =&gt; x.ClassData == _classes[0]))
</code></pre>

<p>But, I think this implementation of second case is not efficient because <code>_images.Sum(~)</code> implicates foreach loop of <code>_images</code>. If this program handles thousands of images, it gets slower.</p>

<p>So my question is that is there a better implementation of <code>ImageData</code>, <code>LabelData</code>, and <code>ClassData</code> to improve efficiency. For example, i) the program handles list of images, list of labels, list of classes; ii) and additionally handles other join index containers like <code>Tuple&lt;int, int&gt; LabelImageIndices</code>, <code>Tuple&lt;int, int&gt; LabelClassIndices</code>.</p>

<p>Which data structures fit this issue?
Or which keywords that I can find to solve this issue?
Thanks!!!</p>
","['image', 'list', 'querying', 'forward', 'linq', 'handles', 'a', 'imagedata', 'constructor', 'foreach', 'index', 'private', 'exists', 'labels', 'c#', 'containers', 'x', 'label', 'draw', 'machine learning', 'data-structures']"
Best Machine learning algorithm for grouping database rules in sets,"<p>My optimization problem is network security related. I am using Python.
In the network, there exist devices that we call objects, each has an ID
Each object has a number of firewall rules that control the traffic from and to the object for micro-segmentation of the network. Each rule has an ID. The rules are somehow generic. Which means that different network objects can use the same rules, it's a many to many relationships. The data is in for form of a table of IDS as below:</p>

<pre><code>Object ID    Rule ID
O1            R1
O2            R1
O2            R2
O3            R1
O3            R2
O3            R3
O4            R4
</code></pre>

<p>In the network, the firewall rules are checked from top to bottom in order. The target is to group the rules into sets and to link the object to the sets that contain the rules the object uses to speeds up the process of checking the rules. The output should be something like:</p>

<pre><code>Object ID    Rule ID      SetID
O1            R1            S1
O2            R1            S1
O2            R2            S1
O3            R1            S1
O3            R2            S1
O3            R3            S2
O4            R4            S2
</code></pre>

<p>So instead of being linked to one bucket containing all rules, the object will only be linked to a subset of rules to minimize the time of checking rules the object is not originally linked to.
The grouping is governed by the below constraints and objective function:
Cluster these rules into a number of rule sets (no restrictions on the number of sets) such that</p>

<p>1-      each object ID is by maximum linked to 3 rule sets. Which means that per object the linked rules can be distributed on a maximum of 3 sets.</p>

<p>2-      If an object ID is linked to a rule set ID, we need to minimize the number of rules within the set that the object does not use (is not originally linked to) (our objective function).</p>

<p>3-      The number of rules per set should not exceed 250</p>

<p>4-      No restriction on the number of objects that can be linked to a rules set</p>

<p>5-      A rule can appear in more than one set (not optimal)</p>

<p>What is the best machine-learning algorithm to implement that?</p>

<p>What I thought of is agglomerative clustering where:</p>

<p>Nb of clusters=nb of rules/250 (although this is not always optimal)</p>

<p>Distance between 2 rules: nb of non-common objects the rules are linked to (the complement of their objects intersection )divided by the total number of objects they are both linked to</p>

<p>How then should I specify a condition while clustering that each object can be by maximum linked to 3 sets?</p>

<p>is it a clustering problem at all?</p>
","['constraints', 'form', 'firewall', 'security', 'intersection', 'bucket', 'rules', 'network', 'optimization', 'complement', 'grouping', 'a', 'python', 'machine-learning', 'distance', 'checked', 'minimize', 'distributed', 'database', 'exceed', 'machine learning']"
Best Machine learning algorithm for grouping database rules in sets,"<p>My optimization problem is network security related. I am using Python.
In the network, there exist devices that we call objects, each has an ID
Each object has a number of firewall rules that control the traffic from and to the object for micro-segmentation of the network. Each rule has an ID. The rules are somehow generic. Which means that different network objects can use the same rules, it's a many to many relationships. The data is in for form of a table of IDS as below:</p>

<pre><code>Object ID    Rule ID
O1            R1
O2            R1
O2            R2
O3            R1
O3            R2
O3            R3
O4            R4
</code></pre>

<p>In the network, the firewall rules are checked from top to bottom in order. The target is to group the rules into sets and to link the object to the sets that contain the rules the object uses to speeds up the process of checking the rules. The output should be something like:</p>

<pre><code>Object ID    Rule ID      SetID
O1            R1            S1
O2            R1            S1
O2            R2            S1
O3            R1            S1
O3            R2            S1
O3            R3            S2
O4            R4            S2
</code></pre>

<p>So instead of being linked to one bucket containing all rules, the object will only be linked to a subset of rules to minimize the time of checking rules the object is not originally linked to.
The grouping is governed by the below constraints and objective function:
Cluster these rules into a number of rule sets (no restrictions on the number of sets) such that</p>

<p>1-      each object ID is by maximum linked to 3 rule sets. Which means that per object the linked rules can be distributed on a maximum of 3 sets.</p>

<p>2-      If an object ID is linked to a rule set ID, we need to minimize the number of rules within the set that the object does not use (is not originally linked to) (our objective function).</p>

<p>3-      The number of rules per set should not exceed 250</p>

<p>4-      No restriction on the number of objects that can be linked to a rules set</p>

<p>5-      A rule can appear in more than one set (not optimal)</p>

<p>What is the best machine-learning algorithm to implement that?</p>

<p>What I thought of is agglomerative clustering where:</p>

<p>Nb of clusters=nb of rules/250 (although this is not always optimal)</p>

<p>Distance between 2 rules: nb of non-common objects the rules are linked to (the complement of their objects intersection )divided by the total number of objects they are both linked to</p>

<p>How then should I specify a condition while clustering that each object can be by maximum linked to 3 sets?</p>

<p>is it a clustering problem at all?</p>
","['constraints', 'form', 'firewall', 'security', 'intersection', 'bucket', 'rules', 'network', 'optimization', 'complement', 'grouping', 'a', 'python', 'machine-learning', 'distance', 'checked', 'minimize', 'distributed', 'database', 'exceed', 'machine learning']"
Control AWS Sagemaker costs,"<p>I want to use GPU capacity for deep learning models. Sagemaker is great in its flexibility of starting on demand clusters for training. However, my department wants to have guarantees we won't overspend on the AWS budget. Is there a way to 'cap' the costs without resorting to using a dedicated machine? </p>
","['models', 'cap', 'flexibility', 'amazon-sagemaker', 'amazon-web-services', 'a', 'gpu', 'capacity', 'machine learning', 'aws', 'dedicated']"
Control AWS Sagemaker costs,"<p>I want to use GPU capacity for deep learning models. Sagemaker is great in its flexibility of starting on demand clusters for training. However, my department wants to have guarantees we won't overspend on the AWS budget. Is there a way to 'cap' the costs without resorting to using a dedicated machine? </p>
","['models', 'cap', 'flexibility', 'amazon-sagemaker', 'amazon-web-services', 'a', 'gpu', 'capacity', 'machine learning', 'aws', 'dedicated']"
Run PyQt and Keras app developed on Mac on Windows,"<p>first of all I am not sure if this is the right place to ask my question. Please let me know if this is the wrong place and I will remove my question.</p>

<p>I am doing medical research and develop programs. I prefer to develop on a macbook and the app I am currently working on uses PyQt as a GUI. The app prepares a dataset for a machine learning (with keras) task (images).</p>

<p>Since a macbook is not strong enough to perform the machine learning task, we have a workstation running windows. My thought was to develop the app on a macbook and then run the app on the windows workstation. </p>

<p>My question is: what is the best workflow to get the app running on the windows machine with as little setup time as possible? Could docker be used for this ?</p>

<p>Any suggetions pointing me to the right direction are welcome!</p>

<p>Thanks for your help.</p>
","['dataset', 'docker', 'pyqt', 'a', 'medical', 'machine learning', 'app', 'python', 'machine-learning', 'keras']"
Run PyQt and Keras app developed on Mac on Windows,"<p>first of all I am not sure if this is the right place to ask my question. Please let me know if this is the wrong place and I will remove my question.</p>

<p>I am doing medical research and develop programs. I prefer to develop on a macbook and the app I am currently working on uses PyQt as a GUI. The app prepares a dataset for a machine learning (with keras) task (images).</p>

<p>Since a macbook is not strong enough to perform the machine learning task, we have a workstation running windows. My thought was to develop the app on a macbook and then run the app on the windows workstation. </p>

<p>My question is: what is the best workflow to get the app running on the windows machine with as little setup time as possible? Could docker be used for this ?</p>

<p>Any suggetions pointing me to the right direction are welcome!</p>

<p>Thanks for your help.</p>
","['dataset', 'docker', 'pyqt', 'a', 'medical', 'machine learning', 'app', 'python', 'machine-learning', 'keras']"
Can I access my MetaMask accounts on my projects and Display it?,"<p>Is it possible to display the account which I have logged-in in MetaMask? Also, Can I auto-fill the form-control whenever I log-in my MetaMask account? </p>
","['javascript', 'form-control', 'access', 'smartcontracts', 'metamask', 'accounts', 'ethereum', 'html', 'logged']"
Can I access my MetaMask accounts on my projects and Display it?,"<p>Is it possible to display the account which I have logged-in in MetaMask? Also, Can I auto-fill the form-control whenever I log-in my MetaMask account? </p>
","['javascript', 'form-control', 'access', 'smartcontracts', 'metamask', 'accounts', 'ethereum', 'html', 'logged']"
Is this good or bad someone can see web3.currentProvider inconsole?,"<p><a href=""https://i.stack.imgur.com/8PCw4.png"" rel=""nofollow noreferrer"">screenshot of console</a></p>

<p>in my website i'm using <strong>Laravel</strong> and <strong>VueJS</strong> for developing.</p>

<p>So when is type <code>web3.currentProvider</code> in console i can see my provider details. Is this good or bad.</p>

<p>Any way to hide it.</p>
","['php', 'laravel', 'vue.js', 'web3js', 'provider', 'ethereum', 'console', 'hide']"
Is this good or bad someone can see web3.currentProvider inconsole?,"<p><a href=""https://i.stack.imgur.com/8PCw4.png"" rel=""nofollow noreferrer"">screenshot of console</a></p>

<p>in my website i'm using <strong>Laravel</strong> and <strong>VueJS</strong> for developing.</p>

<p>So when is type <code>web3.currentProvider</code> in console i can see my provider details. Is this good or bad.</p>

<p>Any way to hide it.</p>
","['php', 'laravel', 'vue.js', 'web3js', 'provider', 'ethereum', 'console', 'hide']"
Debian 10 - VServer - Terminal zeigt nach Benutzerwechsel nur ein $-Zeichen,"<p>ich kenne mich noch nicht wirklich gut mit Debian VServern aus deshalb komme ich bei meinem Problem nicht weiter bzw. ich weiß nicht wie man das Problem behebt.</p>

<p>Ich habe einen VServer (Debian 10 - 64bit) wo ich gerne einen GameServer drauf laufen lassen würde. Das ganze ist auch kein Problem und habe ich auch schon mal hinbekommen, dass Problem was momentan besteht ist. Das ich - wenn ich einen neuen Benutzer mit ""useradd -m [Irgendein-Name]"" erstelle und zu diesem dann wechsel, zeigt er mir im Terminal nur ein ""$""-Zeichen an. Ich sehe nicht in welchem Directory ich mich befinde oder mit welchem Benutzer ich angemeldet bin.</p>

<p>Ich habe den Server vor kurzem neu aufsetzen müssen und habe genau so wie davor den Server wieder eingerichtet. Der Server ist geupdatet mit ""apt update &amp;&amp; apt upgrade"" und es sind keine neuen Services oder Librarys hinzugekommen. Als ich damals den Server eingerichtet hatte ging alles ohne Probleme da konnte ich auch nach dem Benutzerwechsel mehr sehen als nur ein ""$""-Zeichen.</p>

<p>Vielleicht kann mir ja wer helfen und sagen warum das passiert und wie ich es hin bekomme wieder den kompletten Pfad zu sehen. Als ""root"" Benutzer sehe ich den kompletten Pfad auch, sowie als wen ich mich eingeloggt habe.</p>
","['debian-buster', 'binance', 'services', 'terminal', 'der', 'linux', 'debian', 'mal', '64bit', 'apt']"
Debian 10 - VServer - Terminal zeigt nach Benutzerwechsel nur ein $-Zeichen,"<p>ich kenne mich noch nicht wirklich gut mit Debian VServern aus deshalb komme ich bei meinem Problem nicht weiter bzw. ich weiß nicht wie man das Problem behebt.</p>

<p>Ich habe einen VServer (Debian 10 - 64bit) wo ich gerne einen GameServer drauf laufen lassen würde. Das ganze ist auch kein Problem und habe ich auch schon mal hinbekommen, dass Problem was momentan besteht ist. Das ich - wenn ich einen neuen Benutzer mit ""useradd -m [Irgendein-Name]"" erstelle und zu diesem dann wechsel, zeigt er mir im Terminal nur ein ""$""-Zeichen an. Ich sehe nicht in welchem Directory ich mich befinde oder mit welchem Benutzer ich angemeldet bin.</p>

<p>Ich habe den Server vor kurzem neu aufsetzen müssen und habe genau so wie davor den Server wieder eingerichtet. Der Server ist geupdatet mit ""apt update &amp;&amp; apt upgrade"" und es sind keine neuen Services oder Librarys hinzugekommen. Als ich damals den Server eingerichtet hatte ging alles ohne Probleme da konnte ich auch nach dem Benutzerwechsel mehr sehen als nur ein ""$""-Zeichen.</p>

<p>Vielleicht kann mir ja wer helfen und sagen warum das passiert und wie ich es hin bekomme wieder den kompletten Pfad zu sehen. Als ""root"" Benutzer sehe ich den kompletten Pfad auch, sowie als wen ich mich eingeloggt habe.</p>
","['debian-buster', 'binance', 'services', 'terminal', 'der', 'linux', 'debian', 'mal', '64bit', 'apt']"
FFmpeg installation on Raspberry Pi Zero W: Undefined references to __atomic,"<p>I'm trying to install ffmpeg on my Raspberry Pi Zero W, but I get several error messages.</p>

<p>OS: Raspberry Pi OS (32-bit) Lite (May 2020)</p>

<p>I have executed the following commands:</p>

<pre><code>sudo apt update
sudo apt full-upgrade
sudo apt install git
git clone https://github.com/FFmpeg/FFmpeg.git
cd FFmpeg
./configure --arch=armel --target-os=linux --enable-gpl --enable-omx --enable-omx-rpi --enable-nonfree
make -j2
sudo make install
</code></pre>

<p>Output of last command:</p>

<p>...
LD      ffmpeg_g
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function <code>fifo_init':
/home/pi/FFmpeg/libavformat/fifo.c:519: undefined reference to</code>__atomic_store_8'
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function <code>fifo_write_trailer':
/home/pi/FFmpeg/libavformat/fifo.c:624: undefined reference to</code>__atomic_fetch_add_8'
/usr/bin/ld: /home/pi/FFmpeg/libavformat/fifo.c:631: undefined reference to <code>__atomic_store_8'
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function</code>fifo_thread_write_packet':
/home/pi/FFmpeg/libavformat/fifo.c:188: undefined reference to <code>__atomic_fetch_sub_8'
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function</code>fifo_consumer_thread':
/home/pi/FFmpeg/libavformat/fifo.c:457: undefined reference to <code>__atomic_load_8'
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function</code>fifo_write_packet':
/home/pi/FFmpeg/libavformat/fifo.c:597: undefined reference to `__atomic_fetch_add_8'
collect2: error: ld returned 1 exit status
make: *** [Makefile:114: ffmpeg_g] Error 1</p>

<p>Maybe another package is missing? Do I have to change anything in the config?</p>
","['install', 'package', 'gpl', 'linux', 'libavformat', '32-bit', 'cd', 'ld', 'config', 'a', 'fifo', 'clone', 'apt', 'raspberry-pi', 'configure', 'git', 'installation', 'arch', 'pi', 'c', 'makefile', 'binance', 'exit', 'github', 'ffmpeg']"
FFmpeg installation on Raspberry Pi Zero W: Undefined references to __atomic,"<p>I'm trying to install ffmpeg on my Raspberry Pi Zero W, but I get several error messages.</p>

<p>OS: Raspberry Pi OS (32-bit) Lite (May 2020)</p>

<p>I have executed the following commands:</p>

<pre><code>sudo apt update
sudo apt full-upgrade
sudo apt install git
git clone https://github.com/FFmpeg/FFmpeg.git
cd FFmpeg
./configure --arch=armel --target-os=linux --enable-gpl --enable-omx --enable-omx-rpi --enable-nonfree
make -j2
sudo make install
</code></pre>

<p>Output of last command:</p>

<p>...
LD      ffmpeg_g
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function <code>fifo_init':
/home/pi/FFmpeg/libavformat/fifo.c:519: undefined reference to</code>__atomic_store_8'
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function <code>fifo_write_trailer':
/home/pi/FFmpeg/libavformat/fifo.c:624: undefined reference to</code>__atomic_fetch_add_8'
/usr/bin/ld: /home/pi/FFmpeg/libavformat/fifo.c:631: undefined reference to <code>__atomic_store_8'
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function</code>fifo_thread_write_packet':
/home/pi/FFmpeg/libavformat/fifo.c:188: undefined reference to <code>__atomic_fetch_sub_8'
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function</code>fifo_consumer_thread':
/home/pi/FFmpeg/libavformat/fifo.c:457: undefined reference to <code>__atomic_load_8'
/usr/bin/ld: libavformat/libavformat.a(fifo.o): in function</code>fifo_write_packet':
/home/pi/FFmpeg/libavformat/fifo.c:597: undefined reference to `__atomic_fetch_add_8'
collect2: error: ld returned 1 exit status
make: *** [Makefile:114: ffmpeg_g] Error 1</p>

<p>Maybe another package is missing? Do I have to change anything in the config?</p>
","['install', 'package', 'gpl', 'linux', 'libavformat', '32-bit', 'cd', 'ld', 'config', 'a', 'fifo', 'clone', 'apt', 'raspberry-pi', 'configure', 'git', 'installation', 'arch', 'pi', 'c', 'makefile', 'binance', 'exit', 'github', 'ffmpeg']"
ISServerExec.exe crash when executing from SQL schedule job,"<p>I have ETL packages that works fine in Visual Studio 2019</p>

<p>When we deploy and run it from SQL Server Agent, it gives the error below</p>

<p>Additional info: the package is set to ""Encrypt sensitive with password"" and we set the connection password in the SQL Job step settings</p>

<p>Faulting application name: ISServerExec.exe, version: 14.0.1000.169, time stamp: 0x599ccc42</p>

<p>Faulting module name: **OraOLEDB11.DLL, version: 11.2.0.1, time stamp: 0x4b9a19db</p>

<p>Exception code: 0xc0000005</p>

<p>Fault offset: 0x000000000002c000</p>

<p>Faulting process id: 0x3954</p>

<p>Faulting application start time: 0x01d642753af07699</p>

<p>Faulting application path: c:\Program Files\Microsoft SQL Server\140\DTS\Binn\ISServerExec.exe</p>

<p>Faulting module path: C:\oracle\oracleclient64\bin\OraOLEDB11.DLL</p>

<p>Report Id: 0db0c583-e4aa-4dec-89f5-509f3969595c</p>

<p>Faulting package full name: </p>

<p>Faulting package-relative application ID: </p>
","['agent', 'ssis', 'c', 'oracle', 'password', 'binance', 'exception', 'package', 'sql-server-agent', 'sql', 'offset', 'dll', 'etl', 'packages', 'crash', 'sql-server-job', 'fault', 'dts', 'exe', 'connection']"
ISServerExec.exe crash when executing from SQL schedule job,"<p>I have ETL packages that works fine in Visual Studio 2019</p>

<p>When we deploy and run it from SQL Server Agent, it gives the error below</p>

<p>Additional info: the package is set to ""Encrypt sensitive with password"" and we set the connection password in the SQL Job step settings</p>

<p>Faulting application name: ISServerExec.exe, version: 14.0.1000.169, time stamp: 0x599ccc42</p>

<p>Faulting module name: **OraOLEDB11.DLL, version: 11.2.0.1, time stamp: 0x4b9a19db</p>

<p>Exception code: 0xc0000005</p>

<p>Fault offset: 0x000000000002c000</p>

<p>Faulting process id: 0x3954</p>

<p>Faulting application start time: 0x01d642753af07699</p>

<p>Faulting application path: c:\Program Files\Microsoft SQL Server\140\DTS\Binn\ISServerExec.exe</p>

<p>Faulting module path: C:\oracle\oracleclient64\bin\OraOLEDB11.DLL</p>

<p>Report Id: 0db0c583-e4aa-4dec-89f5-509f3969595c</p>

<p>Faulting package full name: </p>

<p>Faulting package-relative application ID: </p>
","['agent', 'ssis', 'c', 'oracle', 'password', 'binance', 'exception', 'package', 'sql-server-agent', 'sql', 'offset', 'dll', 'etl', 'packages', 'crash', 'sql-server-job', 'fault', 'dts', 'exe', 'connection']"
How to fix Next.js Vercel deployment module not found error,"<p>My next.js app works on my machine and was working when deployed on Vercel but now it fails when building on Vercel with the following error: </p>

<p>I've tried deleting node_modules and running <code>npm install</code> a few times but with no joy. </p>

<p>Any help would be hugely appreciated. Thank you!</p>

<blockquote>
  <p>Analyzing source code...
  21:30:52.642<br>
  Installing build runtime...
  21:30:53.159<br>
  Build runtime installed: 517.126ms
  21:30:53.683<br>
  Looking up build cache...
  21:30:53.718<br>
  Build cache not found
  21:30:54.237<br>
  WARNING: You should not upload the <code>.next</code> directory.
  21:30:54.241<br>
  Installing dependencies...
  21:31:01.088<br>
  npm WARN tsutils@3.17.1 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself.
  21:31:01.088<br>
  npm WARN tdwcks@1.0.0 No description
  21:31:01.088<br>
  npm WARN tdwcks@1.0.0 No repository field.
  21:31:01.089<br>
  up to date in 6.441s
  21:31:01.747<br>
  83 packages are looking for funding
  21:31:01.747<br>
    run <code>npm fund</code> for details
  21:31:01.764<br>
  Running ""npm run build""
  21:31:01.946<br>
  tdwcks@1.0.0 build /vercel/6c3ebd16
  21:31:01.946<br>
  next build
  21:31:01.988<br>
  internal/modules/cjs/loader.js:983
  21:31:01.988<br>
    throw err;
  21:31:01.988<br>
    ^
  21:31:01.988<br>
  Error: Cannot find module '../build/output/log'
  21:31:01.988<br>
  Require stack:
  21:31:01.988<br>
  - /vercel/6c3ebd16/node_modules/.bin/next
  21:31:01.988<br>
      at Function.Module._resolveFilename (internal/modules/cjs/loader.js:980:15)
  21:31:01.988<br>
      at Function.Module._load (internal/modules/cjs/loader.js:862:27)
  21:31:01.988<br>
      at Module.require (internal/modules/cjs/loader.js:1042:19)
  21:31:01.988<br>
      at require (internal/modules/cjs/helpers.js:77:18)
  21:31:01.989<br>
      at Object. (/vercel/6c3ebd16/node_modules/.bin/next:2:46)
  21:31:01.989<br>
      at Module._compile (internal/modules/cjs/loader.js:1156:30)
  21:31:01.989<br>
      at Object.Module._extensions..js (internal/modules/cjs/loader.js:1176:10)
  21:31:01.989<br>
      at Module.load (internal/modules/cjs/loader.js:1000:32)
  21:31:01.989<br>
      at Function.Module._load (internal/modules/cjs/loader.js:899:14)
  21:31:01.989<br>
      at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:74:12) {
  21:31:01.989<br>
    code: 'MODULE_NOT_FOUND',
  21:31:01.989<br>
    requireStack: [ '/vercel/6c3ebd16/node_modules/.bin/next' ]
  21:31:01.989<br>
  }
  21:31:02.000<br>
  npm ERR! code ELIFECYCLE
  21:31:02.000<br>
  npm ERR! errno 1
  21:31:02.000<br>
  npm ERR! tdwcks@1.0.0 build: <code>next build</code>
  21:31:02.000<br>
  npm ERR! Exit status 1
  21:31:02.000<br>
  npm ERR! 
  21:31:02.000<br>
  npm ERR! Failed at the tdwcks@1.0.0 build script.
  21:31:02.000<br>
  npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
  21:31:02.004<br>
  npm ERR! A complete log of this run can be found in:
  21:31:02.004<br>
  npm ERR!     /vercel/.npm/_logs/2020-06-14T20_31_02_000Z-debug.log
  21:31:02.008<br>
  Error: Command ""npm run build"" exited with 1
  21:31:03.187<br>
  Done with ""package.json""</p>
</blockquote>

<p>Here's my Package.json</p>

<pre><code>{
  ""name"": ""tdwcks"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""index.js"",
  ""scripts"": {
    ""dev"": ""next"",
    ""build"": ""next build"",
    ""start"": ""next start""
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""dependencies"": {
    ""cjs"": ""0.0.11"",
    ""core-util-is"": ""^1.0.2"",
    ""framer-motion"": ""^1.11.0"",
    ""gray-matter"": ""^4.0.2"",
    ""next"": ""^9.4.4"",
    ""raw-loader"": ""^4.0.1"",
    ""react"": ""^16.13.1"",
    ""react-dom"": ""^16.13.1"",
    ""react-ga"": ""^3.0.0"",
    ""react-markdown"": ""^4.3.1"",
    ""react-player"": ""^2.2.0"",
    ""react-scripts"": ""^3.4.1""
  },
  ""devDependencies"": {
    ""postcss-preset-env"": ""^6.7.0"",
    ""tailwindcss"": ""^1.4.6""
  }
}
</code></pre>
","['dependencies', 'motion', 'js', 'install', 'errno', 'helpers', 'postcss', 'npm', 'beta', 'app', 'raw-loader', 'react', 'preset', 'reactjs', 'logging', 'react-ga', 'a', 'next.js', 'react-dom', 'packages', 'package.json', 'vercel', 'build', 'index', 'markdown', 'typescript', 'loader', 'binance', 'peer', 'exit', 'next', 'deployment', 'env']"
How to fix Next.js Vercel deployment module not found error,"<p>My next.js app works on my machine and was working when deployed on Vercel but now it fails when building on Vercel with the following error: </p>

<p>I've tried deleting node_modules and running <code>npm install</code> a few times but with no joy. </p>

<p>Any help would be hugely appreciated. Thank you!</p>

<blockquote>
  <p>Analyzing source code...
  21:30:52.642<br>
  Installing build runtime...
  21:30:53.159<br>
  Build runtime installed: 517.126ms
  21:30:53.683<br>
  Looking up build cache...
  21:30:53.718<br>
  Build cache not found
  21:30:54.237<br>
  WARNING: You should not upload the <code>.next</code> directory.
  21:30:54.241<br>
  Installing dependencies...
  21:31:01.088<br>
  npm WARN tsutils@3.17.1 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself.
  21:31:01.088<br>
  npm WARN tdwcks@1.0.0 No description
  21:31:01.088<br>
  npm WARN tdwcks@1.0.0 No repository field.
  21:31:01.089<br>
  up to date in 6.441s
  21:31:01.747<br>
  83 packages are looking for funding
  21:31:01.747<br>
    run <code>npm fund</code> for details
  21:31:01.764<br>
  Running ""npm run build""
  21:31:01.946<br>
  tdwcks@1.0.0 build /vercel/6c3ebd16
  21:31:01.946<br>
  next build
  21:31:01.988<br>
  internal/modules/cjs/loader.js:983
  21:31:01.988<br>
    throw err;
  21:31:01.988<br>
    ^
  21:31:01.988<br>
  Error: Cannot find module '../build/output/log'
  21:31:01.988<br>
  Require stack:
  21:31:01.988<br>
  - /vercel/6c3ebd16/node_modules/.bin/next
  21:31:01.988<br>
      at Function.Module._resolveFilename (internal/modules/cjs/loader.js:980:15)
  21:31:01.988<br>
      at Function.Module._load (internal/modules/cjs/loader.js:862:27)
  21:31:01.988<br>
      at Module.require (internal/modules/cjs/loader.js:1042:19)
  21:31:01.988<br>
      at require (internal/modules/cjs/helpers.js:77:18)
  21:31:01.989<br>
      at Object. (/vercel/6c3ebd16/node_modules/.bin/next:2:46)
  21:31:01.989<br>
      at Module._compile (internal/modules/cjs/loader.js:1156:30)
  21:31:01.989<br>
      at Object.Module._extensions..js (internal/modules/cjs/loader.js:1176:10)
  21:31:01.989<br>
      at Module.load (internal/modules/cjs/loader.js:1000:32)
  21:31:01.989<br>
      at Function.Module._load (internal/modules/cjs/loader.js:899:14)
  21:31:01.989<br>
      at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:74:12) {
  21:31:01.989<br>
    code: 'MODULE_NOT_FOUND',
  21:31:01.989<br>
    requireStack: [ '/vercel/6c3ebd16/node_modules/.bin/next' ]
  21:31:01.989<br>
  }
  21:31:02.000<br>
  npm ERR! code ELIFECYCLE
  21:31:02.000<br>
  npm ERR! errno 1
  21:31:02.000<br>
  npm ERR! tdwcks@1.0.0 build: <code>next build</code>
  21:31:02.000<br>
  npm ERR! Exit status 1
  21:31:02.000<br>
  npm ERR! 
  21:31:02.000<br>
  npm ERR! Failed at the tdwcks@1.0.0 build script.
  21:31:02.000<br>
  npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
  21:31:02.004<br>
  npm ERR! A complete log of this run can be found in:
  21:31:02.004<br>
  npm ERR!     /vercel/.npm/_logs/2020-06-14T20_31_02_000Z-debug.log
  21:31:02.008<br>
  Error: Command ""npm run build"" exited with 1
  21:31:03.187<br>
  Done with ""package.json""</p>
</blockquote>

<p>Here's my Package.json</p>

<pre><code>{
  ""name"": ""tdwcks"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""index.js"",
  ""scripts"": {
    ""dev"": ""next"",
    ""build"": ""next build"",
    ""start"": ""next start""
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""dependencies"": {
    ""cjs"": ""0.0.11"",
    ""core-util-is"": ""^1.0.2"",
    ""framer-motion"": ""^1.11.0"",
    ""gray-matter"": ""^4.0.2"",
    ""next"": ""^9.4.4"",
    ""raw-loader"": ""^4.0.1"",
    ""react"": ""^16.13.1"",
    ""react-dom"": ""^16.13.1"",
    ""react-ga"": ""^3.0.0"",
    ""react-markdown"": ""^4.3.1"",
    ""react-player"": ""^2.2.0"",
    ""react-scripts"": ""^3.4.1""
  },
  ""devDependencies"": {
    ""postcss-preset-env"": ""^6.7.0"",
    ""tailwindcss"": ""^1.4.6""
  }
}
</code></pre>
","['dependencies', 'motion', 'js', 'install', 'errno', 'helpers', 'postcss', 'npm', 'beta', 'app', 'raw-loader', 'react', 'preset', 'reactjs', 'logging', 'react-ga', 'a', 'next.js', 'react-dom', 'packages', 'package.json', 'vercel', 'build', 'index', 'markdown', 'typescript', 'loader', 'binance', 'peer', 'exit', 'next', 'deployment', 'env']"
Unable to determine Hadoop version information. &#39;hadoop version&#39; returned:,"<p>I have installed hive 3.1.2_1 on my Mac OS Catalina using brew.
Now when I try to run $hive from the bin directory it gives me, </p>

<blockquote>
  <p>Unable to determine Hadoop version information. 'hadoop version' returned:</p>
</blockquote>

<p>As per the <a href=""https://stackoverflow.com/questions/18559555/unable-to-determine-hadoop-version-information"">Unable to determine Hadoop version information</a>, I updated my  Hadoop version to <code>HADOOP_VERSION=Hadoop 3.2.1</code> in my <code>bashrc</code> file. </p>

<p>This still hasn't resolved my issue. </p>
","['hadoop', 'macos', 'hive', 'binance', 'catalina']"
Unable to determine Hadoop version information. &#39;hadoop version&#39; returned:,"<p>I have installed hive 3.1.2_1 on my Mac OS Catalina using brew.
Now when I try to run $hive from the bin directory it gives me, </p>

<blockquote>
  <p>Unable to determine Hadoop version information. 'hadoop version' returned:</p>
</blockquote>

<p>As per the <a href=""https://stackoverflow.com/questions/18559555/unable-to-determine-hadoop-version-information"">Unable to determine Hadoop version information</a>, I updated my  Hadoop version to <code>HADOOP_VERSION=Hadoop 3.2.1</code> in my <code>bashrc</code> file. </p>

<p>This still hasn't resolved my issue. </p>
","['hadoop', 'macos', 'hive', 'binance', 'catalina']"
"in Rust, include a module from src/bin directory","<p>How can I import a module from a <em>program</em> which is in <em>src/bin</em> directory?</p>

<p>Currently, I have a directory structure like this:</p>

<pre><code>my_program
--src
----bin
------program.rs
----foo
-------bar.rs
----foo.rs
</code></pre>

<p>in <em>foo.rs</em>, I have:</p>

<pre><code>mod bar;
pub use self::bar::Bar;
</code></pre>

<p>In <em>bar.rs</em>:</p>

<pre><code>pub struct Bar;
impl Bar{
    pub fn hello(){
        println!(""Hello from bar"");
    }
}
</code></pre>

<p>The question is, in <em>program.rs</em>, how I can import the struct <strong>Bar</strong>, because:</p>

<pre><code>mod foo;
use foo::Bar

fn main(){
    Bar::hello();
}
</code></pre>

<p>Does not seems to be work.</p>

<p>Can any body help me with this?</p>
","['binance', 'a', 'rust', 'import', 'println', 'mod', 'fn']"
"in Rust, include a module from src/bin directory","<p>How can I import a module from a <em>program</em> which is in <em>src/bin</em> directory?</p>

<p>Currently, I have a directory structure like this:</p>

<pre><code>my_program
--src
----bin
------program.rs
----foo
-------bar.rs
----foo.rs
</code></pre>

<p>in <em>foo.rs</em>, I have:</p>

<pre><code>mod bar;
pub use self::bar::Bar;
</code></pre>

<p>In <em>bar.rs</em>:</p>

<pre><code>pub struct Bar;
impl Bar{
    pub fn hello(){
        println!(""Hello from bar"");
    }
}
</code></pre>

<p>The question is, in <em>program.rs</em>, how I can import the struct <strong>Bar</strong>, because:</p>

<pre><code>mod foo;
use foo::Bar

fn main(){
    Bar::hello();
}
</code></pre>

<p>Does not seems to be work.</p>

<p>Can any body help me with this?</p>
","['binance', 'a', 'rust', 'import', 'println', 'mod', 'fn']"
Is it possible to use primary key along with secondary key in Aerospike for fetching a record?,"<p>Is it possible to use primary key along with secondary key. I want to get all records on the basis of primary key and then apply secondary index over it? Is it the correct way to do it or some other approach should be used?</p>

<p>I have a bin with primary key. Once i fetch the record using primary key, i want to get only the subset of the record. 
Below is the record example. Also is there a better way to organise the record? Like keeping manager, supervisor and otherStaff as the key and then querying.</p>

<pre><code> {
    ""name"" : ""test"",
    ""companyName"" : ""company1"",
    ""details"" : [{
           ""salary"" : 1000,
           ""designation"" : ""manager""
         },
         {
           ""salary"" : 2000,
           ""designation"" : ""supervisor""
         },
         {
           ""staff"" : 500,
           ""designation"" : ""otherStaff""
         }]
} 
</code></pre>
","['key', 'aerospike', 'binance', 'aerospike-ce', 'index', 'a', 'aerospike-loader', 'querying']"
Is it possible to use primary key along with secondary key in Aerospike for fetching a record?,"<p>Is it possible to use primary key along with secondary key. I want to get all records on the basis of primary key and then apply secondary index over it? Is it the correct way to do it or some other approach should be used?</p>

<p>I have a bin with primary key. Once i fetch the record using primary key, i want to get only the subset of the record. 
Below is the record example. Also is there a better way to organise the record? Like keeping manager, supervisor and otherStaff as the key and then querying.</p>

<pre><code> {
    ""name"" : ""test"",
    ""companyName"" : ""company1"",
    ""details"" : [{
           ""salary"" : 1000,
           ""designation"" : ""manager""
         },
         {
           ""salary"" : 2000,
           ""designation"" : ""supervisor""
         },
         {
           ""staff"" : 500,
           ""designation"" : ""otherStaff""
         }]
} 
</code></pre>
","['key', 'aerospike', 'binance', 'aerospike-ce', 'index', 'a', 'aerospike-loader', 'querying']"
Easy question regarding how to access the return of a function,"<p>I'm new to python/programming so this may be a simple solution. I'm just trying to figure out how to output the function?</p>

<p>I use the following:</p>

<pre><code>import time
from binance.client import Client
from datetime import datetime

def process_message(msg):
    print(""message type: {}"".format(msg['e']))
    print(msg)
    var1 = msg['s']
    var2 = msg['p']
    print(var1 + var2)
    return(var1)

from binance.websockets import BinanceSocketManager
bm = BinanceSocketManager(client)
bm.start_trade_socket('BNBBTC', process_message)
bm.start()
</code></pre>

<p>At this point, the websocket starts streaming data as expected. </p>

<p>So i can see the results of the function if i call it from inside the function, but i receive an error if i try and call it like this (outside of the function):</p>

<pre><code>print(process_message)
</code></pre>

<p>I receive the following:
function process_message at 0x03A919B8</p>

<p>If i call the function on it's own:</p>

<pre><code>process_message()
</code></pre>

<p>I receive: ""process_message() missing 1 required positional argument: 'msg'""</p>

<p>If i call the function with the argument:</p>

<pre><code>process_message(msg)
</code></pre>

<p>I get: name 'msg' is not defined</p>

<p>What am i doing wrong? How would i go about accessing the data outside of the function?</p>

<p>Any help or clarity would be appreciated,</p>

<p>Many Thanks,</p>
","['argument', 'access', 'clarity', 'format', 'go', 'binance', 'a', 'websocket', 'import', 'function', 'python', 'datetime']"
Easy question regarding how to access the return of a function,"<p>I'm new to python/programming so this may be a simple solution. I'm just trying to figure out how to output the function?</p>

<p>I use the following:</p>

<pre><code>import time
from binance.client import Client
from datetime import datetime

def process_message(msg):
    print(""message type: {}"".format(msg['e']))
    print(msg)
    var1 = msg['s']
    var2 = msg['p']
    print(var1 + var2)
    return(var1)

from binance.websockets import BinanceSocketManager
bm = BinanceSocketManager(client)
bm.start_trade_socket('BNBBTC', process_message)
bm.start()
</code></pre>

<p>At this point, the websocket starts streaming data as expected. </p>

<p>So i can see the results of the function if i call it from inside the function, but i receive an error if i try and call it like this (outside of the function):</p>

<pre><code>print(process_message)
</code></pre>

<p>I receive the following:
function process_message at 0x03A919B8</p>

<p>If i call the function on it's own:</p>

<pre><code>process_message()
</code></pre>

<p>I receive: ""process_message() missing 1 required positional argument: 'msg'""</p>

<p>If i call the function with the argument:</p>

<pre><code>process_message(msg)
</code></pre>

<p>I get: name 'msg' is not defined</p>

<p>What am i doing wrong? How would i go about accessing the data outside of the function?</p>

<p>Any help or clarity would be appreciated,</p>

<p>Many Thanks,</p>
","['argument', 'access', 'clarity', 'format', 'go', 'binance', 'a', 'websocket', 'import', 'function', 'python', 'datetime']"
Can this function be optimized to run in O(n) time using Python?,"<p>Here's the scenario.</p>

<p>The function takes in an array of n item weights and an array of q capacities. The objective is to find the number of items that each bin can hold, depending on its capacity.</p>

<p>I've written the following function but the problem I'm having is that it's timing out on very large input values. Check it out below:</p>

<pre><code>def noItems(weights, capacities):
    number_of_items = 0
    result = []
    weight_sums = [sum(weights[0:w:1]) for w in range(1, len(weights) + 1)]

    for i in range(0, len(capacities)):
        for j in range(0, len(weight_sums)):
            if weight_sums[j] &lt;= capacities[i]:              
                number_of_items = number_of_items + 1

        result.append(number_of_items)

        number_of_items = 0

    return(result)
</code></pre>

<p>Update: sample input and output</p>

<p>input weights: [2, 3, 5, 8, 1, 4, 7]</p>

<p>input capacities: [10, 20, 18, 1, 40, 4]</p>

<p>input constraints:
weights[i] > 1 and &lt; 1000
capacities[i] > 1 and &lt; 10^9</p>

<p>output: [3, 5, 4, 0, 7, 1]</p>

<p>How can this function be optimized to have a faster runtime, so that it doesn't time out on very large inputs?</p>
","['append', 'constraints', 'optimization', 'binance', 'python-3.x', 'algorithm', 'time-complexity', 'a', 'range', 'capacity', 'python']"
Can this function be optimized to run in O(n) time using Python?,"<p>Here's the scenario.</p>

<p>The function takes in an array of n item weights and an array of q capacities. The objective is to find the number of items that each bin can hold, depending on its capacity.</p>

<p>I've written the following function but the problem I'm having is that it's timing out on very large input values. Check it out below:</p>

<pre><code>def noItems(weights, capacities):
    number_of_items = 0
    result = []
    weight_sums = [sum(weights[0:w:1]) for w in range(1, len(weights) + 1)]

    for i in range(0, len(capacities)):
        for j in range(0, len(weight_sums)):
            if weight_sums[j] &lt;= capacities[i]:              
                number_of_items = number_of_items + 1

        result.append(number_of_items)

        number_of_items = 0

    return(result)
</code></pre>

<p>Update: sample input and output</p>

<p>input weights: [2, 3, 5, 8, 1, 4, 7]</p>

<p>input capacities: [10, 20, 18, 1, 40, 4]</p>

<p>input constraints:
weights[i] > 1 and &lt; 1000
capacities[i] > 1 and &lt; 10^9</p>

<p>output: [3, 5, 4, 0, 7, 1]</p>

<p>How can this function be optimized to have a faster runtime, so that it doesn't time out on very large inputs?</p>
","['append', 'constraints', 'optimization', 'binance', 'python-3.x', 'algorithm', 'time-complexity', 'a', 'range', 'capacity', 'python']"
How do I fix syntax error using Shrimpy API?,"<p>So I am trying to link my binance account using the Shrimpy API. However, I keep getting a syntax error and I don't understand why. VS code says that this line of code is invalid syntax however that is definitely a shrimpy method and I have the latest versions installed. Any help is appreciated!</p>

<pre><code>link_account_response = client.link_account(
    user_id,
    exchange_name,
    exchange_public_key,
    exchange_secret_key
    )
</code></pre>

<p>Syntax Error </p>

<pre><code> File ""/Users/myName/Desktop/Random projects /Binance Bot /priceTick.py"", line 14
    link_account_response = client.link_account(
                        ^
SyntaxError: invalid syntax
</code></pre>
","['desktop', 'cryptocurrency', 'binance', 'algorithmic-trading', 'random', 'a', 'bot', 'python', 'api']"
How do I fix syntax error using Shrimpy API?,"<p>So I am trying to link my binance account using the Shrimpy API. However, I keep getting a syntax error and I don't understand why. VS code says that this line of code is invalid syntax however that is definitely a shrimpy method and I have the latest versions installed. Any help is appreciated!</p>

<pre><code>link_account_response = client.link_account(
    user_id,
    exchange_name,
    exchange_public_key,
    exchange_secret_key
    )
</code></pre>

<p>Syntax Error </p>

<pre><code> File ""/Users/myName/Desktop/Random projects /Binance Bot /priceTick.py"", line 14
    link_account_response = client.link_account(
                        ^
SyntaxError: invalid syntax
</code></pre>
","['desktop', 'cryptocurrency', 'binance', 'algorithmic-trading', 'random', 'a', 'bot', 'python', 'api']"
collect2: error: ld returned 1 exit status undefined references,"<p>This is my code for an online shopping cart containing two items. 
I'm getting the following linker error. </p>

<p>c:/mingw/bin/../lib/gcc/mingw32/9.2.0/../../../../mingw32/bin/ld.exe: C:\Users\chine\AppData\Local\Temp\ccQnOWnO.o:main.cpp:(.text+0x121): undefined reference to `ItemToPurchase::SetPrice(int)</p>

<p>I get similar errors to all my functions in my public class. I double-checked all my parameters and made sure they had the matching data types and made sure I'd defined my functions. Here is my code. I'm fairly new to C++ so I'm just looking to learn and make sure this doesn't happen again. </p>

<p>ItemToPurchase.cpp</p>

<pre><code>#include ""ItemToPurchase.h""

ItemToPurchase::ItemToPurchase() {
   string itemName = ""none""; 
   int itemPrice = 0; 
   int itemQuantity = 0;
}

string ItemToPurchase::GetName() {
   return itemName; 
}

int ItemToPurchase::GetPrice() {
   return itemPrice; 
}

int ItemToPurchase::GetQuantity() {
   return itemQuantity; 
}

void ItemToPurchase::SetName(const char* itemName) {
   this-&gt;itemName = itemName;
}

void ItemToPurchase::SetPrice(int price) {
   itemPrice = price; 
}

void ItemToPurchase::SetQuantity(int quantity) {
   itemQuantity = quantity; 
}
</code></pre>

<p>main.c</p>

<pre><code>#include ""ItemToPurchase.h""

int main() {
    ItemToPurchase Item1;  
    ItemToPurchase Item2; 
    string item1name; 
    int item1price;
    int item1quantity; 
    string item2name; 
    int item2price; 
    int item2quantity; 

    cout &lt;&lt; ""Item 1""; 
    cout &lt;&lt; ""Enter the item name: ""; 
    getline(cin, item1name);

    item1name = Item1.GetName(); 
    Item1.SetName(item1name.c_str()); 

    cout &lt;&lt; ""Enter the item price: "";
    cin &gt;&gt; item1price;

    item1price = Item1.GetPrice(); 
    Item1.SetPrice(item1price);

    cout &lt;&lt; ""Enter the item quantity: "";
    cin &gt;&gt; item1quantity; 

    item1quantity = Item1.GetQuantity();
    Item1.SetQuantity(item1quantity);

    cout &lt;&lt; ""Item 2"";
    cout &lt;&lt; ""Enter the item name: "";
    getline(cin, item2name); 

    item2name = Item2.GetName();
    Item2.SetName(item2name.c_str()); 

    cout &lt;&lt; ""Enter the item price: "";
    cin &gt;&gt; item2price; 

    item2price = Item2.GetPrice();
    Item2.SetPrice(item2price); 

    cout &lt;&lt; ""Enter the item quantity: "";
    cin &gt;&gt; item2quantity; 

    item2quantity = Item2.GetQuantity();
    Item2.SetQuantity(item2quantity);


    cout &lt;&lt; ""TOTAL COST"" &lt;&lt; endl; 
    cout &lt;&lt; item1name &lt;&lt; item1quantity &lt;&lt; ""@ "" &lt;&lt; ""$"" &lt;&lt; item1price &lt;&lt; ""= "" &lt;&lt; ""$"" &lt;&lt; item1price * item1quantity &lt;&lt; endl; 
    cout &lt;&lt; item2name &lt;&lt; item2quantity &lt;&lt; ""@ "" &lt;&lt; ""$"" &lt;&lt; item2price &lt;&lt; ""= "" &lt;&lt; ""$"" &lt;&lt; item2price * item2quantity &lt;&lt; endl;

    cout &lt;&lt; ""TOTAL: "" &lt;&lt; ""$"" &lt;&lt; (item1price * item1quantity) + (item2price * item2quantity) &lt;&lt; endl;


    return 0; 

}
</code></pre>
","['mingw', 'cart', 'linker-errors', 'appdata', 'getline', 'ld', 'functions', 'lib', 'linker', 'const', 'matching', 'cpp', 'c++', 'checked', 'c', 'endl', 'mingw32', 'cin', 'binance', 'exit', 'gcc', 'parameters', 'cout', 'local', 'exe']"
collect2: error: ld returned 1 exit status undefined references,"<p>This is my code for an online shopping cart containing two items. 
I'm getting the following linker error. </p>

<p>c:/mingw/bin/../lib/gcc/mingw32/9.2.0/../../../../mingw32/bin/ld.exe: C:\Users\chine\AppData\Local\Temp\ccQnOWnO.o:main.cpp:(.text+0x121): undefined reference to `ItemToPurchase::SetPrice(int)</p>

<p>I get similar errors to all my functions in my public class. I double-checked all my parameters and made sure they had the matching data types and made sure I'd defined my functions. Here is my code. I'm fairly new to C++ so I'm just looking to learn and make sure this doesn't happen again. </p>

<p>ItemToPurchase.cpp</p>

<pre><code>#include ""ItemToPurchase.h""

ItemToPurchase::ItemToPurchase() {
   string itemName = ""none""; 
   int itemPrice = 0; 
   int itemQuantity = 0;
}

string ItemToPurchase::GetName() {
   return itemName; 
}

int ItemToPurchase::GetPrice() {
   return itemPrice; 
}

int ItemToPurchase::GetQuantity() {
   return itemQuantity; 
}

void ItemToPurchase::SetName(const char* itemName) {
   this-&gt;itemName = itemName;
}

void ItemToPurchase::SetPrice(int price) {
   itemPrice = price; 
}

void ItemToPurchase::SetQuantity(int quantity) {
   itemQuantity = quantity; 
}
</code></pre>

<p>main.c</p>

<pre><code>#include ""ItemToPurchase.h""

int main() {
    ItemToPurchase Item1;  
    ItemToPurchase Item2; 
    string item1name; 
    int item1price;
    int item1quantity; 
    string item2name; 
    int item2price; 
    int item2quantity; 

    cout &lt;&lt; ""Item 1""; 
    cout &lt;&lt; ""Enter the item name: ""; 
    getline(cin, item1name);

    item1name = Item1.GetName(); 
    Item1.SetName(item1name.c_str()); 

    cout &lt;&lt; ""Enter the item price: "";
    cin &gt;&gt; item1price;

    item1price = Item1.GetPrice(); 
    Item1.SetPrice(item1price);

    cout &lt;&lt; ""Enter the item quantity: "";
    cin &gt;&gt; item1quantity; 

    item1quantity = Item1.GetQuantity();
    Item1.SetQuantity(item1quantity);

    cout &lt;&lt; ""Item 2"";
    cout &lt;&lt; ""Enter the item name: "";
    getline(cin, item2name); 

    item2name = Item2.GetName();
    Item2.SetName(item2name.c_str()); 

    cout &lt;&lt; ""Enter the item price: "";
    cin &gt;&gt; item2price; 

    item2price = Item2.GetPrice();
    Item2.SetPrice(item2price); 

    cout &lt;&lt; ""Enter the item quantity: "";
    cin &gt;&gt; item2quantity; 

    item2quantity = Item2.GetQuantity();
    Item2.SetQuantity(item2quantity);


    cout &lt;&lt; ""TOTAL COST"" &lt;&lt; endl; 
    cout &lt;&lt; item1name &lt;&lt; item1quantity &lt;&lt; ""@ "" &lt;&lt; ""$"" &lt;&lt; item1price &lt;&lt; ""= "" &lt;&lt; ""$"" &lt;&lt; item1price * item1quantity &lt;&lt; endl; 
    cout &lt;&lt; item2name &lt;&lt; item2quantity &lt;&lt; ""@ "" &lt;&lt; ""$"" &lt;&lt; item2price &lt;&lt; ""= "" &lt;&lt; ""$"" &lt;&lt; item2price * item2quantity &lt;&lt; endl;

    cout &lt;&lt; ""TOTAL: "" &lt;&lt; ""$"" &lt;&lt; (item1price * item1quantity) + (item2price * item2quantity) &lt;&lt; endl;


    return 0; 

}
</code></pre>
","['mingw', 'cart', 'linker-errors', 'appdata', 'getline', 'ld', 'functions', 'lib', 'linker', 'const', 'matching', 'cpp', 'c++', 'checked', 'c', 'endl', 'mingw32', 'cin', 'binance', 'exit', 'gcc', 'parameters', 'cout', 'local', 'exe']"
How to get search keyword SUM value in cloud watch both Dashboard(Number widget) and Insight visualization from log group (application log)?,"<ol>
<li>How to sum with search keyword(Authentication Success) in Dashbaord (Number Widget) from log group{application log}.</li>
<li>How to insight sum value show in visualizarion.</li>
</ol>

<p><strong>Query:</strong></p>

<p>fields @timestamp, @message
| filter @message like /(?=.<em>Authentication Success</em>)/
| stats count() by bin(1h)
| sort @timestamp desc  </p>

<p><strong>Sample Application log :</strong> </p>

<p>1</p>

<p>[2020-06-15 05:02:20] AuthLogging.INFO: Log message {""message"":""Authentication Success""} </p>

<p>2</p>

<p>[2020-06-15 05:02:20] AuthLogging.INFO: Log message {""message"":""Authentication Success""} </p>

<p>3</p>

<p>[2020-06-15 05:02:20] AuthLogging.INFO: Log message {""message"":""Authentication Failed""} </p>
","['binance', 'aws-cloudwatch-log-insights', 'insight', 'amazon-cloudwatchlogs', 'amazon-web-services', 'query', 'cloud', 'filter', 'amazon-cloudwatch', 'authentication', 'dashboard']"
How to get search keyword SUM value in cloud watch both Dashboard(Number widget) and Insight visualization from log group (application log)?,"<ol>
<li>How to sum with search keyword(Authentication Success) in Dashbaord (Number Widget) from log group{application log}.</li>
<li>How to insight sum value show in visualizarion.</li>
</ol>

<p><strong>Query:</strong></p>

<p>fields @timestamp, @message
| filter @message like /(?=.<em>Authentication Success</em>)/
| stats count() by bin(1h)
| sort @timestamp desc  </p>

<p><strong>Sample Application log :</strong> </p>

<p>1</p>

<p>[2020-06-15 05:02:20] AuthLogging.INFO: Log message {""message"":""Authentication Success""} </p>

<p>2</p>

<p>[2020-06-15 05:02:20] AuthLogging.INFO: Log message {""message"":""Authentication Success""} </p>

<p>3</p>

<p>[2020-06-15 05:02:20] AuthLogging.INFO: Log message {""message"":""Authentication Failed""} </p>
","['binance', 'aws-cloudwatch-log-insights', 'insight', 'amazon-cloudwatchlogs', 'amazon-web-services', 'query', 'cloud', 'filter', 'amazon-cloudwatch', 'authentication', 'dashboard']"
Visual Studio Code Failed to Start At Debug for dotnet core project,"<p>My project which consists of Angular 8 and .net core cannot start in debug mode in Visual Studio Code. It start for a short time like a second and fail. How can i fix this?</p>

<p>task.json</p>

<hr>

<p>{
    ""version"": ""2.0.0"",
    ""tasks"": [
        {
            ""label"": ""build"",
            ""command"": ""dotnet"",
            ""type"": ""process"",
            ""args"": [
                ""build"",
                ""${workspaceFolder}/X.csproj"",
                ""/property:GenerateFullPaths=true"",
                ""/consoleloggerparameters:NoSummary""
            ],
            ""problemMatcher"": ""$msCompile""
        },
        {
            ""label"": ""publish"",
            ""command"": ""dotnet"",
            ""type"": ""process"",
            ""args"": [
                ""publish"",
                ""${workspaceFolder}/X.csproj"",
                ""/property:GenerateFullPaths=true"",
                ""/consoleloggerparameters:NoSummary""
            ],
            ""problemMatcher"": ""$msCompile""
        },
        {
            ""label"": ""watch"",
            ""command"": ""dotnet"",
            ""type"": ""process"",
            ""args"": [
                ""watch"",
                ""run"",
                ""${workspaceFolder}/X.csproj"",
                ""/property:GenerateFullPaths=true"",
                ""/consoleloggerparameters:NoSummary""
            ],
            ""problemMatcher"": ""$msCompile""
        }
    ]
}</p>

<p>launch.json</p>

<hr>

<p>{
    ""version"": ""0.2.0"",
    ""configurations"": [
        {
            ""name"": "".NET Core Launch (web)"",
            ""type"": ""coreclr"",
            ""request"": ""launch"",
            ""preLaunchTask"": ""build"",
            ""program"": ""${workspaceFolder}/bin/Debug/netcoreapp3.1/X.dll"",
            ""args"": [],
            ""cwd"": ""${workspaceFolder}"",
            ""stopAtEntry"": false,
            ""serverReadyAction"": {
                ""action"": ""openExternally"",
                ""pattern"": ""^\s*Now listening on:\s+(https?://\S+)""
            },
            ""env"": {
                ""ASPNETCORE_ENVIRONMENT"": ""Development""
            },
            ""sourceFileMap"": {
                ""/Views"": ""${workspaceFolder}/Views""
            }
        },
        {
            ""name"": "".NET Core Attach"",
            ""type"": ""coreclr"",
            ""request"": ""attach"",
            ""processId"": ""${command:pickProcess}""
        }
    ]
}</p>

<p>Output Logs:</p>

<hr>

<p>[2020-06-15 09:49:00.672] [exthost] [info] extension host started
[2020-06-15 09:49:00.926] [exthost] [info] ExtensionService#_doActivateExtension ms-dotnettools.csharp {""startup"":false,""extensionId"":{""value"":""ms-dotnettools.csharp"",""_lower"":""ms-dotnettools.csharp""},""activationEvent"":""onLanguage:csharp""}
[2020-06-15 09:49:00.926] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/ms-dotnettools.csharp-1.22.0/dist/extension
[2020-06-15 09:49:01.420] [exthost] [info] ExtensionService#_doActivateExtension vscode.debug-auto-launch {""startup"":true,""extensionId"":{""value"":""vscode.debug-auto-launch"",""_lower"":""vscode.debug-auto-launch""},""activationEvent"":""<em>""}
[2020-06-15 09:49:01.420] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/debug-auto-launch/dist/extension
[2020-06-15 09:49:01.431] [exthost] [info] ExtensionService#_doActivateExtension vscode.emmet {""startup"":true,""extensionId"":{""value"":""vscode.emmet"",""_lower"":""vscode.emmet""},""activationEvent"":""</em>""}
[2020-06-15 09:49:01.431] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/emmet/dist/extension
[2020-06-15 09:49:01.459] [exthost] [info] ExtensionService#_doActivateExtension vscode.git {""startup"":true,""extensionId"":{""value"":""vscode.git"",""_lower"":""vscode.git""},""activationEvent"":""<em>""}
[2020-06-15 09:49:01.459] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/git/dist/main
[2020-06-15 09:49:01.515] [exthost] [info] ExtensionService#_doActivateExtension vscode.github-authentication {""startup"":true,""extensionId"":{""value"":""vscode.github-authentication"",""_lower"":""vscode.github-authentication""},""activationEvent"":""</em>""}
[2020-06-15 09:49:01.515] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/github-authentication/dist/extension.js
[2020-06-15 09:49:01.536] [exthost] [info] ExtensionService#_doActivateExtension vscode.merge-conflict {""startup"":true,""extensionId"":{""value"":""vscode.merge-conflict"",""_lower"":""vscode.merge-conflict""},""activationEvent"":""<em>""}
[2020-06-15 09:49:01.536] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/merge-conflict/dist/extension
[2020-06-15 09:49:01.547] [exthost] [info] ExtensionService#_doActivateExtension vscode.search-result {""startup"":true,""extensionId"":{""value"":""vscode.search-result"",""_lower"":""vscode.search-result""},""activationEvent"":""</em>""}
[2020-06-15 09:49:01.547] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/search-result/dist/extension.js
[2020-06-15 09:49:01.552] [exthost] [info] ExtensionService#_doActivateExtension vscode.vscode-account {""startup"":true,""extensionId"":{""value"":""vscode.vscode-account"",""_lower"":""vscode.vscode-account""},""activationEvent"":""<em>""}
[2020-06-15 09:49:01.552] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/vscode-account/dist/extension.js
[2020-06-15 09:49:01.573] [exthost] [info] ExtensionService#_doActivateExtension donjayamanne.githistory {""startup"":true,""extensionId"":{""value"":""donjayamanne.githistory"",""_lower"":""donjayamanne.githistory""},""activationEvent"":""</em>""}
[2020-06-15 09:49:01.573] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/donjayamanne.githistory-0.6.5/dist/src/extension
[2020-06-15 09:49:01.826] [exthost] [info] ExtensionService#_doActivateExtension HookyQR.minify {""startup"":true,""extensionId"":{""value"":""HookyQR.minify"",""_lower"":""hookyqr.minify""},""activationEvent"":""*""}
[2020-06-15 09:49:01.826] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/hookyqr.minify-0.4.3/extension
[2020-06-15 09:49:03.926] [exthost] [info] eager extensions activated
[2020-06-15 09:49:57.031] [exthost] [info] ExtensionService#_doActivateExtension vscode.configuration-editing {""startup"":false,""extensionId"":{""value"":""vscode.configuration-editing"",""_lower"":""vscode.configuration-editing""},""activationEvent"":""onLanguage:jsonc""}
[2020-06-15 09:49:57.031] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/configuration-editing/dist/extension
[2020-06-15 09:49:57.050] [exthost] [info] ExtensionService#_doActivateExtension vscode.json-language-features {""startup"":false,""extensionId"":{""value"":""vscode.json-language-features"",""_lower"":""vscode.json-language-features""},""activationEvent"":""onLanguage:jsonc""}
[2020-06-15 09:49:57.050] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/json-language-features/client/dist/jsonMain
[2020-06-15 09:49:57.112] [exthost] [info] ExtensionService#_doActivateExtension vscode.typescript-language-features {""startup"":false,""extensionId"":{""value"":""vscode.typescript-language-features"",""_lower"":""vscode.typescript-language-features""},""activationEvent"":""onLanguage:jsonc""}
[2020-06-15 09:49:57.112] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/typescript-language-features/dist/extension
[2020-06-15 09:50:05.361] [exthost] [info] ExtensionService#_doActivateExtension ms-vscode.cpptools {""startup"":false,""extensionId"":{""value"":""ms-vscode.cpptools"",""_lower"":""ms-vscode.cpptools""},""activationEvent"":""onDebug""}
[2020-06-15 09:50:05.361] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/ms-vscode.cpptools-0.28.3/dist/main
[2020-06-15 09:50:06.097] [exthost] [info] ExtensionService#_doActivateExtension vscode.debug-server-ready {""startup"":false,""extensionId"":{""value"":""vscode.debug-server-ready"",""_lower"":""vscode.debug-server-ready""},""activationEvent"":""onDebugResolve""}
[2020-06-15 09:50:06.097] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/debug-server-ready/dist/extension
[2020-06-15 09:54:20.473] [exthost] [info] ExtensionService#_doActivateExtension ms-vscode.node-debug2 {""startup"":false,""extensionId"":{""value"":""ms-vscode.node-debug"",""_lower"":""ms-vscode.node-debug""},""activationEvent"":""onDebugInitialConfigurations""}
[2020-06-15 09:54:20.473] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/ms-vscode.node-debug2/out/src/extension
[2020-06-15 09:54:20.481] [exthost] [info] ExtensionService#_doActivateExtension vscjava.vscode-java-debug {""startup"":false,""extensionId"":{""value"":""vscjava.vscode-java-debug"",""_lower"":""vscjava.vscode-java-debug""},""activationEvent"":""onDebugInitialConfigurations""}
[2020-06-15 09:54:20.481] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/vscjava.vscode-java-debug-0.26.0/dist/extension
[2020-06-15 09:54:20.619] [exthost] [info] ExtensionService#_doActivateExtension ms-vscode.node-debug {""startup"":false,""extensionId"":{""value"":""ms-vscode.node-debug"",""_lower"":""ms-vscode.node-debug""},""activationEvent"":""onDebugInitialConfigurations""}
[2020-06-15 09:54:20.619] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/ms-vscode.node-debug/dist/extension.js
[2020-06-15 09:54:20.634] [exthost] [info] ExtensionService#_doActivateExtension ms-azuretools.vscode-docker {""startup"":false,""extensionId"":{""value"":""ms-azuretools.vscode-docker"",""_lower"":""ms-azuretools.vscode-docker""},""activationEvent"":""onDebugInitialConfigurations""}
[2020-06-15 09:54:20.634] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/ms-azuretools.vscode-docker-1.2.1/main
[2020-06-15 09:54:38.934] [exthost] [info] ExtensionService#_doActivateExtension vscode.grunt {""startup"":false,""extensionId"":{""value"":""vscode.grunt"",""_lower"":""vscode.grunt""},""activationEvent"":""onCommand:workbench.action.tasks.runTask""}
[2020-06-15 09:54:38.934] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/grunt/dist/main
[2020-06-15 09:54:38.944] [exthost] [info] ExtensionService#_doActivateExtension vscode.gulp {""startup"":false,""extensionId"":{""value"":""vscode.gulp"",""_lower"":""vscode.gulp""},""activationEvent"":""onCommand:workbench.action.tasks.runTask""}
[2020-06-15 09:54:38.944] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/gulp/dist/main
[2020-06-15 09:54:38.952] [exthost] [info] ExtensionService#_doActivateExtension vscode.jake {""startup"":false,""extensionId"":{""value"":""vscode.jake"",""_lower"":""vscode.jake""},""activationEvent"":""onCommand:workbench.action.tasks.runTask""}
[2020-06-15 09:54:38.952] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/jake/dist/main
[2020-06-15 09:54:38.960] [exthost] [info] ExtensionService#_doActivateExtension vscode.npm {""startup"":false,""extensionId"":{""value"":""vscode.npm"",""_lower"":""vscode.npm""},""activationEvent"":""onCommand:workbench.action.tasks.runTask""}
[2020-06-15 09:54:38.960] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/npm/dist/main</p>
","['args', 'js', 'npm', 'dll', 'gulp', 'vscode-settings', 'app', 'appdata', 'visual-studio-code', 'csharp', '.net', 'a', 'angular', 'authentication', 'conflict', 'env', 'merge', 'json', 'csproj', 'git', 'language-features', 'build', '.net-core', 'vscode', 'c', 'editing', 'minify', 'dotnet', 'publish', 'typescript', 'java', 'x', 'launch', 'binance', 'configuration', 'docker', 'label', 'github', 'local', 'jake', 'cwd', 'emmet', 'node', 'coreclr']"
Visual Studio Code Failed to Start At Debug for dotnet core project,"<p>My project which consists of Angular 8 and .net core cannot start in debug mode in Visual Studio Code. It start for a short time like a second and fail. How can i fix this?</p>

<p>task.json</p>

<hr>

<p>{
    ""version"": ""2.0.0"",
    ""tasks"": [
        {
            ""label"": ""build"",
            ""command"": ""dotnet"",
            ""type"": ""process"",
            ""args"": [
                ""build"",
                ""${workspaceFolder}/X.csproj"",
                ""/property:GenerateFullPaths=true"",
                ""/consoleloggerparameters:NoSummary""
            ],
            ""problemMatcher"": ""$msCompile""
        },
        {
            ""label"": ""publish"",
            ""command"": ""dotnet"",
            ""type"": ""process"",
            ""args"": [
                ""publish"",
                ""${workspaceFolder}/X.csproj"",
                ""/property:GenerateFullPaths=true"",
                ""/consoleloggerparameters:NoSummary""
            ],
            ""problemMatcher"": ""$msCompile""
        },
        {
            ""label"": ""watch"",
            ""command"": ""dotnet"",
            ""type"": ""process"",
            ""args"": [
                ""watch"",
                ""run"",
                ""${workspaceFolder}/X.csproj"",
                ""/property:GenerateFullPaths=true"",
                ""/consoleloggerparameters:NoSummary""
            ],
            ""problemMatcher"": ""$msCompile""
        }
    ]
}</p>

<p>launch.json</p>

<hr>

<p>{
    ""version"": ""0.2.0"",
    ""configurations"": [
        {
            ""name"": "".NET Core Launch (web)"",
            ""type"": ""coreclr"",
            ""request"": ""launch"",
            ""preLaunchTask"": ""build"",
            ""program"": ""${workspaceFolder}/bin/Debug/netcoreapp3.1/X.dll"",
            ""args"": [],
            ""cwd"": ""${workspaceFolder}"",
            ""stopAtEntry"": false,
            ""serverReadyAction"": {
                ""action"": ""openExternally"",
                ""pattern"": ""^\s*Now listening on:\s+(https?://\S+)""
            },
            ""env"": {
                ""ASPNETCORE_ENVIRONMENT"": ""Development""
            },
            ""sourceFileMap"": {
                ""/Views"": ""${workspaceFolder}/Views""
            }
        },
        {
            ""name"": "".NET Core Attach"",
            ""type"": ""coreclr"",
            ""request"": ""attach"",
            ""processId"": ""${command:pickProcess}""
        }
    ]
}</p>

<p>Output Logs:</p>

<hr>

<p>[2020-06-15 09:49:00.672] [exthost] [info] extension host started
[2020-06-15 09:49:00.926] [exthost] [info] ExtensionService#_doActivateExtension ms-dotnettools.csharp {""startup"":false,""extensionId"":{""value"":""ms-dotnettools.csharp"",""_lower"":""ms-dotnettools.csharp""},""activationEvent"":""onLanguage:csharp""}
[2020-06-15 09:49:00.926] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/ms-dotnettools.csharp-1.22.0/dist/extension
[2020-06-15 09:49:01.420] [exthost] [info] ExtensionService#_doActivateExtension vscode.debug-auto-launch {""startup"":true,""extensionId"":{""value"":""vscode.debug-auto-launch"",""_lower"":""vscode.debug-auto-launch""},""activationEvent"":""<em>""}
[2020-06-15 09:49:01.420] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/debug-auto-launch/dist/extension
[2020-06-15 09:49:01.431] [exthost] [info] ExtensionService#_doActivateExtension vscode.emmet {""startup"":true,""extensionId"":{""value"":""vscode.emmet"",""_lower"":""vscode.emmet""},""activationEvent"":""</em>""}
[2020-06-15 09:49:01.431] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/emmet/dist/extension
[2020-06-15 09:49:01.459] [exthost] [info] ExtensionService#_doActivateExtension vscode.git {""startup"":true,""extensionId"":{""value"":""vscode.git"",""_lower"":""vscode.git""},""activationEvent"":""<em>""}
[2020-06-15 09:49:01.459] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/git/dist/main
[2020-06-15 09:49:01.515] [exthost] [info] ExtensionService#_doActivateExtension vscode.github-authentication {""startup"":true,""extensionId"":{""value"":""vscode.github-authentication"",""_lower"":""vscode.github-authentication""},""activationEvent"":""</em>""}
[2020-06-15 09:49:01.515] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/github-authentication/dist/extension.js
[2020-06-15 09:49:01.536] [exthost] [info] ExtensionService#_doActivateExtension vscode.merge-conflict {""startup"":true,""extensionId"":{""value"":""vscode.merge-conflict"",""_lower"":""vscode.merge-conflict""},""activationEvent"":""<em>""}
[2020-06-15 09:49:01.536] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/merge-conflict/dist/extension
[2020-06-15 09:49:01.547] [exthost] [info] ExtensionService#_doActivateExtension vscode.search-result {""startup"":true,""extensionId"":{""value"":""vscode.search-result"",""_lower"":""vscode.search-result""},""activationEvent"":""</em>""}
[2020-06-15 09:49:01.547] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/search-result/dist/extension.js
[2020-06-15 09:49:01.552] [exthost] [info] ExtensionService#_doActivateExtension vscode.vscode-account {""startup"":true,""extensionId"":{""value"":""vscode.vscode-account"",""_lower"":""vscode.vscode-account""},""activationEvent"":""<em>""}
[2020-06-15 09:49:01.552] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/vscode-account/dist/extension.js
[2020-06-15 09:49:01.573] [exthost] [info] ExtensionService#_doActivateExtension donjayamanne.githistory {""startup"":true,""extensionId"":{""value"":""donjayamanne.githistory"",""_lower"":""donjayamanne.githistory""},""activationEvent"":""</em>""}
[2020-06-15 09:49:01.573] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/donjayamanne.githistory-0.6.5/dist/src/extension
[2020-06-15 09:49:01.826] [exthost] [info] ExtensionService#_doActivateExtension HookyQR.minify {""startup"":true,""extensionId"":{""value"":""HookyQR.minify"",""_lower"":""hookyqr.minify""},""activationEvent"":""*""}
[2020-06-15 09:49:01.826] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/hookyqr.minify-0.4.3/extension
[2020-06-15 09:49:03.926] [exthost] [info] eager extensions activated
[2020-06-15 09:49:57.031] [exthost] [info] ExtensionService#_doActivateExtension vscode.configuration-editing {""startup"":false,""extensionId"":{""value"":""vscode.configuration-editing"",""_lower"":""vscode.configuration-editing""},""activationEvent"":""onLanguage:jsonc""}
[2020-06-15 09:49:57.031] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/configuration-editing/dist/extension
[2020-06-15 09:49:57.050] [exthost] [info] ExtensionService#_doActivateExtension vscode.json-language-features {""startup"":false,""extensionId"":{""value"":""vscode.json-language-features"",""_lower"":""vscode.json-language-features""},""activationEvent"":""onLanguage:jsonc""}
[2020-06-15 09:49:57.050] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/json-language-features/client/dist/jsonMain
[2020-06-15 09:49:57.112] [exthost] [info] ExtensionService#_doActivateExtension vscode.typescript-language-features {""startup"":false,""extensionId"":{""value"":""vscode.typescript-language-features"",""_lower"":""vscode.typescript-language-features""},""activationEvent"":""onLanguage:jsonc""}
[2020-06-15 09:49:57.112] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/typescript-language-features/dist/extension
[2020-06-15 09:50:05.361] [exthost] [info] ExtensionService#_doActivateExtension ms-vscode.cpptools {""startup"":false,""extensionId"":{""value"":""ms-vscode.cpptools"",""_lower"":""ms-vscode.cpptools""},""activationEvent"":""onDebug""}
[2020-06-15 09:50:05.361] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/ms-vscode.cpptools-0.28.3/dist/main
[2020-06-15 09:50:06.097] [exthost] [info] ExtensionService#_doActivateExtension vscode.debug-server-ready {""startup"":false,""extensionId"":{""value"":""vscode.debug-server-ready"",""_lower"":""vscode.debug-server-ready""},""activationEvent"":""onDebugResolve""}
[2020-06-15 09:50:06.097] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/debug-server-ready/dist/extension
[2020-06-15 09:54:20.473] [exthost] [info] ExtensionService#_doActivateExtension ms-vscode.node-debug2 {""startup"":false,""extensionId"":{""value"":""ms-vscode.node-debug"",""_lower"":""ms-vscode.node-debug""},""activationEvent"":""onDebugInitialConfigurations""}
[2020-06-15 09:54:20.473] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/ms-vscode.node-debug2/out/src/extension
[2020-06-15 09:54:20.481] [exthost] [info] ExtensionService#_doActivateExtension vscjava.vscode-java-debug {""startup"":false,""extensionId"":{""value"":""vscjava.vscode-java-debug"",""_lower"":""vscjava.vscode-java-debug""},""activationEvent"":""onDebugInitialConfigurations""}
[2020-06-15 09:54:20.481] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/vscjava.vscode-java-debug-0.26.0/dist/extension
[2020-06-15 09:54:20.619] [exthost] [info] ExtensionService#_doActivateExtension ms-vscode.node-debug {""startup"":false,""extensionId"":{""value"":""ms-vscode.node-debug"",""_lower"":""ms-vscode.node-debug""},""activationEvent"":""onDebugInitialConfigurations""}
[2020-06-15 09:54:20.619] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/ms-vscode.node-debug/dist/extension.js
[2020-06-15 09:54:20.634] [exthost] [info] ExtensionService#_doActivateExtension ms-azuretools.vscode-docker {""startup"":false,""extensionId"":{""value"":""ms-azuretools.vscode-docker"",""_lower"":""ms-azuretools.vscode-docker""},""activationEvent"":""onDebugInitialConfigurations""}
[2020-06-15 09:54:20.634] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/.vscode/extensions/ms-azuretools.vscode-docker-1.2.1/main
[2020-06-15 09:54:38.934] [exthost] [info] ExtensionService#_doActivateExtension vscode.grunt {""startup"":false,""extensionId"":{""value"":""vscode.grunt"",""_lower"":""vscode.grunt""},""activationEvent"":""onCommand:workbench.action.tasks.runTask""}
[2020-06-15 09:54:38.934] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/grunt/dist/main
[2020-06-15 09:54:38.944] [exthost] [info] ExtensionService#_doActivateExtension vscode.gulp {""startup"":false,""extensionId"":{""value"":""vscode.gulp"",""_lower"":""vscode.gulp""},""activationEvent"":""onCommand:workbench.action.tasks.runTask""}
[2020-06-15 09:54:38.944] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/gulp/dist/main
[2020-06-15 09:54:38.952] [exthost] [info] ExtensionService#_doActivateExtension vscode.jake {""startup"":false,""extensionId"":{""value"":""vscode.jake"",""_lower"":""vscode.jake""},""activationEvent"":""onCommand:workbench.action.tasks.runTask""}
[2020-06-15 09:54:38.952] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/jake/dist/main
[2020-06-15 09:54:38.960] [exthost] [info] ExtensionService#_doActivateExtension vscode.npm {""startup"":false,""extensionId"":{""value"":""vscode.npm"",""_lower"":""vscode.npm""},""activationEvent"":""onCommand:workbench.action.tasks.runTask""}
[2020-06-15 09:54:38.960] [exthost] [info] ExtensionService#loadCommonJSModule file:///c:/Users/User/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/npm/dist/main</p>
","['args', 'js', 'npm', 'dll', 'gulp', 'vscode-settings', 'app', 'appdata', 'visual-studio-code', 'csharp', '.net', 'a', 'angular', 'authentication', 'conflict', 'env', 'merge', 'json', 'csproj', 'git', 'language-features', 'build', '.net-core', 'vscode', 'c', 'editing', 'minify', 'dotnet', 'publish', 'typescript', 'java', 'x', 'launch', 'binance', 'configuration', 'docker', 'label', 'github', 'local', 'jake', 'cwd', 'emmet', 'node', 'coreclr']"
How to solve docker OCI runtime create failed starting container process caused &quot;exec: \&quot;java\&quot;: executable file not found in $PATH&quot;:,"<h2>Below is my Dockerfile-</h2>

<p>FROM centos</p>

<p>ENV JAVA_HOME /home/jovyan/work/myprojects/jdk-11.0.7</p>

<p>ENV PATH $PATH:/home/jovyan/work/myprojects/jdk-11.0.7/bin</p>

<p>ADD build/libs/CatalogModel-1.0.jar CatalogModel-1.0.jar</p>

<p>EXPOSE 9081</p>

<p>ENTRYPOINT [""java"", ""-jar"", ""CatalogModel-1.0.jar""]</p>

<hr>

<p>CatalogModel-1.0.jar is my springboot application jar file</p>

<p>instead of adding jdk file in Dockerfile using ADD command</p>

<p>Using Below command,i am creating an image </p>

<p><strong>docker build -f Dockerfile -t catalogmodelimage .</strong></p>

<p>i am trying to use bind mount the jdk file using below command,</p>

<p><strong>docker run -p 9081:9081 --mount type=bind,source=C:/Docker/jdk-11.0.7,target=/home/jovyan/work/myprojects catalogmodelimage</strong></p>

<p>But when i execute  the above command,i get this error</p>

<p><strong>docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""java\"": executable file not found in $PATH"": unknown.</strong></p>

<p><a href=""https://i.stack.imgur.com/i3hyg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i3hyg.png"" alt=""enter image description here""></a></p>
","['java', 'mount', 'daemon', 'image', 'bind', 'executable', 'go', 'binance', 'oci', 'docker', 'jar', 'build', 'libs', 'exec', 'centos', 'dockerfile', 'c', 'env']"
How to solve docker OCI runtime create failed starting container process caused &quot;exec: \&quot;java\&quot;: executable file not found in $PATH&quot;:,"<h2>Below is my Dockerfile-</h2>

<p>FROM centos</p>

<p>ENV JAVA_HOME /home/jovyan/work/myprojects/jdk-11.0.7</p>

<p>ENV PATH $PATH:/home/jovyan/work/myprojects/jdk-11.0.7/bin</p>

<p>ADD build/libs/CatalogModel-1.0.jar CatalogModel-1.0.jar</p>

<p>EXPOSE 9081</p>

<p>ENTRYPOINT [""java"", ""-jar"", ""CatalogModel-1.0.jar""]</p>

<hr>

<p>CatalogModel-1.0.jar is my springboot application jar file</p>

<p>instead of adding jdk file in Dockerfile using ADD command</p>

<p>Using Below command,i am creating an image </p>

<p><strong>docker build -f Dockerfile -t catalogmodelimage .</strong></p>

<p>i am trying to use bind mount the jdk file using below command,</p>

<p><strong>docker run -p 9081:9081 --mount type=bind,source=C:/Docker/jdk-11.0.7,target=/home/jovyan/work/myprojects catalogmodelimage</strong></p>

<p>But when i execute  the above command,i get this error</p>

<p><strong>docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""java\"": executable file not found in $PATH"": unknown.</strong></p>

<p><a href=""https://i.stack.imgur.com/i3hyg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i3hyg.png"" alt=""enter image description here""></a></p>
","['java', 'mount', 'daemon', 'image', 'bind', 'executable', 'go', 'binance', 'oci', 'docker', 'jar', 'build', 'libs', 'exec', 'centos', 'dockerfile', 'c', 'env']"
mongo-conector not connecting with solr - Exception during collection dump,"<p>I am connecting MongoDB with solr,</p>

<p>Following this document for integration:
<a href=""https://blog.toadworld.com/2017/02/03/indexing-mongodb-data-in-apache-solr"" rel=""nofollow noreferrer"">https://blog.toadworld.com/2017/02/03/indexing-mongodb-data-in-apache-solr</a></p>

<p><strong>DB.Collection:</strong> solr.wlslog1</p>

<p><strong>D:\path to solr\bin></strong> </p>

<pre><code>mongo-connector --unique-key=id -n solr.wlslog1 -m localhost:27017 -t http://localhost:8983/solr/wlslog1 -d solr_doc_manager
</code></pre>

<p><strong>I am getting below response and error:</strong></p>

<pre><code>2020-06-15 12:15:45,744 [ALWAYS] mongo_connector.connector:50 - Starting mongo-connector version: 3.1.1
2020-06-15 12:15:45,744 [ALWAYS] mongo_connector.connector:50 - Python version: 3.8.3 (tags/v3.8.3:6f8c832, May 13 2020, 22:37:02) [MSC v.1924 64 bit (AMD64)]
2020-06-15 12:15:45,745 [ALWAYS] mongo_connector.connector:50 - Platform: Windows-10-10.0.18362-SP0
2020-06-15 12:15:45,745 [ALWAYS] mongo_connector.connector:50 - pymongo version: 3.10.1
2020-06-15 12:15:45,755 [ALWAYS] mongo_connector.connector:50 - Source MongoDB version: 4.2.2
2020-06-15 12:15:45,755 [ALWAYS] mongo_connector.connector:50 - Target DocManager: mongo_connector.doc_managers.solr_doc_manager version: 0.1.0
2020-06-15 12:15:45,787 [CRITICAL] mongo_connector.oplog_manager:713 - Exception during collection dump
Traceback (most recent call last):
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\doc_managers\solr_doc_manager.py"", line 292, in
batch = list(next(cleaned) for i in range(self.chunk_size))
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\oplog_manager.py"", line 668, in do_dump
upsert_all(dm)
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\oplog_manager.py"", line 651, in upsert_all
dm.bulk_upsert(docs_to_dump(from_coll), mapped_ns, long_ts)
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\util.py"", line 33, in wrapped
return f(*args, **kwargs)
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\doc_managers\solr_doc_manager.py"", line 292, in bulk_upsert
batch = list(next(cleaned) for i in range(self.chunk_size))
RuntimeError: generator raised StopIteration
2020-06-15 12:15:45,801 [ERROR] mongo_connector.oplog_manager:723 - OplogThread: Failed during dump collection cannot recover! Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, replicaset='rs0'), 'local'), 'oplog.rs')
2020-06-15 12:15:46,782 [ERROR] mongo_connector.connector:408 - MongoConnector: OplogThread &lt;OplogThread(Thread-2, started 4936)&gt; unexpectedly stopped! Shutting down
</code></pre>

<p>I searched over in GitHub issues of <a href=""https://github.com/yougov/mongo-connector/issues"" rel=""nofollow noreferrer"">mongo-connector</a> but not getting any solutions.</p>
","['localhost', 'args', 'exception', 'list', 'kwargs', 'range', 'appdata', 'integration', 'key', 'packages', 'python', 'apache', 'dm', 'c', 'database', 'connector', 'generator', 'pymongo', 'binance', 'next', 'mongodb', 'local', 'solr', 'indexing', 'github']"
mongo-conector not connecting with solr - Exception during collection dump,"<p>I am connecting MongoDB with solr,</p>

<p>Following this document for integration:
<a href=""https://blog.toadworld.com/2017/02/03/indexing-mongodb-data-in-apache-solr"" rel=""nofollow noreferrer"">https://blog.toadworld.com/2017/02/03/indexing-mongodb-data-in-apache-solr</a></p>

<p><strong>DB.Collection:</strong> solr.wlslog1</p>

<p><strong>D:\path to solr\bin></strong> </p>

<pre><code>mongo-connector --unique-key=id -n solr.wlslog1 -m localhost:27017 -t http://localhost:8983/solr/wlslog1 -d solr_doc_manager
</code></pre>

<p><strong>I am getting below response and error:</strong></p>

<pre><code>2020-06-15 12:15:45,744 [ALWAYS] mongo_connector.connector:50 - Starting mongo-connector version: 3.1.1
2020-06-15 12:15:45,744 [ALWAYS] mongo_connector.connector:50 - Python version: 3.8.3 (tags/v3.8.3:6f8c832, May 13 2020, 22:37:02) [MSC v.1924 64 bit (AMD64)]
2020-06-15 12:15:45,745 [ALWAYS] mongo_connector.connector:50 - Platform: Windows-10-10.0.18362-SP0
2020-06-15 12:15:45,745 [ALWAYS] mongo_connector.connector:50 - pymongo version: 3.10.1
2020-06-15 12:15:45,755 [ALWAYS] mongo_connector.connector:50 - Source MongoDB version: 4.2.2
2020-06-15 12:15:45,755 [ALWAYS] mongo_connector.connector:50 - Target DocManager: mongo_connector.doc_managers.solr_doc_manager version: 0.1.0
2020-06-15 12:15:45,787 [CRITICAL] mongo_connector.oplog_manager:713 - Exception during collection dump
Traceback (most recent call last):
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\doc_managers\solr_doc_manager.py"", line 292, in
batch = list(next(cleaned) for i in range(self.chunk_size))
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\oplog_manager.py"", line 668, in do_dump
upsert_all(dm)
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\oplog_manager.py"", line 651, in upsert_all
dm.bulk_upsert(docs_to_dump(from_coll), mapped_ns, long_ts)
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\util.py"", line 33, in wrapped
return f(*args, **kwargs)
File ""C:\Users\ancubate\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mongo_connector\doc_managers\solr_doc_manager.py"", line 292, in bulk_upsert
batch = list(next(cleaned) for i in range(self.chunk_size))
RuntimeError: generator raised StopIteration
2020-06-15 12:15:45,801 [ERROR] mongo_connector.oplog_manager:723 - OplogThread: Failed during dump collection cannot recover! Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, replicaset='rs0'), 'local'), 'oplog.rs')
2020-06-15 12:15:46,782 [ERROR] mongo_connector.connector:408 - MongoConnector: OplogThread &lt;OplogThread(Thread-2, started 4936)&gt; unexpectedly stopped! Shutting down
</code></pre>

<p>I searched over in GitHub issues of <a href=""https://github.com/yougov/mongo-connector/issues"" rel=""nofollow noreferrer"">mongo-connector</a> but not getting any solutions.</p>
","['localhost', 'args', 'exception', 'list', 'kwargs', 'range', 'appdata', 'integration', 'key', 'packages', 'python', 'apache', 'dm', 'c', 'database', 'connector', 'generator', 'pymongo', 'binance', 'next', 'mongodb', 'local', 'solr', 'indexing', 'github']"
im getting error in install Gdal library on heroku,"<p>devs,</p>

<p>can anybody please tell me how to install gdal library on Heroku because I'm getting an error while I'm deploying my project on Heroku.</p>

<p>I got two errors as follows:</p>

<p>ERROR: Command errored out with exit status 1:
            command: /app/.heroku/python/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-_s4gq6_a/GDAL/setup.py'""'""'; <strong>file</strong>='""'""'/tmp/pip-install-_s4gq6_a/GDAL/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(<strong>file</strong>);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, <strong>file</strong>, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info
                cwd: /tmp/pip-install-_s4gq6_a/GDAL/
           Complete output (75 lines):
           WARNING: numpy not available!  Array support will not be enabled
           /app/.heroku/python/lib/python3.6/distutils/dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'
             warnings.warn(msg)
           running egg_info
           creating /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info
           writing /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info/PKG-INFO
           writing dependency_links to /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info/dependency_links.txt
           writing top-level names to /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info/top_level.txt
           writing manifest file '/tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info/SOURCES.txt'
           Traceback (most recent call last):
             File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 151, in fetch_config
               p = subprocess.Popen([command, args], stdout=subprocess.PIPE)
             File ""/app/.heroku/python/lib/python3.6/subprocess.py"", line 729, in <strong>init</strong>
               restore_signals, start_new_session)
             File ""/app/.heroku/python/lib/python3.6/subprocess.py"", line 1364, in _execute_child
               raise child_exception_type(errno_num, err_msg, err_filename)
           FileNotFoundError: [Errno 2] No such file or directory: '../../apps/gdal-config': '../../apps/gdal-config'</p>

<pre><code>       During handling of the above exception, another exception occurred:

       Traceback (most recent call last):
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 249, in get_gdal_config
           return fetch_config(option, gdal_config=self.gdal_config)
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 154, in fetch_config
           raise gdal_config_error(e)
       __main__.gdal_config_error: [Errno 2] No such file or directory: '../../apps/gdal-config': '../../apps/gdal-config'

       During handling of the above exception, another exception occurred:

       Traceback (most recent call last):
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 151, in fetch_config
           p = subprocess.Popen([command, args], stdout=subprocess.PIPE)
         File ""/app/.heroku/python/lib/python3.6/subprocess.py"", line 729, in __init__
           restore_signals, start_new_session)
         File ""/app/.heroku/python/lib/python3.6/subprocess.py"", line 1364, in _execute_child
           raise child_exception_type(errno_num, err_msg, err_filename)
       FileNotFoundError: [Errno 2] No such file or directory: 'gdal-config': 'gdal-config'

       During handling of the above exception, another exception occurred:

       Traceback (most recent call last):
         File ""&lt;string&gt;"", line 1, in &lt;module&gt;
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 450, in &lt;module&gt;
           setup(**setup_kwargs)
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/__init__.py"", line 129, in setup
           return distutils.core.setup(**attrs)
         File ""/app/.heroku/python/lib/python3.6/distutils/core.py"", line 148, in setup
           dist.run_commands()
         File ""/app/.heroku/python/lib/python3.6/distutils/dist.py"", line 955, in run_commands
           self.run_command(cmd)
         File ""/app/.heroku/python/lib/python3.6/distutils/dist.py"", line 974, in run_command
           cmd_obj.run()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 278, in run
           self.find_sources()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 293, in find_sources
           mm.run()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 524, in run
           self.add_defaults()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 560, in add_defaults
           sdist.add_defaults(self)
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/py36compat.py"", line 36, in add_defaults
           self._add_defaults_ext()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/py36compat.py"", line 119, in _add_defaults_ext
           build_ext = self.get_finalized_command('build_ext')
         File ""/app/.heroku/python/lib/python3.6/distutils/cmd.py"", line 299, in get_finalized_command
           cmd_obj.ensure_finalized()
         File ""/app/.heroku/python/lib/python3.6/distutils/cmd.py"", line 107, in ensure_finalized
           self.finalize_options()
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 311, in finalize_options
           self.gdaldir = self.get_gdal_config('prefix')
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 255, in get_gdal_config
           return fetch_config(option)
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 154, in fetch_config
           raise gdal_config_error(e)
       __main__.gdal_config_error: [Errno 2] No such file or directory: 'gdal-config': 'gdal-config'
       ----------------------------------------
   ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>
","['args', 'install', 'distutils', 'deploying', 'errno', 'exception', 'prefix', 'argv', 'distribution', 'app', 'init', 'popen', 'lib', 'numpy', 'config', 'heroku', 'exec', 'import', 'raise', 'packages', 'python', 'pip', 'cmd', 'c', 'gdal', 'open', 'getattr', 'geodjango', 'binance', 'exit', 'manifest', 'egg', 'django', 'pipe', 'cwd']"
im getting error in install Gdal library on heroku,"<p>devs,</p>

<p>can anybody please tell me how to install gdal library on Heroku because I'm getting an error while I'm deploying my project on Heroku.</p>

<p>I got two errors as follows:</p>

<p>ERROR: Command errored out with exit status 1:
            command: /app/.heroku/python/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-_s4gq6_a/GDAL/setup.py'""'""'; <strong>file</strong>='""'""'/tmp/pip-install-_s4gq6_a/GDAL/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(<strong>file</strong>);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, <strong>file</strong>, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info
                cwd: /tmp/pip-install-_s4gq6_a/GDAL/
           Complete output (75 lines):
           WARNING: numpy not available!  Array support will not be enabled
           /app/.heroku/python/lib/python3.6/distutils/dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'
             warnings.warn(msg)
           running egg_info
           creating /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info
           writing /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info/PKG-INFO
           writing dependency_links to /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info/dependency_links.txt
           writing top-level names to /tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info/top_level.txt
           writing manifest file '/tmp/pip-install-_s4gq6_a/GDAL/pip-egg-info/GDAL.egg-info/SOURCES.txt'
           Traceback (most recent call last):
             File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 151, in fetch_config
               p = subprocess.Popen([command, args], stdout=subprocess.PIPE)
             File ""/app/.heroku/python/lib/python3.6/subprocess.py"", line 729, in <strong>init</strong>
               restore_signals, start_new_session)
             File ""/app/.heroku/python/lib/python3.6/subprocess.py"", line 1364, in _execute_child
               raise child_exception_type(errno_num, err_msg, err_filename)
           FileNotFoundError: [Errno 2] No such file or directory: '../../apps/gdal-config': '../../apps/gdal-config'</p>

<pre><code>       During handling of the above exception, another exception occurred:

       Traceback (most recent call last):
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 249, in get_gdal_config
           return fetch_config(option, gdal_config=self.gdal_config)
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 154, in fetch_config
           raise gdal_config_error(e)
       __main__.gdal_config_error: [Errno 2] No such file or directory: '../../apps/gdal-config': '../../apps/gdal-config'

       During handling of the above exception, another exception occurred:

       Traceback (most recent call last):
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 151, in fetch_config
           p = subprocess.Popen([command, args], stdout=subprocess.PIPE)
         File ""/app/.heroku/python/lib/python3.6/subprocess.py"", line 729, in __init__
           restore_signals, start_new_session)
         File ""/app/.heroku/python/lib/python3.6/subprocess.py"", line 1364, in _execute_child
           raise child_exception_type(errno_num, err_msg, err_filename)
       FileNotFoundError: [Errno 2] No such file or directory: 'gdal-config': 'gdal-config'

       During handling of the above exception, another exception occurred:

       Traceback (most recent call last):
         File ""&lt;string&gt;"", line 1, in &lt;module&gt;
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 450, in &lt;module&gt;
           setup(**setup_kwargs)
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/__init__.py"", line 129, in setup
           return distutils.core.setup(**attrs)
         File ""/app/.heroku/python/lib/python3.6/distutils/core.py"", line 148, in setup
           dist.run_commands()
         File ""/app/.heroku/python/lib/python3.6/distutils/dist.py"", line 955, in run_commands
           self.run_command(cmd)
         File ""/app/.heroku/python/lib/python3.6/distutils/dist.py"", line 974, in run_command
           cmd_obj.run()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 278, in run
           self.find_sources()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 293, in find_sources
           mm.run()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 524, in run
           self.add_defaults()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 560, in add_defaults
           sdist.add_defaults(self)
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/py36compat.py"", line 36, in add_defaults
           self._add_defaults_ext()
         File ""/app/.heroku/python/lib/python3.6/site-packages/setuptools/command/py36compat.py"", line 119, in _add_defaults_ext
           build_ext = self.get_finalized_command('build_ext')
         File ""/app/.heroku/python/lib/python3.6/distutils/cmd.py"", line 299, in get_finalized_command
           cmd_obj.ensure_finalized()
         File ""/app/.heroku/python/lib/python3.6/distutils/cmd.py"", line 107, in ensure_finalized
           self.finalize_options()
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 311, in finalize_options
           self.gdaldir = self.get_gdal_config('prefix')
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 255, in get_gdal_config
           return fetch_config(option)
         File ""/tmp/pip-install-_s4gq6_a/GDAL/setup.py"", line 154, in fetch_config
           raise gdal_config_error(e)
       __main__.gdal_config_error: [Errno 2] No such file or directory: 'gdal-config': 'gdal-config'
       ----------------------------------------
   ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>
","['args', 'install', 'distutils', 'deploying', 'errno', 'exception', 'prefix', 'argv', 'distribution', 'app', 'init', 'popen', 'lib', 'numpy', 'config', 'heroku', 'exec', 'import', 'raise', 'packages', 'python', 'pip', 'cmd', 'c', 'gdal', 'open', 'getattr', 'geodjango', 'binance', 'exit', 'manifest', 'egg', 'django', 'pipe', 'cwd']"
Django Tutorial - install the previously cloned copy of Django,"<p>I was following this tutorial when this error occurred. I would appreciate it if anyone could tell me what is going wrong here.
<a href=""https://docs.djangoproject.com/en/3.0/intro/contributing/"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/3.0/intro/contributing/</a></p>

<blockquote>
  <p>(djangodev) (base) XXXX@XXXX-MacBook-Air hello_django % python -m pip
  install -e /path/to/your/local/clone/django/</p>
</blockquote>

<p>ERROR: /path/to/your/local/clone/django/ is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with svn+, git+, hg+, or bzr+).</p>

<p>This occurred after entering the following code</p>

<blockquote>
  <p>$ python3 -m venv ~/.virtualenvs/djangodev $ source
  ~/.virtualenvs/djangodev/bin/activate $ .
  ~/.virtualenvs/djangodev/bin/activate</p>
</blockquote>
","['binance', 'install', 'git', 'air', 'django', 'a', 'clone', 'local', 'python', 'pip']"
Django Tutorial - install the previously cloned copy of Django,"<p>I was following this tutorial when this error occurred. I would appreciate it if anyone could tell me what is going wrong here.
<a href=""https://docs.djangoproject.com/en/3.0/intro/contributing/"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/3.0/intro/contributing/</a></p>

<blockquote>
  <p>(djangodev) (base) XXXX@XXXX-MacBook-Air hello_django % python -m pip
  install -e /path/to/your/local/clone/django/</p>
</blockquote>

<p>ERROR: /path/to/your/local/clone/django/ is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with svn+, git+, hg+, or bzr+).</p>

<p>This occurred after entering the following code</p>

<blockquote>
  <p>$ python3 -m venv ~/.virtualenvs/djangodev $ source
  ~/.virtualenvs/djangodev/bin/activate $ .
  ~/.virtualenvs/djangodev/bin/activate</p>
</blockquote>
","['binance', 'install', 'git', 'air', 'django', 'a', 'clone', 'local', 'python', 'pip']"
Jlink can&#39;t put the JDBC into the runtime?,"<p>I am working on a game in Java11 + JavaFX. The building tool is Maven. The IDE is Netbeans 11.</p>

<p>It is a modular project.</p>

<p>I use this manual: <a href=""https://openjfx.io/openjfx-docs/#IDE-NetBeans"" rel=""nofollow noreferrer"">https://openjfx.io/openjfx-docs/#IDE-NetBeans</a></p>

<p>The game compiles and runs smoothly, it can create/read/update the Derby DB when I start it in Netbeans via maven by 'clean javafx:run'.
When I want to create the runtime from terminal by 'mvn clean javafx:jlink' I get a 'BUILD SUCCESS' result too.
But when I try to start the launcher by 'target/adventuregame/bin/adventuregamelauncher' I get an error message:</p>

<p>Connection error: java.sql.SQLException: No suitable driver found for jdbc:derby:advDB;create=true
Exception in Application start method
java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:464)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:363)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at java.base/sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:1051)
Caused by: java.lang.RuntimeException: Exception in Application start method
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:900)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.lambda$launchApplication$2(LauncherImpl.java:195)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: javafx.fxml.LoadException: 
/com.mycompany.adventuregameengine/com/mycompany/adventuregameengine/FXMLDocument.fxml:15</p>

<pre><code>at javafx.fxml/javafx.fxml.FXMLLoader.constructLoadException(FXMLLoader.java:2625)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2603)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2466)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3237)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3194)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3163)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3136)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3113)
at javafx.fxml/javafx.fxml.FXMLLoader.load(FXMLLoader.java:3106)
at com.mycompany.adventuregameengine@1.0-SNAPSHOT/com.mycompany.adventuregameengine.AdventureGame.start(AdventureGame.java:17)
at javafx.graphics/com.sun.javafx.application.LauncherImpl.lambda$launchApplication1$9(LauncherImpl.java:846)
at javafx.graphics/com.sun.javafx.application.PlatformImpl.lambda$runAndWait$12(PlatformImpl.java:455)
at javafx.graphics/com.sun.javafx.application.PlatformImpl.lambda$runLater$10(PlatformImpl.java:428)
at java.base/java.security.AccessController.doPrivileged(Native Method)
at javafx.graphics/com.sun.javafx.application.PlatformImpl.lambda$runLater$11(PlatformImpl.java:427)
at javafx.graphics/com.sun.glass.ui.InvokeLaterDispatcher$Future.run(InvokeLaterDispatcher.java:96)
at javafx.graphics/com.sun.glass.ui.gtk.GtkApplication._runLoop(Native Method)
at javafx.graphics/com.sun.glass.ui.gtk.GtkApplication.lambda$runLoop$11(GtkApplication.java:277)
... 1 more
</code></pre>

<p>Caused by: java.lang.NullPointerException
    at com.mycompany.adventuregameengine@1.0-SNAPSHOT/com.mycompany.adventuregameengine.DB.(DB.java:46)
    at com.mycompany.adventuregameengine@1.0-SNAPSHOT/com.mycompany.adventuregameengine.FXMLDocumentController.(FXMLDocumentController.java:888)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at java.base/java.lang.Class.newInstance(Class.java:584)
    at javafx.fxml/javafx.fxml.FXMLLoader$ValueElement.processAttribute(FXMLLoader.java:936)
    at javafx.fxml/javafx.fxml.FXMLLoader$InstanceDeclarationElement.processAttribute(FXMLLoader.java:980)
    at javafx.fxml/javafx.fxml.FXMLLoader$Element.processStartElement(FXMLLoader.java:227)
    at javafx.fxml/javafx.fxml.FXMLLoader$ValueElement.processStartElement(FXMLLoader.java:752)
    at javafx.fxml/javafx.fxml.FXMLLoader.processStartElement(FXMLLoader.java:2722)
    at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2552)
    ... 17 more
Exception running application com.mycompany.adventuregameengine.AdventureGame</p>

<p>I checked the target/adventuregame/lib/classlist file and it seems to me that the problem is that the jdbc / derby classes are not included. It seems to me that the Jlink can not put the jdbc into the runtime, despite that this manual
<a href=""https://github.com/openjfx/javafx-maven-plugin#javafxjlink-options"" rel=""nofollow noreferrer"">https://github.com/openjfx/javafx-maven-plugin#javafxjlink-options</a>
writes that 'The plugin includes by default: --module-path, --add-modules and -classpath options.' </p>

<p>Here is my module-info.java file:</p>

<pre><code>module com.mycompany.adventuregameengine {
    requires javafx.controls;
    requires javafx.fxml;
    requires java.sql;
    requires java.desktop;
//    requires java.naming;
//    requires java.management;

    opens com.mycompany.adventuregameengine to javafx.fxml; //java.desktop, java.sql
    exports com.mycompany.adventuregameengine;
}
</code></pre>

<p>Before I posted I've checked the similar topics here on stackoverflow, and one suggested that including java.naming and java.management could help, I tried it but it doesn't.</p>

<p>Here is my pom file:</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;com.mycompany&lt;/groupId&gt;
    &lt;artifactId&gt;AdventureGameEngine&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;11&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;11&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;11&lt;/version&gt;
            &lt;classifier&gt;linux&lt;/classifier&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-fxml&lt;/artifactId&gt;
            &lt;version&gt;11&lt;/version&gt;
            &lt;classifier&gt;linux&lt;/classifier&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.derby&lt;/groupId&gt;
            &lt;artifactId&gt;derby&lt;/artifactId&gt;
            &lt;version&gt;10.15.2.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.8.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;release&gt;11&lt;/release&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
                &lt;artifactId&gt;javafx-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;0.0.3&lt;/version&gt;
                &lt;configuration&gt;

                    &lt;compilerArgs&gt;
                        &lt;arg&gt;--add-modules&lt;/arg&gt;
                        &lt;arg&gt;java.sql&lt;/arg&gt;
                    &lt;/compilerArgs&gt;

                    &lt;launcher&gt;adventuregamelauncher&lt;/launcher&gt;
                    &lt;jlinkImageName&gt;adventuregame&lt;/jlinkImageName&gt;
                    &lt;mainClass&gt;com.mycompany.adventuregameengine.AdventureGame&lt;/mainClass&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>I tried the 'compilerArgs' section, but it did not help.</p>

<p>Here is the section of my DB file what the error message refers to, but it must be OK since it runs properly via Netbeans, the problem is with the runtime.</p>

<pre><code>public class DB {

    final String JDBC_DRIVER = ""org.apache.derby.jdbc.EmbeddedDriver"";
    final String URL = ""jdbc:derby:advDB;create=true"";

    Connection conn = null;
    Statement createStatement = null;
    DatabaseMetaData dbmd = null;

    public DB(){
        try {
            conn = DriverManager.getConnection(URL);
            System.out.println(""There is a connection."");
        } catch (SQLException ex) {
            System.err.println(""Connection error: "" + ex);
        }

        if (conn != null){
            try {
                createStatement = conn.createStatement(); //create a statement (auto)
            } catch (SQLException ex) {
                System.err.println(""Statement error: "" + ex);
            }
        }

        //is the database empty, so the program runs first time?
        try {
            dbmd = conn.getMetaData();
            ResultSet rs = dbmd.getTables(null, ""APP"", ""HERO"", null); //capital letter!
            if(!rs.next()){
                createStatement.execute(""create table hero(hp INT, score INT, currentRoom INT, vanBackpack boolean)"");
                createStatement.execute(""create table rooms(description varchar(200), eszakra INT, keletre INT, delre INT, nyugatra INT, fel INT, le INT, id INT)"");
                createStatement.execute(""create table items(roomnumber INT, name varchar(30), description varchar(200), felveheto boolean, vizsgal varchar(200), pozX INT, pozY INT)"");
            }
        } catch (SQLException ex) {
            System.err.println(""ResultSet getTables/create table error: "" + ex);
        }
    }
</code></pre>

<p>If I create a simple sample project on the same way, as the above manual suggests, then the runtime runs properly. It seems it can not include only the jdbc/derby.</p>

<p>I thank you in advance for every suggestion to solve this really annoying issue.
Zed</p>
","['dependencies', 'fxmlloader', 'exception', 'maven-compiler-plugin', 'lambda', 'security', 'linux', 'modular', 'ex', 'app', 'controls', 'nullpointerexception', 'javafx', 'openjfx', 'naming', 'graphics', 'maven-plugin', 'lib', 'classpath', 'module-path', 'ide', 'final', 'a', 'module-info', 'compiler', 'java.lang', 'println', 'constructor', 'derby', 'connection', 'apache', 'desktop', 'jdbc', 'java.lang.class', 'native', 'maven', 'build', 'checked', 'mainclass', 'invocationtargetexception', 'boolean', 'netbeans', 'database', 'fxml', 'invoke', 'java', 'plugins', 'gtk', 'www', 'io', 'binance', 'driver', 'configuration', 'manual', 'next', 'sql', 'jlink', 'launcher', 'github']"
Jlink can&#39;t put the JDBC into the runtime?,"<p>I am working on a game in Java11 + JavaFX. The building tool is Maven. The IDE is Netbeans 11.</p>

<p>It is a modular project.</p>

<p>I use this manual: <a href=""https://openjfx.io/openjfx-docs/#IDE-NetBeans"" rel=""nofollow noreferrer"">https://openjfx.io/openjfx-docs/#IDE-NetBeans</a></p>

<p>The game compiles and runs smoothly, it can create/read/update the Derby DB when I start it in Netbeans via maven by 'clean javafx:run'.
When I want to create the runtime from terminal by 'mvn clean javafx:jlink' I get a 'BUILD SUCCESS' result too.
But when I try to start the launcher by 'target/adventuregame/bin/adventuregamelauncher' I get an error message:</p>

<p>Connection error: java.sql.SQLException: No suitable driver found for jdbc:derby:advDB;create=true
Exception in Application start method
java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:464)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:363)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at java.base/sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:1051)
Caused by: java.lang.RuntimeException: Exception in Application start method
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:900)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.lambda$launchApplication$2(LauncherImpl.java:195)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: javafx.fxml.LoadException: 
/com.mycompany.adventuregameengine/com/mycompany/adventuregameengine/FXMLDocument.fxml:15</p>

<pre><code>at javafx.fxml/javafx.fxml.FXMLLoader.constructLoadException(FXMLLoader.java:2625)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2603)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2466)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3237)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3194)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3163)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3136)
at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:3113)
at javafx.fxml/javafx.fxml.FXMLLoader.load(FXMLLoader.java:3106)
at com.mycompany.adventuregameengine@1.0-SNAPSHOT/com.mycompany.adventuregameengine.AdventureGame.start(AdventureGame.java:17)
at javafx.graphics/com.sun.javafx.application.LauncherImpl.lambda$launchApplication1$9(LauncherImpl.java:846)
at javafx.graphics/com.sun.javafx.application.PlatformImpl.lambda$runAndWait$12(PlatformImpl.java:455)
at javafx.graphics/com.sun.javafx.application.PlatformImpl.lambda$runLater$10(PlatformImpl.java:428)
at java.base/java.security.AccessController.doPrivileged(Native Method)
at javafx.graphics/com.sun.javafx.application.PlatformImpl.lambda$runLater$11(PlatformImpl.java:427)
at javafx.graphics/com.sun.glass.ui.InvokeLaterDispatcher$Future.run(InvokeLaterDispatcher.java:96)
at javafx.graphics/com.sun.glass.ui.gtk.GtkApplication._runLoop(Native Method)
at javafx.graphics/com.sun.glass.ui.gtk.GtkApplication.lambda$runLoop$11(GtkApplication.java:277)
... 1 more
</code></pre>

<p>Caused by: java.lang.NullPointerException
    at com.mycompany.adventuregameengine@1.0-SNAPSHOT/com.mycompany.adventuregameengine.DB.(DB.java:46)
    at com.mycompany.adventuregameengine@1.0-SNAPSHOT/com.mycompany.adventuregameengine.FXMLDocumentController.(FXMLDocumentController.java:888)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at java.base/java.lang.Class.newInstance(Class.java:584)
    at javafx.fxml/javafx.fxml.FXMLLoader$ValueElement.processAttribute(FXMLLoader.java:936)
    at javafx.fxml/javafx.fxml.FXMLLoader$InstanceDeclarationElement.processAttribute(FXMLLoader.java:980)
    at javafx.fxml/javafx.fxml.FXMLLoader$Element.processStartElement(FXMLLoader.java:227)
    at javafx.fxml/javafx.fxml.FXMLLoader$ValueElement.processStartElement(FXMLLoader.java:752)
    at javafx.fxml/javafx.fxml.FXMLLoader.processStartElement(FXMLLoader.java:2722)
    at javafx.fxml/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2552)
    ... 17 more
Exception running application com.mycompany.adventuregameengine.AdventureGame</p>

<p>I checked the target/adventuregame/lib/classlist file and it seems to me that the problem is that the jdbc / derby classes are not included. It seems to me that the Jlink can not put the jdbc into the runtime, despite that this manual
<a href=""https://github.com/openjfx/javafx-maven-plugin#javafxjlink-options"" rel=""nofollow noreferrer"">https://github.com/openjfx/javafx-maven-plugin#javafxjlink-options</a>
writes that 'The plugin includes by default: --module-path, --add-modules and -classpath options.' </p>

<p>Here is my module-info.java file:</p>

<pre><code>module com.mycompany.adventuregameengine {
    requires javafx.controls;
    requires javafx.fxml;
    requires java.sql;
    requires java.desktop;
//    requires java.naming;
//    requires java.management;

    opens com.mycompany.adventuregameengine to javafx.fxml; //java.desktop, java.sql
    exports com.mycompany.adventuregameengine;
}
</code></pre>

<p>Before I posted I've checked the similar topics here on stackoverflow, and one suggested that including java.naming and java.management could help, I tried it but it doesn't.</p>

<p>Here is my pom file:</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;com.mycompany&lt;/groupId&gt;
    &lt;artifactId&gt;AdventureGameEngine&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;11&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;11&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;11&lt;/version&gt;
            &lt;classifier&gt;linux&lt;/classifier&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-fxml&lt;/artifactId&gt;
            &lt;version&gt;11&lt;/version&gt;
            &lt;classifier&gt;linux&lt;/classifier&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.derby&lt;/groupId&gt;
            &lt;artifactId&gt;derby&lt;/artifactId&gt;
            &lt;version&gt;10.15.2.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.8.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;release&gt;11&lt;/release&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
                &lt;artifactId&gt;javafx-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;0.0.3&lt;/version&gt;
                &lt;configuration&gt;

                    &lt;compilerArgs&gt;
                        &lt;arg&gt;--add-modules&lt;/arg&gt;
                        &lt;arg&gt;java.sql&lt;/arg&gt;
                    &lt;/compilerArgs&gt;

                    &lt;launcher&gt;adventuregamelauncher&lt;/launcher&gt;
                    &lt;jlinkImageName&gt;adventuregame&lt;/jlinkImageName&gt;
                    &lt;mainClass&gt;com.mycompany.adventuregameengine.AdventureGame&lt;/mainClass&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>I tried the 'compilerArgs' section, but it did not help.</p>

<p>Here is the section of my DB file what the error message refers to, but it must be OK since it runs properly via Netbeans, the problem is with the runtime.</p>

<pre><code>public class DB {

    final String JDBC_DRIVER = ""org.apache.derby.jdbc.EmbeddedDriver"";
    final String URL = ""jdbc:derby:advDB;create=true"";

    Connection conn = null;
    Statement createStatement = null;
    DatabaseMetaData dbmd = null;

    public DB(){
        try {
            conn = DriverManager.getConnection(URL);
            System.out.println(""There is a connection."");
        } catch (SQLException ex) {
            System.err.println(""Connection error: "" + ex);
        }

        if (conn != null){
            try {
                createStatement = conn.createStatement(); //create a statement (auto)
            } catch (SQLException ex) {
                System.err.println(""Statement error: "" + ex);
            }
        }

        //is the database empty, so the program runs first time?
        try {
            dbmd = conn.getMetaData();
            ResultSet rs = dbmd.getTables(null, ""APP"", ""HERO"", null); //capital letter!
            if(!rs.next()){
                createStatement.execute(""create table hero(hp INT, score INT, currentRoom INT, vanBackpack boolean)"");
                createStatement.execute(""create table rooms(description varchar(200), eszakra INT, keletre INT, delre INT, nyugatra INT, fel INT, le INT, id INT)"");
                createStatement.execute(""create table items(roomnumber INT, name varchar(30), description varchar(200), felveheto boolean, vizsgal varchar(200), pozX INT, pozY INT)"");
            }
        } catch (SQLException ex) {
            System.err.println(""ResultSet getTables/create table error: "" + ex);
        }
    }
</code></pre>

<p>If I create a simple sample project on the same way, as the above manual suggests, then the runtime runs properly. It seems it can not include only the jdbc/derby.</p>

<p>I thank you in advance for every suggestion to solve this really annoying issue.
Zed</p>
","['dependencies', 'fxmlloader', 'exception', 'maven-compiler-plugin', 'lambda', 'security', 'linux', 'modular', 'ex', 'app', 'controls', 'nullpointerexception', 'javafx', 'openjfx', 'naming', 'graphics', 'maven-plugin', 'lib', 'classpath', 'module-path', 'ide', 'final', 'a', 'module-info', 'compiler', 'java.lang', 'println', 'constructor', 'derby', 'connection', 'apache', 'desktop', 'jdbc', 'java.lang.class', 'native', 'maven', 'build', 'checked', 'mainclass', 'invocationtargetexception', 'boolean', 'netbeans', 'database', 'fxml', 'invoke', 'java', 'plugins', 'gtk', 'www', 'io', 'binance', 'driver', 'configuration', 'manual', 'next', 'sql', 'jlink', 'launcher', 'github']"
Selecting values from GroupBy Dataframe in Python Pandas,"<p>I am trying to select particular values from a DATAFRAME that I got using GrouBy function, but received AttributeError: 'DataFrameGroupBy' object has no attribute 'iloc'
cars_groups = cars.groupby(cars.cyl)
cars_grops.max()
    car_names   mpg disp    hp  drat    wt  qsec    vs  am  gear    carb
cyl<br>
4   Volvo 142E  33.9    146.7   113 4.93    3.190   22.90   1   1   5   2
6   Valiant 21.4    258.0   175 3.92    3.460   20.22   1   1   5   6
8   Pontiac Firebird    19.2    472.0   335 4.22    5.424   18.00   0   1   5   8</p>

<p>I am trying to select cyc = 4, mpg = 33.9 and and hp =113 all from row=1. I tried using print('The max mpg of {} cyclinder '.format(cars_groups.iloc[0]))</p>
","['format', 'dataframe', 'python-3.x', 'disp', 'attributeerror', 'max', 'a', 'pandas-groupby', 'drat', 'python', 'firebird', 'pandas']"
Selecting values from GroupBy Dataframe in Python Pandas,"<p>I am trying to select particular values from a DATAFRAME that I got using GrouBy function, but received AttributeError: 'DataFrameGroupBy' object has no attribute 'iloc'
cars_groups = cars.groupby(cars.cyl)
cars_grops.max()
    car_names   mpg disp    hp  drat    wt  qsec    vs  am  gear    carb
cyl<br>
4   Volvo 142E  33.9    146.7   113 4.93    3.190   22.90   1   1   5   2
6   Valiant 21.4    258.0   175 3.92    3.460   20.22   1   1   5   6
8   Pontiac Firebird    19.2    472.0   335 4.22    5.424   18.00   0   1   5   8</p>

<p>I am trying to select cyc = 4, mpg = 33.9 and and hp =113 all from row=1. I tried using print('The max mpg of {} cyclinder '.format(cars_groups.iloc[0]))</p>
","['format', 'dataframe', 'python-3.x', 'disp', 'attributeerror', 'max', 'a', 'pandas-groupby', 'drat', 'python', 'firebird', 'pandas']"
Subtracting two columns within a Pandas GroupBy object,"<p>I have a dataset with marketing campaigns, where each house receive campaign actions like ""flyer"", or ""call"". Each action has it's own creation and end date. Some houses have only 1 action, and some have a couple. </p>

<p>What I want to do is: </p>

<p>I want to calculate the length of the campaign for each house, so the time between the first action (e.g. flyer) and the last recorded action for each house. If each house had only 1 action, I could easily solve this by subtracting the end date column with the start date column.</p>

<p>Because some houses have multiple actions, I figured I could group all the houses with the Pandas GroupBy function. Does anyone know how to subtract within a groupby object?</p>

<p>Data looks like this:</p>

<pre><code>house1 flyer 01-12-2014 05-12-2014
house1 phonecall 06-12-2014 06-12-2014
house2 flyer 01-12-2014 31-12-2014
</code></pre>

<p>my expected output looks like this:</p>

<pre><code>house1 ; 5 days
house2 ; 30 days
house3 ; 12 days
house4 ; 60 days
etc
</code></pre>
","['creation', 'dataset', 'column', 'a', 'python', 'pandas']"
Subtracting two columns within a Pandas GroupBy object,"<p>I have a dataset with marketing campaigns, where each house receive campaign actions like ""flyer"", or ""call"". Each action has it's own creation and end date. Some houses have only 1 action, and some have a couple. </p>

<p>What I want to do is: </p>

<p>I want to calculate the length of the campaign for each house, so the time between the first action (e.g. flyer) and the last recorded action for each house. If each house had only 1 action, I could easily solve this by subtracting the end date column with the start date column.</p>

<p>Because some houses have multiple actions, I figured I could group all the houses with the Pandas GroupBy function. Does anyone know how to subtract within a groupby object?</p>

<p>Data looks like this:</p>

<pre><code>house1 flyer 01-12-2014 05-12-2014
house1 phonecall 06-12-2014 06-12-2014
house2 flyer 01-12-2014 31-12-2014
</code></pre>

<p>my expected output looks like this:</p>

<pre><code>house1 ; 5 days
house2 ; 30 days
house3 ; 12 days
house4 ; 60 days
etc
</code></pre>
","['creation', 'dataset', 'column', 'a', 'python', 'pandas']"
How do I remove square brackets for all the values in a column?,"<p>I have a column named keywords in my pandas dataset. The values of the column are like this : </p>

<pre><code>[jdhdhsn, cultuere, jdhdy]
</code></pre>

<p>I want my output to be </p>

<pre><code>jdhdhsn, cultuere, jdhdy
</code></pre>
","['dataset', 'column', 'a', 'python', 'pandas', 'brackets']"
How do I remove square brackets for all the values in a column?,"<p>I have a column named keywords in my pandas dataset. The values of the column are like this : </p>

<pre><code>[jdhdhsn, cultuere, jdhdy]
</code></pre>

<p>I want my output to be </p>

<pre><code>jdhdhsn, cultuere, jdhdy
</code></pre>
","['dataset', 'column', 'a', 'python', 'pandas', 'brackets']"
Movielens dataset most preferred movie genre,"<p>I'm working with the MovieLens 100K dataset. I would like to have a graph visualizing the most preferred movie genres for the female users. For now that works by summing up how many times they have given a rating for a movie in the specific genre (meaning they have engaged with the genre). However, considering that some of the female users have given way more ratings than others maybe the results are not very true. Maybe there was one female user that really liked 'Drama' and gave 100 ratings to that. What is a way to normalize that data and to have a true representation of what are the most preferred genres?</p>

<pre><code>plt.figure(figsize=(18,10))
for column in all_female_users[['Action', 'Adventure' , 'Animation' ,
              'Childrens' , 'Comedy' , 'Crime' , 'Documentary' , 'Drama' , 'Fantasy' ,
              'Film-Noir' , 'Horror' , 'Musical' , 'Mystery' , 'Romance' , 'Sci-Fi' ,
              'Thriller' , 'War' , 'Western']]:
   # Select column contents by column name using [] operator
    columnSeriesObj = all_female_users[column]

    plt.bar(column, columnSeriesObj.sum())
    plt.xlabel(""Movie Genre"")
    plt.ylabel(""Number of preferred times"")
    plt.title(""Most preferred movie genres by women"")
#     print('Movie Genre:', column)
#     print('Move Genre Sum:', columnSeriesObj.sum())
</code></pre>
","['adventure', 'dataset', 'column', 'noir', 'a', 'rating', 'animation', 'graph', 'plt', 'movie', 'normalize', 'pandas', 'statistics']"
Movielens dataset most preferred movie genre,"<p>I'm working with the MovieLens 100K dataset. I would like to have a graph visualizing the most preferred movie genres for the female users. For now that works by summing up how many times they have given a rating for a movie in the specific genre (meaning they have engaged with the genre). However, considering that some of the female users have given way more ratings than others maybe the results are not very true. Maybe there was one female user that really liked 'Drama' and gave 100 ratings to that. What is a way to normalize that data and to have a true representation of what are the most preferred genres?</p>

<pre><code>plt.figure(figsize=(18,10))
for column in all_female_users[['Action', 'Adventure' , 'Animation' ,
              'Childrens' , 'Comedy' , 'Crime' , 'Documentary' , 'Drama' , 'Fantasy' ,
              'Film-Noir' , 'Horror' , 'Musical' , 'Mystery' , 'Romance' , 'Sci-Fi' ,
              'Thriller' , 'War' , 'Western']]:
   # Select column contents by column name using [] operator
    columnSeriesObj = all_female_users[column]

    plt.bar(column, columnSeriesObj.sum())
    plt.xlabel(""Movie Genre"")
    plt.ylabel(""Number of preferred times"")
    plt.title(""Most preferred movie genres by women"")
#     print('Movie Genre:', column)
#     print('Move Genre Sum:', columnSeriesObj.sum())
</code></pre>
","['adventure', 'dataset', 'column', 'noir', 'a', 'rating', 'animation', 'graph', 'plt', 'movie', 'normalize', 'pandas', 'statistics']"
Create a dataset of all Products in 1 column or 1 list with Python,"<p>I have a dataframe of all the sessions and products (column A and B) that were bought together. </p>

<p>What I would like to do is to have a unique row of session (column D) while concatenating all the products together (like in column E).</p>

<p><img src=""https://i.stack.imgur.com/l5tx5.png"" alt=""image""></p>

<p>It would be great if it could be in the format like: </p>

<pre><code>dataset = [['A'], ['A','B','C'], ['D'], ['C','E']]
</code></pre>
","['dataset', 'format', 'dataframe', 'python-3.x', 'list', 'column', 'a', 'b', 'c', 'python', 'pandas']"
Create a dataset of all Products in 1 column or 1 list with Python,"<p>I have a dataframe of all the sessions and products (column A and B) that were bought together. </p>

<p>What I would like to do is to have a unique row of session (column D) while concatenating all the products together (like in column E).</p>

<p><img src=""https://i.stack.imgur.com/l5tx5.png"" alt=""image""></p>

<p>It would be great if it could be in the format like: </p>

<pre><code>dataset = [['A'], ['A','B','C'], ['D'], ['C','E']]
</code></pre>
","['dataset', 'format', 'dataframe', 'python-3.x', 'list', 'column', 'a', 'b', 'c', 'python', 'pandas']"
Pandas convert datatime,"<p>I have a data frame extract from excel file with a date.
The original date in excel is in this format (dd/mm/yyyy) </p>

<p><code>20/05/2020</code></p>

<p>but, in my code, if I print the date column, the format date is changed (yyyy-mm-dd)</p>

<p><code>2020-05-20</code></p>

<p>When I copy the dataframe in another preformatted sheet excel, the date has converted in yyyy/mm/dd hh:mm:ss </p>

<p><code>2020-05-20 00:00:00</code></p>

<p>In the preformatted sheet, the date column is setting with format ""dd/mm/yyyy"".</p>

<p>I have try to use the to_datetime, but this not work for me.</p>

<pre><code>df.Workdate = pd.to_datetime(df.Workdate.dt.strftime('%d/%m/%Y'))
</code></pre>

<p>or</p>

<pre><code>df['Workdate'] = pd.to_datetime(df['Workdate'], format='%d/%m/%Y')
</code></pre>

<p>The result not change, if I print the df, the date have every the same format yyyy-mm-dd (in excel yyyy-mm-dd hh:mm:ss).</p>

<p>Where am i wrong?</p>

<p>Regards, 
Marco</p>
","['extract', 'format', 'dataframe', 'dd', 'column', 'a', 'pd', 'frame', 'excel', 'dt', 'python', 'df', 'datetime', 'pandas']"
Pandas convert datatime,"<p>I have a data frame extract from excel file with a date.
The original date in excel is in this format (dd/mm/yyyy) </p>

<p><code>20/05/2020</code></p>

<p>but, in my code, if I print the date column, the format date is changed (yyyy-mm-dd)</p>

<p><code>2020-05-20</code></p>

<p>When I copy the dataframe in another preformatted sheet excel, the date has converted in yyyy/mm/dd hh:mm:ss </p>

<p><code>2020-05-20 00:00:00</code></p>

<p>In the preformatted sheet, the date column is setting with format ""dd/mm/yyyy"".</p>

<p>I have try to use the to_datetime, but this not work for me.</p>

<pre><code>df.Workdate = pd.to_datetime(df.Workdate.dt.strftime('%d/%m/%Y'))
</code></pre>

<p>or</p>

<pre><code>df['Workdate'] = pd.to_datetime(df['Workdate'], format='%d/%m/%Y')
</code></pre>

<p>The result not change, if I print the df, the date have every the same format yyyy-mm-dd (in excel yyyy-mm-dd hh:mm:ss).</p>

<p>Where am i wrong?</p>

<p>Regards, 
Marco</p>
","['extract', 'format', 'dataframe', 'dd', 'column', 'a', 'pd', 'frame', 'excel', 'dt', 'python', 'df', 'datetime', 'pandas']"
"ValueError: Length mismatch: Expected axis has 0 elements, new values have 7 elements","<pre><code>def getNearbyVenues(names, latitudes, longitudes, radius=500):

    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        url = 'https://api.foursquare.com/v2/venues/explore?&amp;client_id={}&amp;client_secret={}&amp;v={}&amp;ll={},{}&amp;radius={}&amp;limit={}'.format(
            CLIENT_ID, CLIENT_SECRET, VERSION, lat, lng, radius, LIMIT)

        results = requests.get(url).json()[""response""]['groups'][0]['items']

        venues_list.append([( name, lat, lng, v['venue']['name'], v['venue']['location']['lat'], v['venue']['location']['lng'], v['venue']['categories'][0]['name']) for v in results])


    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 'Neighborhood Latitude', 'Neighborhood Longitude', 'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']


    return(nearby_venues)
</code></pre>

<p>and when i call the above function</p>

<pre><code>toronto_venues = getNearbyVenues(names=df['Neighborhood'], latitudes=df['Latitude'],longitudes=df['Longitude'])
</code></pre>

<p>I got the following error</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-43-0a244f551e86&gt; in &lt;module&gt;
      2 #nearyby_venues
      3 #toronto_venues = pd.DataFrame(pd.np.empty((0, 7)))
----&gt; 4 toronto_venues = getNearbyVenues(names=df['Neighborhood'], latitudes=df['Latitude'],longitudes=df['Longitude'])

&lt;ipython-input-42-217083dbcb80&gt; in getNearbyVenues(names, latitudes, longitudes, radius)
     14 
     15     nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
---&gt; 16     nearby_venues.columns = ['Neighborhood', 'Neighborhood Latitude', 'Neighborhood Longitude', 'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']
     17 
     18 

~\Anaconda3\lib\site-packages\pandas\core\generic.py in __setattr__(self, name, value)
   5285         try:
   5286             object.__getattribute__(self, name)
-&gt; 5287             return object.__setattr__(self, name, value)
   5288         except AttributeError:
   5289             pass

pandas\_libs\properties.pyx in pandas._libs.properties.AxisProperty.__set__()

~\Anaconda3\lib\site-packages\pandas\core\generic.py in _set_axis(self, axis, labels)
    659 
    660     def _set_axis(self, axis, labels) -&gt; None:
--&gt; 661         self._data.set_axis(axis, labels)
    662         self._clear_item_cache()
    663 

~\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in set_axis(self, axis, new_labels)
    176         if new_len != old_len:
    177             raise ValueError(
--&gt; 178                 f""Length mismatch: Expected axis has {old_len} elements, new ""
    179                 f""values have {new_len} elements""
    180             )

ValueError: Length mismatch: Expected axis has 0 elements, new values have 7 elements
</code></pre>

<p>From the stack trace i understand that, the problem is occurring due to the <code>nearby_venues</code> dataframe, the way i am assigning the columns is the main reason behind the error. What can i do to solve this issue? </p>
","['internals', 'axis', 'foursquare', 'categories', 'api', 'pd', 'lib', 'ipython', 'raise', 'packages', 'python', 'df', 'mismatch', 'json', 'format', 'limit', 'radius', 'attributeerror', 'labels', 'pandas', 'append', 'dataframe', 'python-3.x', 'pyx', 'np']"
"ValueError: Length mismatch: Expected axis has 0 elements, new values have 7 elements","<pre><code>def getNearbyVenues(names, latitudes, longitudes, radius=500):

    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        url = 'https://api.foursquare.com/v2/venues/explore?&amp;client_id={}&amp;client_secret={}&amp;v={}&amp;ll={},{}&amp;radius={}&amp;limit={}'.format(
            CLIENT_ID, CLIENT_SECRET, VERSION, lat, lng, radius, LIMIT)

        results = requests.get(url).json()[""response""]['groups'][0]['items']

        venues_list.append([( name, lat, lng, v['venue']['name'], v['venue']['location']['lat'], v['venue']['location']['lng'], v['venue']['categories'][0]['name']) for v in results])


    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 'Neighborhood Latitude', 'Neighborhood Longitude', 'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']


    return(nearby_venues)
</code></pre>

<p>and when i call the above function</p>

<pre><code>toronto_venues = getNearbyVenues(names=df['Neighborhood'], latitudes=df['Latitude'],longitudes=df['Longitude'])
</code></pre>

<p>I got the following error</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-43-0a244f551e86&gt; in &lt;module&gt;
      2 #nearyby_venues
      3 #toronto_venues = pd.DataFrame(pd.np.empty((0, 7)))
----&gt; 4 toronto_venues = getNearbyVenues(names=df['Neighborhood'], latitudes=df['Latitude'],longitudes=df['Longitude'])

&lt;ipython-input-42-217083dbcb80&gt; in getNearbyVenues(names, latitudes, longitudes, radius)
     14 
     15     nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
---&gt; 16     nearby_venues.columns = ['Neighborhood', 'Neighborhood Latitude', 'Neighborhood Longitude', 'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']
     17 
     18 

~\Anaconda3\lib\site-packages\pandas\core\generic.py in __setattr__(self, name, value)
   5285         try:
   5286             object.__getattribute__(self, name)
-&gt; 5287             return object.__setattr__(self, name, value)
   5288         except AttributeError:
   5289             pass

pandas\_libs\properties.pyx in pandas._libs.properties.AxisProperty.__set__()

~\Anaconda3\lib\site-packages\pandas\core\generic.py in _set_axis(self, axis, labels)
    659 
    660     def _set_axis(self, axis, labels) -&gt; None:
--&gt; 661         self._data.set_axis(axis, labels)
    662         self._clear_item_cache()
    663 

~\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in set_axis(self, axis, new_labels)
    176         if new_len != old_len:
    177             raise ValueError(
--&gt; 178                 f""Length mismatch: Expected axis has {old_len} elements, new ""
    179                 f""values have {new_len} elements""
    180             )

ValueError: Length mismatch: Expected axis has 0 elements, new values have 7 elements
</code></pre>

<p>From the stack trace i understand that, the problem is occurring due to the <code>nearby_venues</code> dataframe, the way i am assigning the columns is the main reason behind the error. What can i do to solve this issue? </p>
","['internals', 'axis', 'foursquare', 'categories', 'api', 'pd', 'lib', 'ipython', 'raise', 'packages', 'python', 'df', 'mismatch', 'json', 'format', 'limit', 'radius', 'attributeerror', 'labels', 'pandas', 'append', 'dataframe', 'python-3.x', 'pyx', 'np']"
Make Python Pandas Faster,"<p>I have the code below, which works successfully, and is used to parse, clean log files (very large in size) and output into smaller sized files. This would take about 12-14 mins to process 1 GB worth of logs (on my laptop). Can this be made faster? Could Dask or parallelism or asyncio or another help speed this up? </p>

<p>I am new to python and pandas, and I have googled around, but am totally confused and cant seem to adopt any of the examples I saw.</p>

<p><strong>Pls help improve this code</strong></p>

<pre><code>for root, dirs, files in os.walk('.', topdown=True):
    for file in files:
        try:
            for df in pd.read_csv(file, sep='\n', header=None, engine='python', quoting=3, chunksize=1200000):
                df = df[0].str.strip(' \t""').str.split('[,|;: \t]+', 1, expand=True).rename(columns={0: 'email', 1: 'data'}) 

                mask = (df.email.str.contains(emailreg, regex=True, na=False)) &amp; (~df.data.str.contains(asciireg, regex=True, na=False))
                df2 = df[~mask].copy()
                df = df[mask].copy()
                df2[['email', 'data']].to_csv(""errorfile"", sep=':', index=False, header=False, mode='a', compression='gzip')
                del df2
                del mask

                for x in ""abcdefghijklmnopqrstuvwxyz0123456789"":
                    df2 = df[df.email.str.startswith(x)]
                    if (df.email.size &gt; 0):
                        df2[['email', 'data']].to_csv(x, sep=':', index=False, header=False, mode='a')
</code></pre>

<p><strong>Sample log file</strong></p>

<pre><code>""email1@foo.com:datahere2     
email2@foo.com:datahere2
email3@foo.com datahere2
email5@foo.com;dtat'ah'ere2 
wrongemailfoo.com
email3@foo.com:datahere2
</code></pre>
","['del', 'python-asyncio', 'parse', 'mask', 'pd', 'asyncio', 'compression', 'a', 'python', 'df', 'na', 'index', 'dask', 'os.walk', 'expand', 'gzip', 'header', 'quoting', 'pandas', 'email', 'x']"
Make Python Pandas Faster,"<p>I have the code below, which works successfully, and is used to parse, clean log files (very large in size) and output into smaller sized files. This would take about 12-14 mins to process 1 GB worth of logs (on my laptop). Can this be made faster? Could Dask or parallelism or asyncio or another help speed this up? </p>

<p>I am new to python and pandas, and I have googled around, but am totally confused and cant seem to adopt any of the examples I saw.</p>

<p><strong>Pls help improve this code</strong></p>

<pre><code>for root, dirs, files in os.walk('.', topdown=True):
    for file in files:
        try:
            for df in pd.read_csv(file, sep='\n', header=None, engine='python', quoting=3, chunksize=1200000):
                df = df[0].str.strip(' \t""').str.split('[,|;: \t]+', 1, expand=True).rename(columns={0: 'email', 1: 'data'}) 

                mask = (df.email.str.contains(emailreg, regex=True, na=False)) &amp; (~df.data.str.contains(asciireg, regex=True, na=False))
                df2 = df[~mask].copy()
                df = df[mask].copy()
                df2[['email', 'data']].to_csv(""errorfile"", sep=':', index=False, header=False, mode='a', compression='gzip')
                del df2
                del mask

                for x in ""abcdefghijklmnopqrstuvwxyz0123456789"":
                    df2 = df[df.email.str.startswith(x)]
                    if (df.email.size &gt; 0):
                        df2[['email', 'data']].to_csv(x, sep=':', index=False, header=False, mode='a')
</code></pre>

<p><strong>Sample log file</strong></p>

<pre><code>""email1@foo.com:datahere2     
email2@foo.com:datahere2
email3@foo.com datahere2
email5@foo.com;dtat'ah'ere2 
wrongemailfoo.com
email3@foo.com:datahere2
</code></pre>
","['del', 'python-asyncio', 'parse', 'mask', 'pd', 'asyncio', 'compression', 'a', 'python', 'df', 'na', 'index', 'dask', 'os.walk', 'expand', 'gzip', 'header', 'quoting', 'pandas', 'email', 'x']"
tzinfo in Pandas and datetime seems to be different. Is there a workaround?,"<p>I am trying to find a time difference between two datatimes. One is set from datetime and another one is read from a CSV file into a dataframe.</p>

<p>The CSV file:</p>

<pre><code>,Timestamp,Value
1,2020-04-21 00:46:23,24.965867802122457
</code></pre>

<p>Actual code:</p>

<pre><code>import pandas as pd
import numpy as np
from datetime import datetime, timezone

EPOCH = datetime.utcfromtimestamp(0).replace(tzinfo=timezone.utc)

df = pd.read_csv('./Out/bottom_clamp_pressure.csv', index_col = 0, header = 0)
df['Timestamp'] = df['Timestamp'].apply(pd.to_datetime, utc = True)

print(EPOCH)
print(df.loc[1, 'Timestamp'])

# Output:
# 1970-01-01 00:00:00+00:00
# 2020-04-21 00:46:23+00:00

print(EPOCH.tzinfo)
print(df.loc[1, 'Timestamp'].tzinfo)

# Output:
# UTC
# UTC

print(EPOCH.tzinfo == df.loc[1, 'Timestamp'].tzinfo)

# Output:
# False

print(df.loc[1, 'Timestamp'] - EPOCH)

# Output:
# TypeError: Timestamp subtraction must have the same timezones or no timezones
</code></pre>

<p>As you can see in the output above, both dates seems to have UTC timezone, at the same time, one time zone is not equal to another and subtraction of them does not work. Is there some work around that can allow me to get subtraction results?</p>

<p>Thanks!</p>
","['np', 'dataframe', 'numpy', 'a', 'pd', 'import', 'header', 'csv', 'epoch', 'python', 'df', 'datetime', 'loc', 'pandas']"
tzinfo in Pandas and datetime seems to be different. Is there a workaround?,"<p>I am trying to find a time difference between two datatimes. One is set from datetime and another one is read from a CSV file into a dataframe.</p>

<p>The CSV file:</p>

<pre><code>,Timestamp,Value
1,2020-04-21 00:46:23,24.965867802122457
</code></pre>

<p>Actual code:</p>

<pre><code>import pandas as pd
import numpy as np
from datetime import datetime, timezone

EPOCH = datetime.utcfromtimestamp(0).replace(tzinfo=timezone.utc)

df = pd.read_csv('./Out/bottom_clamp_pressure.csv', index_col = 0, header = 0)
df['Timestamp'] = df['Timestamp'].apply(pd.to_datetime, utc = True)

print(EPOCH)
print(df.loc[1, 'Timestamp'])

# Output:
# 1970-01-01 00:00:00+00:00
# 2020-04-21 00:46:23+00:00

print(EPOCH.tzinfo)
print(df.loc[1, 'Timestamp'].tzinfo)

# Output:
# UTC
# UTC

print(EPOCH.tzinfo == df.loc[1, 'Timestamp'].tzinfo)

# Output:
# False

print(df.loc[1, 'Timestamp'] - EPOCH)

# Output:
# TypeError: Timestamp subtraction must have the same timezones or no timezones
</code></pre>

<p>As you can see in the output above, both dates seems to have UTC timezone, at the same time, one time zone is not equal to another and subtraction of them does not work. Is there some work around that can allow me to get subtraction results?</p>

<p>Thanks!</p>
","['np', 'dataframe', 'numpy', 'a', 'pd', 'import', 'header', 'csv', 'epoch', 'python', 'df', 'datetime', 'loc', 'pandas']"
Can&#39;t convert column to category dtypes Pandas with read_csv,"<p>I have data from csv and load it with read_csv in Pandas. I try to convert 6 column to float32 and its worked, but category column not converted..</p>

<p>I have checked my 'div' column and there is no problem with it:</p>

<pre><code>df_concat['div'].unique()

array(['L', 'J', 'K', 'U', 'E', 'B', 'A', 'C', 'N', 'X', 'M', 'O', 'D',
       'I', 'P', 'Q', 'S', 'R', 'T'], dtype=object)
</code></pre>

<p>I tried to limit data with nrows=4000000 and it success converted to category dtypes !
what's wrong with it? </p>

<p>this my code:</p>

<pre><code>names = ['bdate', 'nama_site', 'kode_store', 'div', 'merdivdesc', 'cat', 'catdesc', 'subcat', 'subcatdesc', 'brand', 'sku', 'sku_desc', 'tillcode', 'netsales', 'profit', 'margin', 'qty']

dtype = {
    'netsales' : 'float32', 'profit' : 'float32', 'margin' : 'float32', 'qty' : 'float32',
    'div' : 'category'
}

data = pd.read_csv('clean_jan20_minified.csv', sep='|', dtype=dtype, chunksize=20000, names=names, skiprows=[0], nrows=4000000)

chunk_list = []  
for chunk in data:  
    chunk_list.append(chunk)

df_concat = pd.concat(chunk_list, ignore_index=True)

</code></pre>

<p>when i try manually convert with <code>df_concat['div']=df_concat['div'].astype('category')</code> it works. but i need convert it when read_csv</p>
","['append', 'x', 'margin', 'concat', 'limit', 'column', 'checked', 'a', 'b', 'cat', 'c', 'csv', 'python', 'pd', 'pandas']"
Can&#39;t convert column to category dtypes Pandas with read_csv,"<p>I have data from csv and load it with read_csv in Pandas. I try to convert 6 column to float32 and its worked, but category column not converted..</p>

<p>I have checked my 'div' column and there is no problem with it:</p>

<pre><code>df_concat['div'].unique()

array(['L', 'J', 'K', 'U', 'E', 'B', 'A', 'C', 'N', 'X', 'M', 'O', 'D',
       'I', 'P', 'Q', 'S', 'R', 'T'], dtype=object)
</code></pre>

<p>I tried to limit data with nrows=4000000 and it success converted to category dtypes !
what's wrong with it? </p>

<p>this my code:</p>

<pre><code>names = ['bdate', 'nama_site', 'kode_store', 'div', 'merdivdesc', 'cat', 'catdesc', 'subcat', 'subcatdesc', 'brand', 'sku', 'sku_desc', 'tillcode', 'netsales', 'profit', 'margin', 'qty']

dtype = {
    'netsales' : 'float32', 'profit' : 'float32', 'margin' : 'float32', 'qty' : 'float32',
    'div' : 'category'
}

data = pd.read_csv('clean_jan20_minified.csv', sep='|', dtype=dtype, chunksize=20000, names=names, skiprows=[0], nrows=4000000)

chunk_list = []  
for chunk in data:  
    chunk_list.append(chunk)

df_concat = pd.concat(chunk_list, ignore_index=True)

</code></pre>

<p>when i try manually convert with <code>df_concat['div']=df_concat['div'].astype('category')</code> it works. but i need convert it when read_csv</p>
","['append', 'x', 'margin', 'concat', 'limit', 'column', 'checked', 'a', 'b', 'cat', 'c', 'csv', 'python', 'pd', 'pandas']"
cohort Analysis on Day level,"<p>Below is the data for phone i.e phone_data that I am using to create month level cohort.
<strong>phone_data</strong></p>

<pre><code>ShieldId  order_reference_number creation_date
74476986877 8277.15211 2015-03-21   
57271838086 277.15049 2015-09-13    
5217209294  8277.15311 2015-07-05
13990324654 8277.15311 2015-09-29   
0331749465  8277.15031 2015-08-13   
</code></pre>

<pre><code># helper function 
def cohort_period(df):
    """"""
    Creates a `CohortPeriod` column on existing dataframe, which is the Nth period based on the user's first purchase.

    """"""
    df['CohortPeriod'] = np.arange(len(df)) + 1

    return df

def cohort_summarized(df):
 df['creation_date']=pd.to_datetime(df['creation_date'])  #create a date column on a monthly basis
 df['order_period']=df.creation_date.apply(lambda x: datetime.strftime(x,'%Y-%m'))

  df.set_index('ShieldId', inplace=True)
  df['CohortGroup'] = df.groupby(level=0)['creation_date'].min().apply(lambda x: x.strftime('%Y-%m'))
  df.reset_index(inplace=True)
  grouped = df.groupby(['CohortGroup','order_period'])
  cohorts = grouped.agg({'ShieldId': pd.Series.nunique})
  cohorts.rename(columns={'ShieldId': 'Total_Users'},inplace=True)
  cohorts = cohorts.groupby(level=0).apply(cohort_period)
  cohort_df = cohorts.reset_index() 

  return cohort_df 
def final_cohort_summary(df):
  """"""
  Creates two dataframe as output for 1---0th month repeated users cohort
                                      2-- all cohort group data 
  """"""
  all_cohort = cohort_summarized(df)

  filtered_grouped = df[df.CohortGroup==df.order_period].groupby(['CohortGroup','order_period','ShieldId'])
  filtered_cohort = filtered_grouped.agg({'order_reference_number': pd.Series.nunique}).reset_index()
  filtered_cohort = filtered_cohort[filtered_cohort.order_reference_number!=1].groupby('CohortGroup')['order_reference_number'].size().reset_index(name='user_0')

return all_cohort, filtered_cohort


def aggregated_cohort(df):
  """"""
  Takes input data frame for that we need to calculate cohort like phone_data, create_data, others_data, home_data
  """"""
  all_output,filtered_output = final_cohort_summary(df) # final cohort summary function used here

  df1 = pd.pivot_table(all_output, values='Total_Users',index=['CohortGroup'],columns=['CohortPeriod'], aggfunc=np.sum).reset_index()
  test_merge = pd.merge(df1,filtered_output,on='CohortGroup')
  df1 = test_merge[test_merge.columns.difference(['CohortGroup'])].sum(axis=0).reset_index(name='users').T
  df1.columns = df1.iloc[0]
  df1 = df1.rename(columns={1:'cohort_size','user_0':'m0'})
  renamed_list = ['cohort_size','m0']
  remaining_list = list(df1.columns.difference(renamed_list))
  columns_list = renamed_list + remaining_list
  df1 = df1[columns_list]
  df1 = df1[1:]

  return df1

phone_segment_cohort = aggregated_cohort(phone_data)

</code></pre>

<p><strong>Now I want to get the same output but at day level i.e for 7 days</strong>
so my output will look like something
        cohort size d0  d1  d2  d3  d4  d5  d6
Phones  1,157,367   91  89  34  24  22  18  10</p>
","['axis', 'list', 'lambda', 'frame', 'analysis', 'datetime', 'pd', 'period', 'final', 'a', 'python', 'df', 'statistics', 'merge', 'helper', 'index', 'pandas', 'x', 'dataframe', 'min', 'column', 'np']"
cohort Analysis on Day level,"<p>Below is the data for phone i.e phone_data that I am using to create month level cohort.
<strong>phone_data</strong></p>

<pre><code>ShieldId  order_reference_number creation_date
74476986877 8277.15211 2015-03-21   
57271838086 277.15049 2015-09-13    
5217209294  8277.15311 2015-07-05
13990324654 8277.15311 2015-09-29   
0331749465  8277.15031 2015-08-13   
</code></pre>

<pre><code># helper function 
def cohort_period(df):
    """"""
    Creates a `CohortPeriod` column on existing dataframe, which is the Nth period based on the user's first purchase.

    """"""
    df['CohortPeriod'] = np.arange(len(df)) + 1

    return df

def cohort_summarized(df):
 df['creation_date']=pd.to_datetime(df['creation_date'])  #create a date column on a monthly basis
 df['order_period']=df.creation_date.apply(lambda x: datetime.strftime(x,'%Y-%m'))

  df.set_index('ShieldId', inplace=True)
  df['CohortGroup'] = df.groupby(level=0)['creation_date'].min().apply(lambda x: x.strftime('%Y-%m'))
  df.reset_index(inplace=True)
  grouped = df.groupby(['CohortGroup','order_period'])
  cohorts = grouped.agg({'ShieldId': pd.Series.nunique})
  cohorts.rename(columns={'ShieldId': 'Total_Users'},inplace=True)
  cohorts = cohorts.groupby(level=0).apply(cohort_period)
  cohort_df = cohorts.reset_index() 

  return cohort_df 
def final_cohort_summary(df):
  """"""
  Creates two dataframe as output for 1---0th month repeated users cohort
                                      2-- all cohort group data 
  """"""
  all_cohort = cohort_summarized(df)

  filtered_grouped = df[df.CohortGroup==df.order_period].groupby(['CohortGroup','order_period','ShieldId'])
  filtered_cohort = filtered_grouped.agg({'order_reference_number': pd.Series.nunique}).reset_index()
  filtered_cohort = filtered_cohort[filtered_cohort.order_reference_number!=1].groupby('CohortGroup')['order_reference_number'].size().reset_index(name='user_0')

return all_cohort, filtered_cohort


def aggregated_cohort(df):
  """"""
  Takes input data frame for that we need to calculate cohort like phone_data, create_data, others_data, home_data
  """"""
  all_output,filtered_output = final_cohort_summary(df) # final cohort summary function used here

  df1 = pd.pivot_table(all_output, values='Total_Users',index=['CohortGroup'],columns=['CohortPeriod'], aggfunc=np.sum).reset_index()
  test_merge = pd.merge(df1,filtered_output,on='CohortGroup')
  df1 = test_merge[test_merge.columns.difference(['CohortGroup'])].sum(axis=0).reset_index(name='users').T
  df1.columns = df1.iloc[0]
  df1 = df1.rename(columns={1:'cohort_size','user_0':'m0'})
  renamed_list = ['cohort_size','m0']
  remaining_list = list(df1.columns.difference(renamed_list))
  columns_list = renamed_list + remaining_list
  df1 = df1[columns_list]
  df1 = df1[1:]

  return df1

phone_segment_cohort = aggregated_cohort(phone_data)

</code></pre>

<p><strong>Now I want to get the same output but at day level i.e for 7 days</strong>
so my output will look like something
        cohort size d0  d1  d2  d3  d4  d5  d6
Phones  1,157,367   91  89  34  24  22  18  10</p>
","['axis', 'list', 'lambda', 'frame', 'analysis', 'datetime', 'pd', 'period', 'final', 'a', 'python', 'df', 'statistics', 'merge', 'helper', 'index', 'pandas', 'x', 'dataframe', 'min', 'column', 'np']"
Is there a reason why when merging a Pandas DataFrame that rows would disappear?,"<p>I have a folder with a lot of pd.DataFrames stored as pickles.  My code is:</p>

<pre><code>for i in list_files:
    with open(data_location + i, ""rb"") as f_le:
        df_temp = pickle.load(f_le)

    df_final = pd.merge(df_final, df_temp, how='outer', on = [""Time""])
</code></pre>

<p>In each pd.DataFrame, Time is the index.  There is no column named Time.  The index values are pd.Timestamps. Each pd.DataFrame has values that start on 10/30/2019, and end either on 12/26, 12/27, or 12/28.</p>

<p>Since I am using ""outer"", I would imagine that my final data frame would range from 10/30 to 12/28.  The merging was going great until for some reason, when trying to merge the last 6 files, the final dataframe got cut to only range from 10/30 to 12/08, which seems to be very random.</p>

<p>Why would this happen? </p>
","['merge', 'dataframe', 'python-3.x', 'column', 'index', 'final', 'a', 'random', 'range', 'open', 'frame', 'python', 'pickle', 'pd', 'pandas']"
Is there a reason why when merging a Pandas DataFrame that rows would disappear?,"<p>I have a folder with a lot of pd.DataFrames stored as pickles.  My code is:</p>

<pre><code>for i in list_files:
    with open(data_location + i, ""rb"") as f_le:
        df_temp = pickle.load(f_le)

    df_final = pd.merge(df_final, df_temp, how='outer', on = [""Time""])
</code></pre>

<p>In each pd.DataFrame, Time is the index.  There is no column named Time.  The index values are pd.Timestamps. Each pd.DataFrame has values that start on 10/30/2019, and end either on 12/26, 12/27, or 12/28.</p>

<p>Since I am using ""outer"", I would imagine that my final data frame would range from 10/30 to 12/28.  The merging was going great until for some reason, when trying to merge the last 6 files, the final dataframe got cut to only range from 10/30 to 12/08, which seems to be very random.</p>

<p>Why would this happen? </p>
","['merge', 'dataframe', 'python-3.x', 'column', 'index', 'final', 'a', 'random', 'range', 'open', 'frame', 'python', 'pickle', 'pd', 'pandas']"
How to speed up Pandas loops,"<p>This code loops over a giant dataframe consisting of ~14 million rows, where each row is a published article. Our goal is to look at each keyword in <code>all_keywords</code> (which has ~400 keywords), and find the dates the keyword first and last appeared. </p>

<pre><code>ranges = {}
for dk in all_keywords:
    dk_df = df.loc[[(dk in map(str.lower, x)) for x in df['keywords']]]
    first_appearance = dk_df['date'].iloc[0].strftime('%Y')
    last_appearance = dk_df['date'].iloc[-1].strftime('%Y')
    ranges[dk] = [first_appearance, last_appearance]
</code></pre>

<p>The problem is that this code is so SLOW. It takes hours. </p>

<p>How can it be faster?</p>

<p>I suspect the problem is either with looping through each keyword, or with mapping all the keywords against each array in <code>df['keywords']</code>.</p>
","['optimization', 'x', 'loops', 'dataframe', 'a', 'map', 'mapping', 'python', 'df', 'loc', 'pandas']"
How to speed up Pandas loops,"<p>This code loops over a giant dataframe consisting of ~14 million rows, where each row is a published article. Our goal is to look at each keyword in <code>all_keywords</code> (which has ~400 keywords), and find the dates the keyword first and last appeared. </p>

<pre><code>ranges = {}
for dk in all_keywords:
    dk_df = df.loc[[(dk in map(str.lower, x)) for x in df['keywords']]]
    first_appearance = dk_df['date'].iloc[0].strftime('%Y')
    last_appearance = dk_df['date'].iloc[-1].strftime('%Y')
    ranges[dk] = [first_appearance, last_appearance]
</code></pre>

<p>The problem is that this code is so SLOW. It takes hours. </p>

<p>How can it be faster?</p>

<p>I suspect the problem is either with looping through each keyword, or with mapping all the keywords against each array in <code>df['keywords']</code>.</p>
","['optimization', 'x', 'loops', 'dataframe', 'a', 'map', 'mapping', 'python', 'df', 'loc', 'pandas']"
construct time series of all stocks from multiple daily data of all the stock in the market,"<p>I would like to extract(construct) time history of all the stocks in the market from a list of daily data of the market.
The daily data of the market have all the stocks' information for that particular date, for instance, a small sample daily data (20200615.csv) for 15/06/2020 and (20200616.csv) for 16/06/2020 maybe looks like below:</p>

<p><strong>20200615.csv</strong></p>

<blockquote>
<pre><code>code,date,open,high,low,close,pre_close,change,pct_chg,vol,amount
000001.XX,20200615,10.69,10.72,10.52,10.58,10.64,-0.06,-0.5639,6989.91,7410.214
000002.XX,20200615,7.0,7.03,6.9,6.97,6.99,-0.02,-0.2861,30254.21,21037.98
000003.XX,20200615,23.74,24.42,23.66,23.79,23.74,0.05,0.2106,6877.29,16457.953
</code></pre>
</blockquote>

<p><strong>20200616.csv</strong></p>

<blockquote>
<pre><code>code,date,open,high,low,close,pre_close,change,pct_chg,vol,amount
000001.XX,20200616,8.76,8.76,8.6,8.62,8.76,-0.14,-1.5982,67786.0,58649.629
000002.XX,20200616,15.43,15.98,14.71,14.94,15.8,-0.86,-5.443,407552.82,629492.589
000004.YY,20200616,6.96,7.21,6.95,7.08,6.95,0.13,1.8705,33382.0,23702.07
000005.XY,20200616,4.2,4.25,4.18,4.23,4.21,0.02,0.4751,46694.01,19702.396
</code></pre>
</blockquote>

<p>And I have this form of daily stock market data for a period of time, mulitple csv files are available, like:
    20200615.csv
    20200616.csv</p>

<p>And the stock ""code"" column may change over time, due to stock listing/delisting.</p>

<p>What I want to do is to extract the time series of all the individual stocks into separated data frames, and the expected output would be five data frames as listed:</p>

<p>dataframe for stock code: ""<strong>000001.XX</strong>""</p>

<blockquote>
<pre><code>000001.XX,20200615,10.69,10.72,10.52,10.58,10.64,-0.06,-0.5639,6989.91,7410.214
000001.XX,20200616,8.76,8.76,8.6,8.62,8.76,-0.14,-1.5982,67786.0,58649.629
</code></pre>
</blockquote>

<p>dataframe for stock code: ""<strong>000002.XX</strong>""</p>

<blockquote>
<pre><code>000002.XX,20200615,7.0,7.03,6.9,6.97,6.99,-0.02,-0.2861,30254.21,21037.98
000002.XX,20200616,15.43,15.98,14.71,14.94,15.8,-0.86,-5.443,407552.82,629492.589
</code></pre>
</blockquote>

<p>dataframe for stock code: ""<strong>000003.XX</strong>""</p>

<blockquote>
<pre><code>000003.XX,20200615,23.74,24.42,23.66,23.79,23.74,0.05,0.2106,6877.29,16457.953
</code></pre>
</blockquote>

<p>dataframe for stock code: ""<strong>000004.YY</strong>""</p>

<blockquote>
<pre><code>000004.YY,20200616,6.96,7.21,6.95,7.08,6.95,0.13,1.8705,33382.0,23702.07
</code></pre>
</blockquote>

<p>dataframe for stock code: ""<strong>000005.XY</strong>""</p>

<blockquote>
<pre><code>000005.XY,20200616,4.2,4.25,4.18,4.23,4.21,0.02,0.4751,46694.01,19702.396
</code></pre>
</blockquote>

<p>In reality, there would be 3000+ listed stocks with 1000+ daily data, so performance may also need to be considered.</p>

<p>Thank you.</p>
","['period', 'extract', 'history', 'form', 'dataframe', 'performance', 'list', 'column', 'a', 'time series', 'open', 'frames', 'csv', 'python', 'pandas']"
construct time series of all stocks from multiple daily data of all the stock in the market,"<p>I would like to extract(construct) time history of all the stocks in the market from a list of daily data of the market.
The daily data of the market have all the stocks' information for that particular date, for instance, a small sample daily data (20200615.csv) for 15/06/2020 and (20200616.csv) for 16/06/2020 maybe looks like below:</p>

<p><strong>20200615.csv</strong></p>

<blockquote>
<pre><code>code,date,open,high,low,close,pre_close,change,pct_chg,vol,amount
000001.XX,20200615,10.69,10.72,10.52,10.58,10.64,-0.06,-0.5639,6989.91,7410.214
000002.XX,20200615,7.0,7.03,6.9,6.97,6.99,-0.02,-0.2861,30254.21,21037.98
000003.XX,20200615,23.74,24.42,23.66,23.79,23.74,0.05,0.2106,6877.29,16457.953
</code></pre>
</blockquote>

<p><strong>20200616.csv</strong></p>

<blockquote>
<pre><code>code,date,open,high,low,close,pre_close,change,pct_chg,vol,amount
000001.XX,20200616,8.76,8.76,8.6,8.62,8.76,-0.14,-1.5982,67786.0,58649.629
000002.XX,20200616,15.43,15.98,14.71,14.94,15.8,-0.86,-5.443,407552.82,629492.589
000004.YY,20200616,6.96,7.21,6.95,7.08,6.95,0.13,1.8705,33382.0,23702.07
000005.XY,20200616,4.2,4.25,4.18,4.23,4.21,0.02,0.4751,46694.01,19702.396
</code></pre>
</blockquote>

<p>And I have this form of daily stock market data for a period of time, mulitple csv files are available, like:
    20200615.csv
    20200616.csv</p>

<p>And the stock ""code"" column may change over time, due to stock listing/delisting.</p>

<p>What I want to do is to extract the time series of all the individual stocks into separated data frames, and the expected output would be five data frames as listed:</p>

<p>dataframe for stock code: ""<strong>000001.XX</strong>""</p>

<blockquote>
<pre><code>000001.XX,20200615,10.69,10.72,10.52,10.58,10.64,-0.06,-0.5639,6989.91,7410.214
000001.XX,20200616,8.76,8.76,8.6,8.62,8.76,-0.14,-1.5982,67786.0,58649.629
</code></pre>
</blockquote>

<p>dataframe for stock code: ""<strong>000002.XX</strong>""</p>

<blockquote>
<pre><code>000002.XX,20200615,7.0,7.03,6.9,6.97,6.99,-0.02,-0.2861,30254.21,21037.98
000002.XX,20200616,15.43,15.98,14.71,14.94,15.8,-0.86,-5.443,407552.82,629492.589
</code></pre>
</blockquote>

<p>dataframe for stock code: ""<strong>000003.XX</strong>""</p>

<blockquote>
<pre><code>000003.XX,20200615,23.74,24.42,23.66,23.79,23.74,0.05,0.2106,6877.29,16457.953
</code></pre>
</blockquote>

<p>dataframe for stock code: ""<strong>000004.YY</strong>""</p>

<blockquote>
<pre><code>000004.YY,20200616,6.96,7.21,6.95,7.08,6.95,0.13,1.8705,33382.0,23702.07
</code></pre>
</blockquote>

<p>dataframe for stock code: ""<strong>000005.XY</strong>""</p>

<blockquote>
<pre><code>000005.XY,20200616,4.2,4.25,4.18,4.23,4.21,0.02,0.4751,46694.01,19702.396
</code></pre>
</blockquote>

<p>In reality, there would be 3000+ listed stocks with 1000+ daily data, so performance may also need to be considered.</p>

<p>Thank you.</p>
","['period', 'extract', 'history', 'form', 'dataframe', 'performance', 'list', 'column', 'a', 'time series', 'open', 'frames', 'csv', 'python', 'pandas']"
Using a dictionary to map specific values in python,"<p>I am iterating through a dataframe and pulling out specific lines and then enriching those lines with some other elements. I have a dictionary that has the following definition mapping:</p>

<pre><code>testdir = {0: 'zero', 40: 'forty', 60: 'sixty', 80: 'eighty'}
</code></pre>

<p>When i pull out a specific line from the original dataframe which looks like this</p>

<pre><code>a   b   c      x   str
0  0   0   0  100.0  aaaa
</code></pre>

<p>i want the str cell to now be set to the string value of column c which is 0 so </p>

<p>output should be </p>

<pre><code>a   b   c      x   str
0  0   0   0  100.0  zero
</code></pre>

<p>and then after meeting some other conditions a new line is pulled out from the original dataframe and the output should be</p>

<pre><code>a   b   c      x   str
0  0   0   0  100.0  zero
3  4  30  60  100.0  sixty
</code></pre>

<p>i tried to use the map() method so something like'</p>

<pre><code>df['str'][-1] = df['c'][-1].map(testdir)
</code></pre>

<p>but i'm erroring all over the place!</p>
","['x', 'dataframe', 'column', 'a', 'map', 'mapping', 'b', 'cell', 'c', 'python', 'df', 'pandas']"
Using a dictionary to map specific values in python,"<p>I am iterating through a dataframe and pulling out specific lines and then enriching those lines with some other elements. I have a dictionary that has the following definition mapping:</p>

<pre><code>testdir = {0: 'zero', 40: 'forty', 60: 'sixty', 80: 'eighty'}
</code></pre>

<p>When i pull out a specific line from the original dataframe which looks like this</p>

<pre><code>a   b   c      x   str
0  0   0   0  100.0  aaaa
</code></pre>

<p>i want the str cell to now be set to the string value of column c which is 0 so </p>

<p>output should be </p>

<pre><code>a   b   c      x   str
0  0   0   0  100.0  zero
</code></pre>

<p>and then after meeting some other conditions a new line is pulled out from the original dataframe and the output should be</p>

<pre><code>a   b   c      x   str
0  0   0   0  100.0  zero
3  4  30  60  100.0  sixty
</code></pre>

<p>i tried to use the map() method so something like'</p>

<pre><code>df['str'][-1] = df['c'][-1].map(testdir)
</code></pre>

<p>but i'm erroring all over the place!</p>
","['x', 'dataframe', 'column', 'a', 'map', 'mapping', 'b', 'cell', 'c', 'python', 'df', 'pandas']"
Converting Networkx graph to data frame with its attributes,"<p>I had two graphs, g1 and g2. I have applied some rules and mapped them as shown below.</p>

<p>graph:</p>

<p><a href=""https://i.stack.imgur.com/AOl1C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AOl1C.png"" alt=""enter image description here""></a></p>

<p>I would like to convert back the mapped graph to the pandas data frame /table/.</p>

<pre><code>From        Attribute        To         
10/10       Start          130/21
130/21      Left           190/190
190/190     Right          240/204
240/204     End             -
</code></pre>

<p>Is there any way to do this in Python Pandas? </p>
","['frame', 'graph', 'python', 'networkx', 'attributes', 'pandas']"
Converting Networkx graph to data frame with its attributes,"<p>I had two graphs, g1 and g2. I have applied some rules and mapped them as shown below.</p>

<p>graph:</p>

<p><a href=""https://i.stack.imgur.com/AOl1C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AOl1C.png"" alt=""enter image description here""></a></p>

<p>I would like to convert back the mapped graph to the pandas data frame /table/.</p>

<pre><code>From        Attribute        To         
10/10       Start          130/21
130/21      Left           190/190
190/190     Right          240/204
240/204     End             -
</code></pre>

<p>Is there any way to do this in Python Pandas? </p>
","['frame', 'graph', 'python', 'networkx', 'attributes', 'pandas']"
Create a &quot;matrix&quot; with pandas,"<p>I need to create a ""preformatted"" sheet in excel as:</p>

<pre><code>Vacation     Permit       Personnel No      Name                 01.01 02.01 ..... ..... 31.12
=SUM()       =SUM()       11111             Jon Snow
=SUM()       =SUM()       22222             Daenerys Targaryen
...          ...          ...               ...
...          ...          ...               ...
</code></pre>

<p>I try manually and work</p>

<pre><code>df = {'Vacation': ['=SUM(E2:Z2)', '=SUM(E2:Z2)'],
         'Permit': ['=SUM(E2:Z2)', '=SUM(E2:Z2)'],
         'Personnel': ['11111', '22222'],
         'Name': ['Jon Snow', ' Daenerys Targaryen'],
         '01.01': ['', ''],
         '02.01': ['', ''],
         '31.12': ['', '']
         }
</code></pre>

<p>But I need to create automatically all date column for the specific year using an existent list of Personal No and Name, then insert for each row a number of columns equal to the list of names.
For the first time every column of days of year are empty, they will be filled in later by other procedures.
This procedure created the basic sheet only the first time.</p>

<p>I hope I explained myself.</p>

<p>Regards, 
Marco</p>

<p>EDIT: I have found this solution</p>

<p>Create a dictionary with Day of Year and add to <code>di_daysOfYears</code> dictionary</p>

<pre><code>start_dt = date(2020, 1, 2)
end_dt = date(2020, 12, 31)
di_daysOfYears = {'01.01': None}
for dt in daterange(start_dt, end_dt):    
    di_daysOfYears[cal] = None 
</code></pre>

<p>Get the list of 2 columns from another excel files</p>

<pre><code>df_empList = pd.read_excel(filename, index_col=None, sheet_name='L5', usecols = ""A, B"")
</code></pre>

<p>Convert df_empList into dictionary</p>

<pre><code>di_empList = df_empList.to_dict('list')
</code></pre>

<p>Create a dictionary with 2 columns with formulas</p>

<pre><code>di_count = {'Vacation Count': '=SUM()',
            'Permit Count': '=SUM()'
            }
</code></pre>

<p>Concat all dictionary and convert do Dataframe</p>

<pre><code>di_count.update(df_empList)
di_count.update(di_daysOfYears)
df_empList = pd.DataFrame(di_count)

print(df_empList)
</code></pre>

<p>I don't know if exist a more fast procedure.
This works.</p>
","['matrix', 'concat', 'dataframe', 'edit', 'list', 'column', 'insert', 'a', 'cal', 'procedures', 'basic', 'formulas', 'excel', 'dt', 'b', 'df', 'pd', 'procedure', 'pandas']"
Create a &quot;matrix&quot; with pandas,"<p>I need to create a ""preformatted"" sheet in excel as:</p>

<pre><code>Vacation     Permit       Personnel No      Name                 01.01 02.01 ..... ..... 31.12
=SUM()       =SUM()       11111             Jon Snow
=SUM()       =SUM()       22222             Daenerys Targaryen
...          ...          ...               ...
...          ...          ...               ...
</code></pre>

<p>I try manually and work</p>

<pre><code>df = {'Vacation': ['=SUM(E2:Z2)', '=SUM(E2:Z2)'],
         'Permit': ['=SUM(E2:Z2)', '=SUM(E2:Z2)'],
         'Personnel': ['11111', '22222'],
         'Name': ['Jon Snow', ' Daenerys Targaryen'],
         '01.01': ['', ''],
         '02.01': ['', ''],
         '31.12': ['', '']
         }
</code></pre>

<p>But I need to create automatically all date column for the specific year using an existent list of Personal No and Name, then insert for each row a number of columns equal to the list of names.
For the first time every column of days of year are empty, they will be filled in later by other procedures.
This procedure created the basic sheet only the first time.</p>

<p>I hope I explained myself.</p>

<p>Regards, 
Marco</p>

<p>EDIT: I have found this solution</p>

<p>Create a dictionary with Day of Year and add to <code>di_daysOfYears</code> dictionary</p>

<pre><code>start_dt = date(2020, 1, 2)
end_dt = date(2020, 12, 31)
di_daysOfYears = {'01.01': None}
for dt in daterange(start_dt, end_dt):    
    di_daysOfYears[cal] = None 
</code></pre>

<p>Get the list of 2 columns from another excel files</p>

<pre><code>df_empList = pd.read_excel(filename, index_col=None, sheet_name='L5', usecols = ""A, B"")
</code></pre>

<p>Convert df_empList into dictionary</p>

<pre><code>di_empList = df_empList.to_dict('list')
</code></pre>

<p>Create a dictionary with 2 columns with formulas</p>

<pre><code>di_count = {'Vacation Count': '=SUM()',
            'Permit Count': '=SUM()'
            }
</code></pre>

<p>Concat all dictionary and convert do Dataframe</p>

<pre><code>di_count.update(df_empList)
di_count.update(di_daysOfYears)
df_empList = pd.DataFrame(di_count)

print(df_empList)
</code></pre>

<p>I don't know if exist a more fast procedure.
This works.</p>
","['matrix', 'concat', 'dataframe', 'edit', 'list', 'column', 'insert', 'a', 'cal', 'procedures', 'basic', 'formulas', 'excel', 'dt', 'b', 'df', 'pd', 'procedure', 'pandas']"
Format doesn&#39;t show,"<p>So I'm trying to get my month numbers to be zero padded from the left meaning {01, 02, 03, ..., 10, 11, 12}.<br></p>

<p>I thought to do so with formatting but after searching the internet I wonder if formatting really adds a zero or if I'll only see a zero when printed.<br><br>
Either way, what I was trying:<br></p>

<pre><code>months.style.format({'Test': '{:0&gt;2}'})
print(months)
</code></pre>

<p>When I use this code it prints the 'Test' column without the format, why is this? I use Jupyter notebook and when I don't use the print function, he displays the data with the right format. Thank you</p>
","['zero-padding', 'jupyter', 'format', 'formatting', 'column', 'a', 'python', 'internet', 'pandas']"
Format doesn&#39;t show,"<p>So I'm trying to get my month numbers to be zero padded from the left meaning {01, 02, 03, ..., 10, 11, 12}.<br></p>

<p>I thought to do so with formatting but after searching the internet I wonder if formatting really adds a zero or if I'll only see a zero when printed.<br><br>
Either way, what I was trying:<br></p>

<pre><code>months.style.format({'Test': '{:0&gt;2}'})
print(months)
</code></pre>

<p>When I use this code it prints the 'Test' column without the format, why is this? I use Jupyter notebook and when I don't use the print function, he displays the data with the right format. Thank you</p>
","['zero-padding', 'jupyter', 'format', 'formatting', 'column', 'a', 'python', 'internet', 'pandas']"
Compare the values of index with column names ; Python Pandas,"<p>Hi I have two Dataframe as given below</p>

<pre><code>df1 = pd.DataFrame.from_dict(({""Column"":{""0"":""A"",""1"":""B"",""2"":""C""},""Column2"":{""0"":""T1"",""1"":""T2"",""2"":""T1""}}))
</code></pre>

<p><a href=""https://i.stack.imgur.com/uEXYA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uEXYA.png"" alt=""enter image description here""></a></p>

<p>Then I created another dataframe using below statement</p>

<pre><code>df2 = pd.DataFrame(np.zeros(shape=(df1.shape[0],df1.shape[0])), columns=df1['Column'].values, index=df1['Column'].values)
</code></pre>

<p><a href=""https://i.stack.imgur.com/6iahD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6iahD.png"" alt=""enter image description here""></a></p>

<p>now i need to update df2 as if index is equals to column then assign value 1 if index is not equal to column then check in df1 if for that index and column value column2 value matches then assign value 2 else assign 3</p>

<p>Expected result:</p>

<p><a href=""https://i.stack.imgur.com/4Jwbz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Jwbz.png"" alt=""enter image description here""></a></p>

<p>Can we achieve it without using for loops ?</p>

<p>Note : Shape and values of df1 can be different every time,</p>
","['loops', 'dataframe', 'column', 'index', 'a', 'compare', 'b', 'c', 'np', 'python', 'pd', 'pandas']"
Compare the values of index with column names ; Python Pandas,"<p>Hi I have two Dataframe as given below</p>

<pre><code>df1 = pd.DataFrame.from_dict(({""Column"":{""0"":""A"",""1"":""B"",""2"":""C""},""Column2"":{""0"":""T1"",""1"":""T2"",""2"":""T1""}}))
</code></pre>

<p><a href=""https://i.stack.imgur.com/uEXYA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uEXYA.png"" alt=""enter image description here""></a></p>

<p>Then I created another dataframe using below statement</p>

<pre><code>df2 = pd.DataFrame(np.zeros(shape=(df1.shape[0],df1.shape[0])), columns=df1['Column'].values, index=df1['Column'].values)
</code></pre>

<p><a href=""https://i.stack.imgur.com/6iahD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6iahD.png"" alt=""enter image description here""></a></p>

<p>now i need to update df2 as if index is equals to column then assign value 1 if index is not equal to column then check in df1 if for that index and column value column2 value matches then assign value 2 else assign 3</p>

<p>Expected result:</p>

<p><a href=""https://i.stack.imgur.com/4Jwbz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Jwbz.png"" alt=""enter image description here""></a></p>

<p>Can we achieve it without using for loops ?</p>

<p>Note : Shape and values of df1 can be different every time,</p>
","['loops', 'dataframe', 'column', 'index', 'a', 'compare', 'b', 'c', 'np', 'python', 'pd', 'pandas']"
Unable to plot more than 1k points ( Approx ) in mplleaflet in Python,"<p>Are there any limitations to the number of points to be plotted on maps using latitude and longitude in mplleaflet in Python ?</p>

<p>Whenever I plot about 1k points, it goes fine. But, as I exceed them to 8-9k ( desired number of points ), I get a full white output ( neither do I see the map )</p>

<p>Note: The input data is clean even after the 1000th index because, if I plot the next 1k points ranging index from 1000 to 2000 it again works well. But, if I include 2k points ( 0 : 2000 ) I get a blank space here</p>

<p>Following is the code I am using.</p>

<p>Lats_longs is a dataframe I am using which has Latitude and Longitudes data.</p>

<pre><code>import mplleaflet as mpl

fig1, axes = plt.subplots(figsize = (11,5))
axes.scatter(Lats_Longs['LON'][0:],Lats_Longs['LAT'][0:] , s=10, alpha=0.7)

#The following 2 commented lines works fine individually.( But not both as the total points then goes to 2k )
#axes.scatter(Lats_Longs['LON'][0:1000],Lats_Longs['LAT'][0:1000] , s=10, alpha=0.7)
#axes.scatter(Lats_Longs['LON'][1000:2000],Lats_Longs['LAT'][1000:2000] , s=10, alpha=0.7)

mplleaflet.display(fig = fig1)

</code></pre>
","['exceed', 'dataframe', 'plot', 'next', 'index', 'a', 'map', 'matplotlib', 'axes', 'alpha', 'import', 'maps', 'plt', 'python', 'fig', 'pandas']"
Unable to plot more than 1k points ( Approx ) in mplleaflet in Python,"<p>Are there any limitations to the number of points to be plotted on maps using latitude and longitude in mplleaflet in Python ?</p>

<p>Whenever I plot about 1k points, it goes fine. But, as I exceed them to 8-9k ( desired number of points ), I get a full white output ( neither do I see the map )</p>

<p>Note: The input data is clean even after the 1000th index because, if I plot the next 1k points ranging index from 1000 to 2000 it again works well. But, if I include 2k points ( 0 : 2000 ) I get a blank space here</p>

<p>Following is the code I am using.</p>

<p>Lats_longs is a dataframe I am using which has Latitude and Longitudes data.</p>

<pre><code>import mplleaflet as mpl

fig1, axes = plt.subplots(figsize = (11,5))
axes.scatter(Lats_Longs['LON'][0:],Lats_Longs['LAT'][0:] , s=10, alpha=0.7)

#The following 2 commented lines works fine individually.( But not both as the total points then goes to 2k )
#axes.scatter(Lats_Longs['LON'][0:1000],Lats_Longs['LAT'][0:1000] , s=10, alpha=0.7)
#axes.scatter(Lats_Longs['LON'][1000:2000],Lats_Longs['LAT'][1000:2000] , s=10, alpha=0.7)

mplleaflet.display(fig = fig1)

</code></pre>
","['exceed', 'dataframe', 'plot', 'next', 'index', 'a', 'map', 'matplotlib', 'axes', 'alpha', 'import', 'maps', 'plt', 'python', 'fig', 'pandas']"
find indexes of minimum points in moving time window in a pandas dataframe,"<h2>The Problem</h2>

<p>I have a dataframe which looks like this (the indexes are actually dates):</p>

<pre><code>    col1  col2 
0   40.0  0 
1   22.0  0 
2   30.0  0 
3   29.1  0
4   20.0  0
5   17.2  0
6   44.5  0
7   30.0  0 
8   19.3  0
9   30.2  0
10  11.7  0 
11  29.1  0
...
</code></pre>

<p>What I'd like is to find in each moving time window (each time the window shifts by 1) of size <code>N</code>, for example 5, to get the indexes of the <code>K</code> smallest values in <code>col1</code>, for example 2, and set <code>col2</code> to <code>1</code> in such rows.</p>

<p>So in this example we would look at this window and set the 3 minimal indexes: <code>1,3,4</code>, to be <code>1</code> in <code>col2</code></p>

<pre><code>   col1  col2 
0   40.0  0 
1   22.0  0 
2   30.0  0 
3   29.1  0
4   20.0  0
</code></pre>

<p>and then at the window here below and set <code>1,4,5</code> to be <code>1</code> in <code>col2</code> (It doesn't matter that we already chose index <code>1</code> in the previous window)</p>

<pre><code>    col1  col2 
1   22.0  0 
2   30.0  0 
3   29.1  0
4   20.0  0
5   17.2  0
</code></pre>

<p>and so on...</p>

<h2>What have I tried?</h2>

<p>I tried using a for loop but it's <strong>extremely slow</strong></p>

<pre><code>shift_df = df.copy(deep=True)  # create a copy of the df to shift by one each time
for frame in range(len(df.index) - N):
    frame_df = shift_df.head(N)
    df.loc[frame_df.nsmallest(K, 'col1').index.tolist(), 'col2'] = 1
    shift_df = shift_df[1:]
</code></pre>

<p>I also thought of using <code>rolling</code> but couldn't find a way to implement it here.</p>
","['minimum', 'dataframe', 'index', 'a', 'range', 'frame', 'python', 'df', 'loc', 'pandas']"
find indexes of minimum points in moving time window in a pandas dataframe,"<h2>The Problem</h2>

<p>I have a dataframe which looks like this (the indexes are actually dates):</p>

<pre><code>    col1  col2 
0   40.0  0 
1   22.0  0 
2   30.0  0 
3   29.1  0
4   20.0  0
5   17.2  0
6   44.5  0
7   30.0  0 
8   19.3  0
9   30.2  0
10  11.7  0 
11  29.1  0
...
</code></pre>

<p>What I'd like is to find in each moving time window (each time the window shifts by 1) of size <code>N</code>, for example 5, to get the indexes of the <code>K</code> smallest values in <code>col1</code>, for example 2, and set <code>col2</code> to <code>1</code> in such rows.</p>

<p>So in this example we would look at this window and set the 3 minimal indexes: <code>1,3,4</code>, to be <code>1</code> in <code>col2</code></p>

<pre><code>   col1  col2 
0   40.0  0 
1   22.0  0 
2   30.0  0 
3   29.1  0
4   20.0  0
</code></pre>

<p>and then at the window here below and set <code>1,4,5</code> to be <code>1</code> in <code>col2</code> (It doesn't matter that we already chose index <code>1</code> in the previous window)</p>

<pre><code>    col1  col2 
1   22.0  0 
2   30.0  0 
3   29.1  0
4   20.0  0
5   17.2  0
</code></pre>

<p>and so on...</p>

<h2>What have I tried?</h2>

<p>I tried using a for loop but it's <strong>extremely slow</strong></p>

<pre><code>shift_df = df.copy(deep=True)  # create a copy of the df to shift by one each time
for frame in range(len(df.index) - N):
    frame_df = shift_df.head(N)
    df.loc[frame_df.nsmallest(K, 'col1').index.tolist(), 'col2'] = 1
    shift_df = shift_df[1:]
</code></pre>

<p>I also thought of using <code>rolling</code> but couldn't find a way to implement it here.</p>
","['minimum', 'dataframe', 'index', 'a', 'range', 'frame', 'python', 'df', 'loc', 'pandas']"
